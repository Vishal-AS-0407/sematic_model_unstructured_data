{
  "nodes": [
    {
      "id": "Transformer",
      "type": "architecture",
      "documents": [
        "attention_is_all_you_need",
        "deit",
        "gpt3_language_models",
        "gpt3",
        "palm",
        "reformer"
      ],
      "mentions": 6,
      "aliases": [
        "transformer",
        "Transformer"
      ],
      "description": "A new network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions.",
      "pagerank": 0.008595654786197731,
      "centrality": 0,
      "betweenness": 0.0011217809763287955,
      "eigenvector": 0.10867697878251624,
      "in_degree": 6,
      "out_degree": 5,
      "total_degree": 11,
      "clustering_coefficient": 0.022222222222222223,
      "community": 0,
      "importance": 0.3332547445162759,
      "inferred_type": "architecture"
    },
    {
      "id": "Attention Mechanism",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Attention Mechanism"
      ],
      "description": "A mechanism that allows modeling of dependencies without regard to their distance in input or output sequences.",
      "pagerank": 0.0026106323658725895,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017080549812389306,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.030448058037685938,
      "inferred_type": "method"
    },
    {
      "id": "BLEU",
      "type": "metric",
      "documents": [
        "attention_is_all_you_need",
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 3,
      "aliases": [
        "BLEU"
      ],
      "description": "A metric for evaluating the quality of machine translation by comparing a machine's output with a reference output.",
      "pagerank": 0.0026106323658725895,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017080549812389306,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.08044805803768593,
      "inferred_type": "metric"
    },
    {
      "id": "WMT 2014 English-to-German",
      "type": "dataset",
      "documents": [
        "attention_is_all_you_need",
        "attention_is_all_you_need"
      ],
      "mentions": 2,
      "aliases": [
        "WMT 2014 English-to-German",
        "WMT 2014 English-German dataset"
      ],
      "description": "A dataset used for evaluating machine translation performance.",
      "pagerank": 0.0026106323658725895,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017080549812389306,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.05544805803768593,
      "inferred_type": "dataset"
    },
    {
      "id": "WMT 2014 English-to-French",
      "type": "dataset",
      "documents": [
        "attention_is_all_you_need",
        "attention_is_all_you_need"
      ],
      "mentions": 2,
      "aliases": [
        "WMT 2014 English-to-French",
        "WMT 2014 English-French dataset"
      ],
      "description": "A dataset used for evaluating machine translation performance.",
      "pagerank": 0.0026106323658725895,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017080549812389306,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.05544805803768593,
      "inferred_type": "dataset"
    },
    {
      "id": "Google Brain",
      "type": "organization",
      "documents": [
        "attention_is_all_you_need",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "Google Brain"
      ],
      "description": "A deep learning research team at Google.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0043149624229508,
      "in_degree": 0,
      "out_degree": 8,
      "total_degree": 8,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.06550119187860652,
      "inferred_type": "organization"
    },
    {
      "id": "Google Research",
      "type": "organization",
      "documents": [
        "attention_is_all_you_need",
        "deit",
        "palm",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "Google Research"
      ],
      "description": "A research division of Google focusing on various scientific and technological advancements.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.06557474607858729,
      "in_degree": 0,
      "out_degree": 2,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.1369031734278509,
      "inferred_type": "organization"
    },
    {
      "id": "Ashish Vaswani",
      "type": "person",
      "documents": [
        "attention_is_all_you_need",
        "deit",
        "reformer"
      ],
      "mentions": 3,
      "aliases": [
        "Ashish Vaswani"
      ],
      "description": "One of the authors who designed and implemented the first Transformer models.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0566681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Noam Shazeer",
      "type": "person",
      "documents": [
        "attention_is_all_you_need",
        "deit",
        "reformer"
      ],
      "mentions": 3,
      "aliases": [
        "Noam Shazeer"
      ],
      "description": "One of the authors who proposed scaled dot-product attention and multi-head attention.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0566681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Niki Parmar",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Niki Parmar"
      ],
      "description": "One of the authors involved in the design and implementation of the Transformer.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0066681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Jakob Uszkoreit",
      "type": "person",
      "documents": [
        "attention_is_all_you_need",
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "Jakob Uszkoreit"
      ],
      "description": "One of the authors who proposed replacing RNNs with self-attention.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.00028450966790947714,
      "eigenvector": 0.022697485219576843,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.09959312814476012,
      "inferred_type": "person"
    },
    {
      "id": "Llion Jones",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Llion Jones"
      ],
      "description": "One of the authors responsible for the initial codebase and efficient inference.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0066681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Aidan N. Gomez",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Aidan N. Gomez"
      ],
      "description": "One of the authors involved in designing various parts of the Tensor2Tensor framework.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0066681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Łukasz Kaiser",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Łukasz Kaiser"
      ],
      "description": "One of the authors who contributed to the design and implementation of Tensor2Tensor.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0066681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Illia Polosukhin",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Illia Polosukhin"
      ],
      "description": "One of the authors who contributed to the design and implementation of the Transformer.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.000678404047195668,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 1,
      "importance": 0.0066681000466778,
      "inferred_type": "person"
    },
    {
      "id": "Encoder",
      "type": "Architecture",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Encoder"
      ],
      "description": "A component of the model composed of a stack of identical layers with multi-head self-attention and feed-forward networks.",
      "pagerank": 0.002614971293513153,
      "centrality": 0,
      "betweenness": 8.12884765455649e-06,
      "eigenvector": 8.574195292261602e-24,
      "in_degree": 3,
      "out_degree": 1,
      "total_degree": 4,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.03804024617157851,
      "inferred_type": "Architecture"
    },
    {
      "id": "Decoder",
      "type": "Architecture",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Decoder"
      ],
      "description": "A component of the model that generates output sequences, incorporating attention mechanisms over encoder outputs.",
      "pagerank": 0.0035921226519339176,
      "centrality": 0,
      "betweenness": 1.3548079424260817e-05,
      "eigenvector": 9.670197354848721e-24,
      "in_degree": 4,
      "out_degree": 1,
      "total_degree": 5,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.055335894156931936,
      "inferred_type": "Architecture"
    },
    {
      "id": "Multi-Head Attention",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Multi-Head Attention"
      ],
      "description": "An attention mechanism that allows the model to jointly attend to information from different representation subspaces.",
      "pagerank": 0.006427215788819732,
      "centrality": 0,
      "betweenness": 1.625769530911298e-05,
      "eigenvector": 7.042293412318417e-24,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.08162044490822758,
      "inferred_type": "Method"
    },
    {
      "id": "Scaled Dot-Product Attention",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Scaled Dot-Product Attention"
      ],
      "description": "An attention function that computes a weighted sum of values based on the compatibility of queries and keys.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 9.001857133124397e-25,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.004838709677419354,
      "inferred_type": "Method"
    },
    {
      "id": "Residual Connection",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Residual Connection"
      ],
      "description": "A technique used to facilitate training by allowing gradients to flow through the network more easily.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 6.1421076990059765e-24,
      "in_degree": 0,
      "out_degree": 2,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.009677419354838708,
      "inferred_type": "Method"
    },
    {
      "id": "Layer Normalization",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Layer Normalization"
      ],
      "description": "A normalization technique applied to the outputs of sub-layers to stabilize and accelerate training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 6.1421076990059765e-24,
      "in_degree": 0,
      "out_degree": 2,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.009677419354838708,
      "inferred_type": "Method"
    },
    {
      "id": "Feed-Forward Network",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Feed-Forward Network"
      ],
      "description": "A fully connected network applied to each position separately in the encoder and decoder.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 6.1421076990059765e-24,
      "in_degree": 0,
      "out_degree": 2,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.009677419354838708,
      "inferred_type": "Method"
    },
    {
      "id": "Embedding Layer",
      "type": "Architecture",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Embedding Layer"
      ],
      "description": "A layer that converts input tokens and output tokens into vector representations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 4,
      "importance": 0.0,
      "inferred_type": "Architecture"
    },
    {
      "id": "Softmax Function",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Softmax Function"
      ],
      "description": "A function used to convert decoder outputs into predicted next-token probabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 3.2555423889942997e-24,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.004838709677419354,
      "inferred_type": "Method"
    },
    {
      "id": "Attention Function",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Attention Function"
      ],
      "description": "A function that maps queries and key-value pairs to an output.",
      "pagerank": 0.007591986466345949,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.6738934603261914e-24,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 3,
      "importance": 0.09131554270345918,
      "inferred_type": "Method"
    },
    {
      "id": "ReLU Activation",
      "type": "Method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "ReLU Activation"
      ],
      "description": "An activation function used in feed-forward networks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 5,
      "importance": 0.0,
      "inferred_type": "Method"
    },
    {
      "id": "Attention(Q, K, V)",
      "type": "Function",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Attention(Q, K, V)"
      ],
      "description": "The mathematical representation of the attention mechanism.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 6,
      "importance": 0.0,
      "inferred_type": "Function"
    },
    {
      "id": "d_model",
      "type": "Metric",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "d_model"
      ],
      "description": "The dimensionality of the model's representations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 7,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "n",
      "type": "Metric",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "n"
      ],
      "description": "The sequence length.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 8,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "k",
      "type": "Metric",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "k"
      ],
      "description": "The kernel size of convolutions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 9,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "r",
      "type": "Metric",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "r"
      ],
      "description": "The size of the neighborhood in restricted self-attention.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 10,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "Self-Attention",
      "type": "method",
      "documents": [
        "attention_is_all_you_need",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Self-Attention",
        "Self-attention"
      ],
      "description": "A mechanism that allows the model to weigh the importance of different tokens in a sequence.",
      "pagerank": 0.0026106323658725895,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017080549812389306,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.060286767715105286,
      "inferred_type": "method"
    },
    {
      "id": "Recurrent Neural Network (RNN)",
      "type": "architecture",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Recurrent Neural Network (RNN)"
      ],
      "description": "A type of neural network designed for sequential data processing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 11,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "Convolutional Neural Network (CNN)",
      "type": "architecture",
      "documents": [
        "attention_is_all_you_need",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "CNN (Convolutional Neural Network)",
        "Convolutional Neural Network (CNN)"
      ],
      "description": "A type of neural network primarily used for processing grid-like data such as images.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 12,
      "importance": 0.049999999999999996,
      "inferred_type": "architecture"
    },
    {
      "id": "Positional Encoding",
      "type": "method",
      "documents": [
        "attention_is_all_you_need",
        "transformer_xl",
        "transformer_xl"
      ],
      "mentions": 3,
      "aliases": [
        "Positional Encoding",
        "relative positional encoding",
        "positional encoding"
      ],
      "description": "A technique used to inject information about the position of tokens in a sequence.",
      "pagerank": 0.004812614192035296,
      "centrality": 0,
      "betweenness": 0.00040915199861267667,
      "eigenvector": 0.02649782496453537,
      "in_degree": 3,
      "out_degree": 3,
      "total_degree": 6,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.14212584158157684,
      "inferred_type": "method"
    },
    {
      "id": "Adam Optimizer",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Adam Optimizer"
      ],
      "description": "An optimization algorithm used for training machine learning models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 13,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "BLEU Score",
      "type": "metric",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "BLEU Score"
      ],
      "description": "A metric for evaluating the quality of text which has been machine-translated.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 14,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Byte-Pair Encoding",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Byte-Pair Encoding"
      ],
      "description": "A data compression technique used for encoding text.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 15,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Word-Piece Vocabulary",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Word-Piece Vocabulary"
      ],
      "description": "A subword tokenization method used in natural language processing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 16,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Residual Dropout",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Residual Dropout"
      ],
      "description": "A regularization technique applied to neural networks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 17,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Label Smoothing",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Label Smoothing"
      ],
      "description": "A technique used during training to prevent overfitting by softening the target labels.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 18,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Separable Convolutions",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Separable Convolutions"
      ],
      "description": "A type of convolution that reduces the computational complexity of standard convolutions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 19,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "NVIDIA P100 GPU",
      "type": "hardware",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "NVIDIA P100 GPU"
      ],
      "description": "A type of graphics processing unit used for training deep learning models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 20,
      "importance": 0.0,
      "inferred_type": "hardware"
    },
    {
      "id": "ConvS2SEnsemble",
      "type": "model",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "ConvS2SEnsemble"
      ],
      "description": "An ensemble model used for machine translation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 21,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "WMT2014",
      "type": "dataset",
      "documents": [
        "attention_is_all_you_need",
        "attention_is_all_you_need"
      ],
      "mentions": 2,
      "aliases": [
        "WMT2014",
        "WMT 2014"
      ],
      "description": "A benchmark dataset for machine translation tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 22,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "P100 GPUs",
      "type": "hardware",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "P100 GPUs"
      ],
      "description": "A type of GPU used for training the models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 23,
      "importance": 0.0,
      "inferred_type": "hardware"
    },
    {
      "id": "Penn Treebank",
      "type": "dataset",
      "documents": [
        "attention_is_all_you_need",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "Penn Treebank"
      ],
      "description": "A dataset used for English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 24,
      "importance": 0.02983870967741935,
      "inferred_type": "dataset"
    },
    {
      "id": "Wall Street Journal",
      "type": "dataset",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Wall Street Journal"
      ],
      "description": "A portion of the Penn Treebank used for training.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 24,
      "importance": 0.01722051862198086,
      "inferred_type": "dataset"
    },
    {
      "id": "Beam Search",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Beam Search"
      ],
      "description": "A search algorithm used during inference to generate translations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 25,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Attention Heads",
      "type": "architecture component",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Attention Heads"
      ],
      "description": "Components of the Transformer that allow it to focus on different parts of the input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 26,
      "importance": 0.0,
      "inferred_type": "architecture component"
    },
    {
      "id": "Semi-supervised Setting",
      "type": "training method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Semi-supervised Setting"
      ],
      "description": "A training approach that uses both labeled and unlabeled data.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 27,
      "importance": 0.0,
      "inferred_type": "training method"
    },
    {
      "id": "Vinyals & Kaiser (2014)",
      "type": "publication",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Vinyals & Kaiser (2014)"
      ],
      "description": "A reference for previous work in English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 28,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Dyer et al. (2016)",
      "type": "publication",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Dyer et al. (2016)"
      ],
      "description": "A reference for previous work in English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 29,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Zhu et al. (2013)",
      "type": "publication",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Zhu et al. (2013)"
      ],
      "description": "A reference for previous work in English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 30,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Huang & Harper (2009)",
      "type": "publication",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Huang & Harper (2009)"
      ],
      "description": "A reference for previous work in English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 31,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "McClosky et al. (2006)",
      "type": "publication",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "McClosky et al. (2006)"
      ],
      "description": "A reference for previous work in English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 32,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Luong et al. (2015)",
      "type": "publication",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Luong et al. (2015)"
      ],
      "description": "A reference for previous work in English constituency parsing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 33,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "attention-based models",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "attention-based models"
      ],
      "description": "Models that use attention mechanisms to improve performance in tasks like translation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 34,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "images",
      "type": "data type",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "images"
      ],
      "description": "Visual data type that the authors plan to apply their models to.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 35,
      "importance": 0.0,
      "inferred_type": "data type"
    },
    {
      "id": "audio",
      "type": "data type",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "audio"
      ],
      "description": "Sound data type that the authors plan to apply their models to.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 36,
      "importance": 0.0,
      "inferred_type": "data type"
    },
    {
      "id": "video",
      "type": "data type",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "video"
      ],
      "description": "Moving visual data type that the authors plan to apply their models to.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 37,
      "importance": 0.0,
      "inferred_type": "data type"
    },
    {
      "id": "local, restricted attention mechanisms",
      "type": "method",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "local, restricted attention mechanisms"
      ],
      "description": "Attention mechanisms designed to efficiently handle large inputs and outputs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 38,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "code repository",
      "type": "resource",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "code repository"
      ],
      "description": "GitHub repository for the code used to train and evaluate models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 39,
      "importance": 0.0,
      "inferred_type": "resource"
    },
    {
      "id": "Nal Kalchbrenner",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Nal Kalchbrenner"
      ],
      "description": "Researcher acknowledged for comments and inspiration.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 40,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Stephan Gouws",
      "type": "person",
      "documents": [
        "attention_is_all_you_need"
      ],
      "mentions": 1,
      "aliases": [
        "Stephan Gouws"
      ],
      "description": "Researcher acknowledged for comments and inspiration.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 41,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "CLIP",
      "type": "model",
      "documents": [
        "clip",
        "segment_anything"
      ],
      "mentions": 2,
      "aliases": [
        "CLIP"
      ],
      "description": "A model that jointly trains an image encoder and a text encoder to predict correct pairings of image and text.",
      "pagerank": 0.004017281052824564,
      "centrality": 0,
      "betweenness": 0.005276344692043123,
      "eigenvector": 0.05211504336835125,
      "in_degree": 4,
      "out_degree": 11,
      "total_degree": 15,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.22870840935667666,
      "inferred_type": "model"
    },
    {
      "id": "GPT-3",
      "type": "model",
      "documents": [
        "clip",
        "gpt3_language_models",
        "gpt3",
        "llama",
        "palm",
        "segment_anything"
      ],
      "mentions": 6,
      "aliases": [
        "GPT-3",
        "GPT-4"
      ],
      "description": "A state-of-the-art language model that is competitive across many tasks.",
      "pagerank": 0.018818293659738593,
      "centrality": 0,
      "betweenness": 0.014426401413335647,
      "eigenvector": 0.45343567893495,
      "in_degree": 9,
      "out_degree": 17,
      "total_degree": 26,
      "clustering_coefficient": 0.03557312252964427,
      "community": 43,
      "importance": 0.8629572421542531,
      "inferred_type": "model"
    },
    {
      "id": "ImageNet",
      "type": "dataset",
      "documents": [
        "clip",
        "deit",
        "reformer",
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "ImageNet",
        "ImageNet ReaL"
      ],
      "description": "A large dataset used for image classification tasks.",
      "pagerank": 0.006318448650840686,
      "centrality": 0,
      "betweenness": 0.0029995447845313447,
      "eigenvector": 0.04306743924321184,
      "in_degree": 3,
      "out_degree": 2,
      "total_degree": 5,
      "clustering_coefficient": 0.3333333333333333,
      "community": 44,
      "importance": 0.24915937681035638,
      "inferred_type": "dataset"
    },
    {
      "id": "YFCC100M",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "YFCC100M"
      ],
      "description": "A dataset containing images and associated metadata used for training models.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.008190087586504509,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.012173733757226562,
      "inferred_type": "dataset"
    },
    {
      "id": "ResNet-50",
      "type": "model",
      "documents": [
        "clip",
        "swin_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "ResNet-50"
      ],
      "description": "A convolutional neural network architecture used for image classification.",
      "pagerank": 0.0018316821043391687,
      "centrality": 0,
      "betweenness": 0.0012426298447932016,
      "eigenvector": 0.014300992814507816,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.07132826856004751,
      "inferred_type": "model"
    },
    {
      "id": "VirTex",
      "type": "model",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "VirTex"
      ],
      "description": "A model that uses natural language supervision for image representation learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 45,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "ICMLM",
      "type": "model",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "ICMLM"
      ],
      "description": "A model that employs masked language modeling for learning image representations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 46,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "ConVIRT",
      "type": "model",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "ConVIRT"
      ],
      "description": "A model that uses contrastive objectives to learn image representations from text.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 47,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "Dai & Le",
      "type": "people",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Dai & Le"
      ],
      "description": "Researchers who contributed to advancements in pre-training methods in NLP.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 48,
      "importance": 0.0,
      "inferred_type": "people"
    },
    {
      "id": "Peters et al.",
      "type": "people",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Peters et al."
      ],
      "description": "Researchers who contributed to advancements in pre-training methods in NLP.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 49,
      "importance": 0.0,
      "inferred_type": "people"
    },
    {
      "id": "OpenAI",
      "type": "organization",
      "documents": [
        "clip",
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 3,
      "aliases": [
        "OpenAI"
      ],
      "description": "The organization behind the development of CLIP and other AI models.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0036769487557443857,
      "eigenvector": 0.08145360671628778,
      "in_degree": 1,
      "out_degree": 3,
      "total_degree": 4,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.166072701867788,
      "inferred_type": "organization"
    },
    {
      "id": "400 million (image, text) pairs",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "400 million (image, text) pairs"
      ],
      "description": "A dataset used for training the model, consisting of image and text pairs collected from the internet.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 50,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "OCR",
      "type": "task",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "OCR"
      ],
      "description": "Optical Character Recognition, a task in computer vision.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 51,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "geo-localization",
      "type": "task",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "geo-localization"
      ],
      "description": "A task in computer vision that involves determining the geographic location of an image.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 52,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "fine-grained object classification",
      "type": "task",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "fine-grained object classification"
      ],
      "description": "A task in computer vision that involves classifying objects into very specific categories.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 53,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "zero-shot transfer",
      "type": "method",
      "documents": [
        "clip",
        "gpt3_language_models"
      ],
      "mentions": 2,
      "aliases": [
        "zero-shot transfer"
      ],
      "description": "A method that allows a model to perform tasks without specific training on those tasks.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 3.323795485418654e-05,
      "eigenvector": 0.012365637944661546,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.044206869305222815,
      "inferred_type": "method"
    },
    {
      "id": "JFT-300M",
      "type": "dataset",
      "documents": [
        "clip",
        "deit",
        "reformer",
        "swin_transformer",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "JFT-300M"
      ],
      "description": "A dataset with noisy labels used for training models.",
      "pagerank": 0.004601464174208732,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.030552754895574886,
      "in_degree": 3,
      "out_degree": 0,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.17094544616896928,
      "inferred_type": "dataset"
    },
    {
      "id": "MS-COCO",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "MS-COCO"
      ],
      "description": "A high-quality crowd-labeled dataset for image captioning and object detection.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 55,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "Visual Genome",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Visual Genome"
      ],
      "description": "A dataset containing images and their associated descriptions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 56,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "GPT",
      "type": "architecture",
      "documents": [
        "clip",
        "clip"
      ],
      "mentions": 2,
      "aliases": [
        "GPT-1",
        "GPT"
      ],
      "description": "Generative Pre-trained Transformer, a family of language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 57,
      "importance": 0.024999999999999998,
      "inferred_type": "architecture"
    },
    {
      "id": "Li et al. (2017)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Li et al. (2017)"
      ],
      "description": "Researchers who achieved 11.5% accuracy on ImageNet in a zero-shot setting.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 58,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Xie et al. (2020)",
      "type": "person",
      "documents": [
        "clip",
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "Xie et al. (2020)"
      ],
      "description": "Researchers who established the current state of the art with 88.4% accuracy.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 59,
      "importance": 0.075,
      "inferred_type": "person"
    },
    {
      "id": "Mahajan et al. (2018)",
      "type": "person",
      "documents": [
        "clip",
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "Mahajan et al. (2018)"
      ],
      "description": "Researchers who demonstrated the effectiveness of predicting hashtags on Instagram images.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 60,
      "importance": 0.075,
      "inferred_type": "person"
    },
    {
      "id": "Kolesnikov et al. (2019)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Kolesnikov et al. (2019)"
      ],
      "description": "Researchers who showed large gains on transfer benchmarks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 61,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Dosovitskiy et al. (2020)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Dosovitskiy et al. (2020)"
      ],
      "description": "Researchers who contributed to the understanding of transfer performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 62,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Hestness et al. (2017)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Hestness et al. (2017)"
      ],
      "description": "Researchers who studied the relationship between compute and transfer performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 63,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Kaplan et al. (2020)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Kaplan et al. (2020)"
      ],
      "description": "Researchers who analyzed the predictability of transfer performance based on compute.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 64,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "McCann et al. (2017)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "McCann et al. (2017)"
      ],
      "description": "Researchers who discussed improvements in deep contextual representation learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 65,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "WIT",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "WIT"
      ],
      "description": "WebImageText dataset used for training CLIP, consisting of image-text pairs.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.008190087586504509,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.012173733757226562,
      "inferred_type": "dataset"
    },
    {
      "id": "Vision Transformer",
      "type": "architecture",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Vision Transformer"
      ],
      "description": "An architecture used for image encoding, based on transformer models.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 66,
      "importance": 0.01722051862198086,
      "inferred_type": "architecture"
    },
    {
      "id": "EfficientNet",
      "type": "architecture",
      "documents": [
        "clip",
        "swin_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "EfficientNet"
      ],
      "description": "A family of convolutional neural networks that optimize accuracy and efficiency.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.014500845684952733,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 67,
      "importance": 0.04541076240407216,
      "inferred_type": "architecture"
    },
    {
      "id": "WebText",
      "type": "dataset",
      "documents": [
        "clip",
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 3,
      "aliases": [
        "WebText"
      ],
      "description": "A dataset used to train the GPT-2 model.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.091371125441119,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 43,
      "importance": 0.10953763007265693,
      "inferred_type": "dataset"
    },
    {
      "id": "Zhang et al. (2020)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Zhang et al. (2020)"
      ],
      "description": "Researchers who adapted contrastive representation learning for medical imaging.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 68,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Oord et al. (2018)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Oord et al. (2018)"
      ],
      "description": "Researchers who popularized the InfoNCE loss for contrastive representation learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 69,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Chen et al. (2020)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Chen et al. (2020)"
      ],
      "description": "Researchers who explored generative models and contrastive models in representation learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 70,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Sohn (2016)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Sohn (2016)"
      ],
      "description": "Introduced the multi-class N-pair loss in deep metric learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 71,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "Tian et al. (2019)",
      "type": "person",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Tian et al. (2019)"
      ],
      "description": "Researchers who found that contrastive objectives can learn better representations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 72,
      "importance": 0.0,
      "inferred_type": "person"
    },
    {
      "id": "ResNet-101",
      "type": "architecture",
      "documents": [
        "clip",
        "deit",
        "reformer",
        "swin_transformer",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "ResNet-101",
        "ResNet"
      ],
      "description": "A deeper version of ResNet-50 with 101 layers.",
      "pagerank": 0.00473218254207593,
      "centrality": 0,
      "betweenness": 0.004033805167779415,
      "eigenvector": 0.0533727962812657,
      "in_degree": 3,
      "out_degree": 3,
      "total_degree": 6,
      "clustering_coefficient": 0.2,
      "community": 67,
      "importance": 0.2525150673902393,
      "inferred_type": "architecture"
    },
    {
      "id": "Vision Transformer (ViT)",
      "type": "architecture",
      "documents": [
        "clip",
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "Vision Transformer (ViT)"
      ],
      "description": "A transformer-based model for image classification.",
      "pagerank": 0.02482473536051485,
      "centrality": 0,
      "betweenness": 0.00946333347784618,
      "eigenvector": 0.14006568537652173,
      "in_degree": 18,
      "out_degree": 11,
      "total_degree": 29,
      "clustering_coefficient": 0.013227513227513227,
      "community": 44,
      "importance": 0.7046706232838295,
      "inferred_type": "architecture"
    },
    {
      "id": "Adam optimizer",
      "type": "algorithm",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Adam optimizer"
      ],
      "description": "An optimization algorithm that computes adaptive learning rates for each parameter.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 73,
      "importance": 0.0,
      "inferred_type": "algorithm"
    },
    {
      "id": "cross_entropy_loss",
      "type": "metric",
      "documents": [
        "clip",
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 3,
      "aliases": [
        "cross_entropy_loss",
        "cross-entropy loss"
      ],
      "description": "A loss function used for classification tasks.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.008190087586504509,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.062173733757226554,
      "inferred_type": "metric"
    },
    {
      "id": "MOTIVATION",
      "type": "section",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "MOTIVATION"
      ],
      "description": "A section discussing the rationale behind zero-shot transfer.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 74,
      "importance": 0.0,
      "inferred_type": "section"
    },
    {
      "id": "zero-shot learning",
      "type": "method",
      "documents": [
        "clip",
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 3,
      "aliases": [
        "zero-shot learning",
        "Zero-Shot Learning"
      ],
      "description": "A method that generalizes to unseen object categories without additional training.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.008190087586504509,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.062173733757226554,
      "inferred_type": "method"
    },
    {
      "id": "temperature parameter (τ)",
      "type": "parameter",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "temperature parameter (τ)"
      ],
      "description": "A parameter used to scale logits in the softmax function.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 75,
      "importance": 0.0,
      "inferred_type": "parameter"
    },
    {
      "id": "V100 GPU",
      "type": "hardware",
      "documents": [
        "clip",
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 3,
      "aliases": [
        "V100 GPU"
      ],
      "description": "A type of GPU used for high-performance computing.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.09634581048692867,
      "inferred_type": "hardware"
    },
    {
      "id": "mixed-precision training",
      "type": "method",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "mixed-precision training"
      ],
      "description": "A training technique that uses both 16-bit and 32-bit floating-point types.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 76,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "gradient checkpointing",
      "type": "method",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "gradient checkpointing"
      ],
      "description": "A technique to save memory during training by storing only a subset of activations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 77,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "CIFAR-10",
      "type": "dataset",
      "documents": [
        "clip",
        "deit",
        "deit",
        "reformer",
        "reformer",
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 7,
      "aliases": [
        "CIFAR-10",
        "CIFAR-100"
      ],
      "description": "A dataset commonly used for image classification tasks.",
      "pagerank": 0.003066973796123852,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.18828092355959203,
      "inferred_type": "dataset"
    },
    {
      "id": "TinyImages",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "TinyImages"
      ],
      "description": "A dataset from which CIFAR-10 is derived.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 78,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "SVHN",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "SVHN"
      ],
      "description": "A dataset for street number transcription from Google Street View images.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 79,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "Visual N-Grams",
      "type": "method",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Visual N-Grams"
      ],
      "description": "A method for zero-shot transfer in image classification.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.00021947888667302523,
      "eigenvector": 0.008397488013043266,
      "in_degree": 1,
      "out_degree": 2,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.024980002200902747,
      "inferred_type": "method"
    },
    {
      "id": "hypernetwork",
      "type": "architecture",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "hypernetwork"
      ],
      "description": "A network that generates weights for another network based on input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 80,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "Inception-V4",
      "type": "architecture",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Inception-V4"
      ],
      "description": "A deep learning model for image classification.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 81,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "Jelinek-Mercer smoothing",
      "type": "method",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Jelinek-Mercer smoothing"
      ],
      "description": "A technique used to optimize probabilities in language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 82,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Szegedy et al. (2016)",
      "type": "people",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Szegedy et al. (2016)"
      ],
      "description": "Researchers who contributed to the development of Inception-V4.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 83,
      "importance": 0.0,
      "inferred_type": "people"
    },
    {
      "id": "Liu et al. (2018)",
      "type": "people",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Liu et al. (2018)"
      ],
      "description": "Researchers who identified task learning as a side-effect in language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 84,
      "importance": 0.0,
      "inferred_type": "people"
    },
    {
      "id": "Lei Ba et al. (2015)",
      "type": "people",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Lei Ba et al. (2015)"
      ],
      "description": "Researchers who introduced a zero-shot image classifier.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 85,
      "importance": 0.0,
      "inferred_type": "people"
    },
    {
      "id": "Elhoseiny et al. (2013)",
      "type": "people",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Elhoseiny et al. (2013)"
      ],
      "description": "Researchers associated with early work on zero-shot learning.",
      "pagerank": 0.0017698331190785556,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0013197082106961389,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.013250322116301928,
      "inferred_type": "people"
    },
    {
      "id": "Flowers102",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Flowers102"
      ],
      "description": "An image classification dataset focused on flower species.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 86,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "GTSRB",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "GTSRB"
      ],
      "description": "German Traffic Sign Recognition Benchmark dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 87,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "SUN",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "SUN"
      ],
      "description": "A dataset used for scene recognition.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.008190087586504509,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.012173733757226562,
      "inferred_type": "dataset"
    },
    {
      "id": "Yahoo",
      "type": "dataset",
      "documents": [
        "clip"
      ],
      "mentions": 1,
      "aliases": [
        "Yahoo"
      ],
      "description": "A dataset used for image classification.",
      "pagerank": 0.0014597569779153515,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.008190087586504509,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.012173733757226562,
      "inferred_type": "dataset"
    },
    {
      "id": "VTAB",
      "type": "dataset",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "VTAB"
      ],
      "description": "A suite of 19 tasks for evaluating transfer learning in vision.",
      "pagerank": 0.003262404067808005,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.024670123613944484,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.09669667225103443,
      "inferred_type": "dataset"
    },
    {
      "id": "Alexey Dosovitskiy",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Alexey Dosovitskiy"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Lucas Beyer",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Lucas Beyer"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Alexander Kolesnikov",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Alexander Kolesnikov"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Dirk Weissenborn",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Dirk Weissenborn"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Xiaohua Zhai",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Xiaohua Zhai"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Thomas Unterthiner",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Thomas Unterthiner"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Mostafa Dehghani",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Mostafa Dehghani"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Matthias Minderer",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Matthias Minderer"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Georg Heigold",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Georg Heigold"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Sylvain Gelly",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Sylvain Gelly"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "Neil Houlsby",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Neil Houlsby"
      ],
      "description": "One of the authors of the paper and a researcher at Google Research.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "person"
    },
    {
      "id": "ImageNet-21k",
      "type": "dataset",
      "documents": [
        "deit",
        "reformer",
        "swin_transformer",
        "swin_transformer",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "ImageNet-21k",
        "ImageNet-1K",
        "ImageNet-22K"
      ],
      "description": "A large dataset used for pre-training the Vision Transformer.",
      "pagerank": 0.010392260613898962,
      "centrality": 0,
      "betweenness": 0.0010919752015954218,
      "eigenvector": 0.046504983137100546,
      "in_degree": 6,
      "out_degree": 3,
      "total_degree": 9,
      "clustering_coefficient": 0.09523809523809523,
      "community": 44,
      "importance": 0.29511652418469037,
      "inferred_type": "dataset"
    },
    {
      "id": "MLP Head",
      "type": "Architecture Component",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "MLP Head"
      ],
      "description": "A multi-layer perceptron used as a classification head in the ViT.",
      "pagerank": 0.003066973796123852,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.08828092355959202,
      "inferred_type": "Architecture Component"
    },
    {
      "id": "Transformer Encoder",
      "type": "Architecture Component",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Transformer Encoder"
      ],
      "description": "A component of the ViT that processes input sequences using self-attention.",
      "pagerank": 0.003066973796123852,
      "centrality": 0,
      "betweenness": 0.002025979797103961,
      "eigenvector": 0.027280234848251737,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.12339110609641057,
      "inferred_type": "Architecture Component"
    },
    {
      "id": "Multi-Head Self-Attention (MSA)",
      "type": "Architecture Component",
      "documents": [
        "deit",
        "vision_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "Multi-Head Self-Attention (MSA)"
      ],
      "description": "A mechanism in transformers that allows the model to focus on different parts of the input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 89,
      "importance": 0.024999999999999998,
      "inferred_type": "Architecture Component"
    },
    {
      "id": "Position Embeddings",
      "type": "Architecture Component",
      "documents": [
        "deit",
        "longformer",
        "reformer"
      ],
      "mentions": 3,
      "aliases": [
        "Position Embeddings"
      ],
      "description": "Learnable embeddings added to retain positional information in the input sequence.",
      "pagerank": 0.001637751006231779,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017124855898013484,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.07297746063310764,
      "inferred_type": "Architecture Component"
    },
    {
      "id": "Self-Supervised Learning",
      "type": "Learning Method",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Self-Supervised Learning"
      ],
      "description": "A method of training models using unlabeled data.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.038980736521023074,
      "inferred_type": "Learning Method"
    },
    {
      "id": "ICLR 2021",
      "type": "Conference",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "ICLR 2021"
      ],
      "description": "The conference where the paper was published.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 90,
      "importance": 0.049999999999999996,
      "inferred_type": "Conference"
    },
    {
      "id": "Touvron et al. (2019)",
      "type": "Citation",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Touvron et al. (2019)"
      ],
      "description": "A referenced work discussing fine-tuning and higher resolution.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 91,
      "importance": 0.049999999999999996,
      "inferred_type": "Citation"
    },
    {
      "id": "Sun et al. (2017)",
      "type": "Citation",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Sun et al. (2017)"
      ],
      "description": "A referenced work studying CNN performance scaling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 92,
      "importance": 0.049999999999999996,
      "inferred_type": "Citation"
    },
    {
      "id": "Kolesnikov et al. (2020)",
      "type": "Citation",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Kolesnikov et al. (2020)"
      ],
      "description": "A referenced work performing empirical exploration of CNN transfer learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 93,
      "importance": 0.049999999999999996,
      "inferred_type": "Citation"
    },
    {
      "id": "Djolonga et al. (2020)",
      "type": "Citation",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Djolonga et al. (2020)"
      ],
      "description": "A referenced work related to CNN transfer learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 94,
      "importance": 0.049999999999999996,
      "inferred_type": "Citation"
    },
    {
      "id": "Vaswani et al. (2017)",
      "type": "Citation",
      "documents": [
        "deit",
        "longformer",
        "reformer",
        "transformer_xl",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "Vaswani et al. (2017)"
      ],
      "description": "The original paper introducing the Transformer architecture.",
      "pagerank": 0.004913647094008437,
      "centrality": 0,
      "betweenness": 0.002532677967571316,
      "eigenvector": 0.033478557179125644,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.21122768127869235,
      "inferred_type": "Citation"
    },
    {
      "id": "GELU",
      "type": "Activation Function",
      "documents": [
        "deit",
        "reformer",
        "swin_transformer",
        "vision_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "GELU"
      ],
      "description": "An activation function used in the MLP layers.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 95,
      "importance": 0.075,
      "inferred_type": "Activation Function"
    },
    {
      "id": "ILSVRC-2012 ImageNet",
      "type": "dataset",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "ILSVRC-2012 ImageNet"
      ],
      "description": "A dataset with 1k classes and 1.3M images used for image classification.",
      "pagerank": 0.004090206388565495,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.007311220754740907,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.09997884662417969,
      "inferred_type": "dataset"
    },
    {
      "id": "JFT",
      "type": "dataset",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "JFT"
      ],
      "description": "A dataset with 18k classes and 303M high-resolution images.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 96,
      "importance": 0.049999999999999996,
      "inferred_type": "dataset"
    },
    {
      "id": "Oxford-IIIT Pets",
      "type": "dataset",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Oxford-IIIT Pets"
      ],
      "description": "A dataset containing images of pets for classification.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 97,
      "importance": 0.049999999999999996,
      "inferred_type": "dataset"
    },
    {
      "id": "Oxford Flowers-102",
      "type": "dataset",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Oxford Flowers-102"
      ],
      "description": "A dataset containing images of flowers for classification.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 98,
      "importance": 0.049999999999999996,
      "inferred_type": "dataset"
    },
    {
      "id": "ViT-Base",
      "type": "model",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "ViT-Base"
      ],
      "description": "A Vision Transformer model variant with 12 layers and 86M parameters.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.002986333398558319,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.0560785952057862,
      "inferred_type": "model"
    },
    {
      "id": "ViT-Large",
      "type": "model",
      "documents": [
        "deit",
        "deit",
        "reformer",
        "reformer",
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 6,
      "aliases": [
        "ViT-Huge",
        "ViT-Large"
      ],
      "description": "A Vision Transformer model variant with 24 layers and 307M parameters.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.002986333398558319,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.1310785952057862,
      "inferred_type": "model"
    },
    {
      "id": "Adam",
      "type": "optimizer",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Adam"
      ],
      "description": "An optimization algorithm used for training models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 99,
      "importance": 0.049999999999999996,
      "inferred_type": "optimizer"
    },
    {
      "id": "SGD",
      "type": "optimizer",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "SGD"
      ],
      "description": "Stochastic Gradient Descent, an optimization algorithm used for fine-tuning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005547049729585245,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.0571417702426477,
      "inferred_type": "optimizer"
    },
    {
      "id": "Noisy Student",
      "type": "model",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Noisy Student"
      ],
      "description": "A large EfficientNet trained using semi-supervised learning.",
      "pagerank": 0.0033261607499860857,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00022775536114470586,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.08735641121552246,
      "inferred_type": "model"
    },
    {
      "id": "Big Transfer (BiT)",
      "type": "model",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Big Transfer (BiT)"
      ],
      "description": "A method for supervised transfer learning with large ResNets.",
      "pagerank": 0.0013446745275661593,
      "centrality": 0,
      "betweenness": 0.00016799618486083411,
      "eigenvector": 0.002878797802708012,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.06567802960119432,
      "inferred_type": "model"
    },
    {
      "id": "Polyak & Juditsky (1992)",
      "type": "publication",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Polyak & Juditsky (1992)"
      ],
      "description": "A paper discussing averaging techniques used in model training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 101,
      "importance": 0.049999999999999996,
      "inferred_type": "publication"
    },
    {
      "id": "ViT-L/16",
      "type": "model",
      "documents": [
        "deit",
        "deit",
        "reformer",
        "segment_anything",
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 6,
      "aliases": [
        "ViT-L/16",
        "ViT-B/16",
        "ViT-L/14"
      ],
      "description": "Vision Transformer model pre-trained on ImageNet-21k",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.016862180491584104,
      "in_degree": 0,
      "out_degree": 5,
      "total_degree": 5,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.15619449931183885,
      "inferred_type": "model"
    },
    {
      "id": "TPUv3",
      "type": "hardware",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "TPUv3"
      ],
      "description": "Cloud TPU version 3 used for training",
      "pagerank": 0.0013446745275661593,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0026510424415633065,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.058415748691442386,
      "inferred_type": "hardware"
    },
    {
      "id": "BiT",
      "type": "method",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "BiT"
      ],
      "description": "State-of-the-art method based on ResNet co-trained on ImageNet and YouTube",
      "pagerank": 0.0033261607499860857,
      "centrality": 0,
      "betweenness": 0.001013396340934709,
      "eigenvector": 0.006110386642775108,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.10384799168583798,
      "inferred_type": "method"
    },
    {
      "id": "VIVI",
      "type": "method",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "VIVI"
      ],
      "description": "ResNet co-trained on ImageNet and YouTube",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 102,
      "importance": 0.049999999999999996,
      "inferred_type": "method"
    },
    {
      "id": "S4L",
      "type": "method",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "S4L"
      ],
      "description": "Supervised plus semi-supervised learning method on ImageNet",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 103,
      "importance": 0.049999999999999996,
      "inferred_type": "method"
    },
    {
      "id": "ViT-H/14",
      "type": "model",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "ViT-H/14"
      ],
      "description": "Vision Transformer model that outperforms BiT-R152x4 on Natural and Structured tasks",
      "pagerank": 0.0051219607154367125,
      "centrality": 0,
      "betweenness": 0.0007424347524494927,
      "eigenvector": 0.001448842488965161,
      "in_degree": 2,
      "out_degree": 2,
      "total_degree": 4,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.13058870928906605,
      "inferred_type": "model"
    },
    {
      "id": "weight decay",
      "type": "hyperparameter",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "weight decay"
      ],
      "description": "Regularization parameter used to optimize model performance",
      "pagerank": 0.002488304362282734,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005882631281630402,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.07424877391793488,
      "inferred_type": "hyperparameter"
    },
    {
      "id": "dropout",
      "type": "hyperparameter",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "dropout"
      ],
      "description": "Regularization technique used to prevent overfitting",
      "pagerank": 0.002488304362282734,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005882631281630402,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.07424877391793488,
      "inferred_type": "hyperparameter"
    },
    {
      "id": "label smoothing",
      "type": "hyperparameter",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "label smoothing"
      ],
      "description": "Regularization technique used to improve model generalization",
      "pagerank": 0.002488304362282734,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005882631281630402,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.07424877391793488,
      "inferred_type": "hyperparameter"
    },
    {
      "id": "ViT-B/32",
      "type": "model",
      "documents": [
        "deit",
        "deit",
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "ViT-B/32",
        "ViT-L/32"
      ],
      "description": "Vision Transformer model with specific architecture",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 104,
      "importance": 0.075,
      "inferred_type": "model"
    },
    {
      "id": "ViT-B",
      "type": "model",
      "documents": [
        "deit",
        "vision_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "ViT-B"
      ],
      "description": "Vision Transformer model with all hidden dimensions halved",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 105,
      "importance": 0.024999999999999998,
      "inferred_type": "model"
    },
    {
      "id": "Hybrid models",
      "type": "architecture",
      "documents": [
        "deit"
      ],
      "mentions": 1,
      "aliases": [
        "Hybrid models"
      ],
      "description": "Models combining ResNet and Vision Transformer architectures",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005882631281630402,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.007281099197066708,
      "inferred_type": "architecture"
    },
    {
      "id": "ICLR2021",
      "type": "conference",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "ICLR2021"
      ],
      "description": "Conference where the paper was published",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 106,
      "importance": 0.049999999999999996,
      "inferred_type": "conference"
    },
    {
      "id": "Attention",
      "type": "mechanism",
      "documents": [
        "deit"
      ],
      "mentions": 1,
      "aliases": [
        "Attention"
      ],
      "description": "Mechanism used in Vision Transformers to integrate information across the image",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 107,
      "importance": 0.0,
      "inferred_type": "mechanism"
    },
    {
      "id": "Principal components",
      "type": "concept",
      "documents": [
        "deit"
      ],
      "mentions": 1,
      "aliases": [
        "Principal components"
      ],
      "description": "Statistical method used to analyze learned embeddings",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 108,
      "importance": 0.0,
      "inferred_type": "concept"
    },
    {
      "id": "self-attention",
      "type": "method",
      "documents": [
        "deit",
        "longformer",
        "swin_transformer",
        "transformer_xl",
        "vision_transformer"
      ],
      "mentions": 5,
      "aliases": [
        "self-attention"
      ],
      "description": "A mechanism that allows the model to weigh the importance of different parts of the input data.",
      "pagerank": 0.0035554805464736244,
      "centrality": 0,
      "betweenness": 0.004540232376658285,
      "eigenvector": 0.045254842298397956,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.2267389306232822,
      "inferred_type": "method"
    },
    {
      "id": "CNN",
      "type": "architecture",
      "documents": [
        "deit",
        "reformer",
        "swin_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "CNN"
      ],
      "description": "Convolutional Neural Networks, commonly used for image processing tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06398073652102307,
      "inferred_type": "architecture"
    },
    {
      "id": "masked language modeling",
      "type": "method",
      "documents": [
        "deit"
      ],
      "mentions": 1,
      "aliases": [
        "masked language modeling"
      ],
      "description": "A self-supervised learning task used in models like BERT.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 109,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "self-supervised pre-training",
      "type": "method",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "self-supervised pre-training"
      ],
      "description": "A training approach where the model learns from unlabeled data.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 110,
      "importance": 0.05483870967741935,
      "inferred_type": "method"
    },
    {
      "id": "attention distance",
      "type": "metric",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "attention distance"
      ],
      "description": "A measure of how far information is integrated across the image based on attention weights.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 111,
      "importance": 0.024999999999999998,
      "inferred_type": "metric"
    },
    {
      "id": "Andreas Steiner",
      "type": "person",
      "documents": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "Andreas Steiner"
      ],
      "description": "A colleague at Google who contributed to the infrastructure.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 112,
      "importance": 0.049999999999999996,
      "inferred_type": "person"
    },
    {
      "id": "Joan Puigcerver",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Joan Puigcerver"
      ],
      "description": "A colleague at Google who assisted with large-scale training infrastructure.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 113,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Maxim Neumann",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Maxim Neumann"
      ],
      "description": "A colleague at Google who helped with large-scale training infrastructure.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 114,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Dmitry Lepikhin",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Dmitry Lepikhin"
      ],
      "description": "A colleague at Google involved in discussions related to the work.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 115,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Aravindh Mahendran",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Aravindh Mahendran"
      ],
      "description": "A colleague at Google involved in discussions related to the work.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 116,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Daniel Keysers",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Daniel Keysers"
      ],
      "description": "A colleague at Google involved in discussions related to the work.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 117,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Mario Lucic",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Mario Lucic"
      ],
      "description": "A colleague at Google involved in discussions related to the work.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 118,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Colin Raffel",
      "type": "person",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Colin Raffel"
      ],
      "description": "A colleague at Google involved in discussions related to the work.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 119,
      "importance": 0.024999999999999998,
      "inferred_type": "person"
    },
    {
      "id": "Devlin et al. (2019)",
      "type": "reference",
      "documents": [
        "deit",
        "longformer",
        "reformer"
      ],
      "mentions": 3,
      "aliases": [
        "Devlin et al. (2019)"
      ],
      "description": "Authors of the BERT paper, which introduced masked language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 120,
      "importance": 0.049999999999999996,
      "inferred_type": "reference"
    },
    {
      "id": "Chen et al. (2020b)",
      "type": "reference",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Chen et al. (2020b)"
      ],
      "description": "Authors of a paper on contrastive pre-training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 121,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "He et al. (2020)",
      "type": "reference",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "He et al. (2020)"
      ],
      "description": "Authors of a paper on unsupervised visual representation learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 122,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "Bachman et al. (2019)",
      "type": "reference",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Bachman et al. (2019)"
      ],
      "description": "Authors of a paper on maximizing mutual information across views.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 123,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "Hénaff et al. (2020)",
      "type": "reference",
      "documents": [
        "deit"
      ],
      "mentions": 1,
      "aliases": [
        "Hénaff et al. (2020)"
      ],
      "description": "Authors of a paper on contrastive learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 124,
      "importance": 0.0,
      "inferred_type": "reference"
    },
    {
      "id": "Carion et al. (2020)",
      "type": "reference",
      "documents": [
        "deit",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Carion et al. (2020)"
      ],
      "description": "Authors of a paper on end-to-end object detection with transformers.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 125,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "NLP",
      "type": "field",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "NLP"
      ],
      "description": "Natural Language Processing, a field of AI focused on the interaction between computers and human language.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.012792387396584783,
      "in_degree": 0,
      "out_degree": 2,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.03998864683330615,
      "inferred_type": "field"
    },
    {
      "id": "few-shot learning",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "gpt3_language_models",
        "gpt3",
        "gpt3",
        "palm"
      ],
      "mentions": 5,
      "aliases": [
        "one-shot learning",
        "few-shot learning"
      ],
      "description": "A learning paradigm where a model learns to perform a task with very few examples.",
      "pagerank": 0.0035560046933113196,
      "centrality": 0,
      "betweenness": 9.989450562154976e-05,
      "eigenvector": 0.0793984399703257,
      "in_degree": 3,
      "out_degree": 1,
      "total_degree": 4,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.18420174919715498,
      "inferred_type": "method"
    },
    {
      "id": "pre-training",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "reformer"
      ],
      "mentions": 2,
      "aliases": [
        "Pre-training",
        "pre-training"
      ],
      "description": "The initial training phase of a model on a large corpus of text.",
      "pagerank": 0.005067357746986259,
      "centrality": 0,
      "betweenness": 0.0011136521286742391,
      "eigenvector": 0.03576128197120354,
      "in_degree": 2,
      "out_degree": 2,
      "total_degree": 4,
      "clustering_coefficient": 0.3333333333333333,
      "community": 44,
      "importance": 0.12428924250315905,
      "inferred_type": "method"
    },
    {
      "id": "fine-tuning",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "reformer"
      ],
      "mentions": 3,
      "aliases": [
        "fine-tuning",
        "Fine-tuning"
      ],
      "description": "The process of adapting a pre-trained model to a specific task using a smaller dataset.",
      "pagerank": 0.006960874140921719,
      "centrality": 0,
      "betweenness": 0.00311876788346484,
      "eigenvector": 0.03528371956233,
      "in_degree": 3,
      "out_degree": 1,
      "total_degree": 4,
      "clustering_coefficient": 0.3333333333333333,
      "community": 44,
      "importance": 0.20088220368245463,
      "inferred_type": "method"
    },
    {
      "id": "language tasks",
      "type": "concept",
      "documents": [
        "gpt3_language_models"
      ],
      "mentions": 1,
      "aliases": [
        "language tasks"
      ],
      "description": "Various tasks that involve understanding or generating human language.",
      "pagerank": 0.0016378199350923885,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00201016495739925,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.011864207648708866,
      "inferred_type": "component"
    },
    {
      "id": "translation",
      "type": "task",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "translation"
      ],
      "description": "The task of converting text from one language to another.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.07134581048692867,
      "inferred_type": "task"
    },
    {
      "id": "question-answering",
      "type": "task",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "question-answering"
      ],
      "description": "The task of providing answers to questions based on a given context.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.07134581048692867,
      "inferred_type": "task"
    },
    {
      "id": "cloze tasks",
      "type": "task",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "cloze tasks"
      ],
      "description": "Tasks that involve filling in the blanks in a sentence.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.07134581048692867,
      "inferred_type": "task"
    },
    {
      "id": "3-digit arithmetic",
      "type": "task",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "3-digit arithmetic"
      ],
      "description": "A task that involves performing arithmetic operations with three-digit numbers.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 127,
      "importance": 0.024999999999999998,
      "inferred_type": "task"
    },
    {
      "id": "Common Crawl",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "llama"
      ],
      "mentions": 3,
      "aliases": [
        "Common Crawl"
      ],
      "description": "A dataset used for training language models, consisting of web-crawled data.",
      "pagerank": 0.008060344444078802,
      "centrality": 0,
      "betweenness": 0.0011109425127893869,
      "eigenvector": 0.12803366782756423,
      "in_degree": 4,
      "out_degree": 1,
      "total_degree": 5,
      "clustering_coefficient": 0.5,
      "community": 43,
      "importance": 0.23032577022035994,
      "inferred_type": "dataset"
    },
    {
      "id": "SuperGLUE",
      "type": "benchmark",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "SuperGLUE"
      ],
      "description": "A benchmark for evaluating the performance of NLP models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 128,
      "importance": 0.024999999999999998,
      "inferred_type": "benchmark"
    },
    {
      "id": "Johns Hopkins University",
      "type": "organization",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Johns Hopkins University"
      ],
      "description": "An academic institution associated with one of the authors.",
      "pagerank": 0.0017416670662853437,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.012799605729005766,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.042659720024149916,
      "inferred_type": "organization"
    },
    {
      "id": "parameters",
      "type": "metric",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "parameters"
      ],
      "description": "The number of adjustable weights in a neural network model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 129,
      "importance": 0.024999999999999998,
      "inferred_type": "metric"
    },
    {
      "id": "societal impacts",
      "type": "concept",
      "documents": [
        "gpt3_language_models"
      ],
      "mentions": 1,
      "aliases": [
        "societal impacts"
      ],
      "description": "The broader implications of language models like GPT-3 on society.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03442166417495598,
      "inferred_type": "concept"
    },
    {
      "id": "in-context learning",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "in-context learning"
      ],
      "description": "A meta-learning approach where a model uses contextual information to adapt to tasks without gradient updates.",
      "pagerank": 0.003067429014100938,
      "centrality": 0,
      "betweenness": 9.718488973669759e-05,
      "eigenvector": 0.07733738343783063,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.097278847368122,
      "inferred_type": "method"
    },
    {
      "id": "meta-learning",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "meta-learning"
      ],
      "description": "A learning paradigm where models develop a broad set of skills during training to adapt quickly to new tasks.",
      "pagerank": 0.008022251854755252,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.026572415221346752,
      "in_degree": 3,
      "out_degree": 0,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.1376387978204921,
      "inferred_type": "method"
    },
    {
      "id": "Natural Questions",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "llama"
      ],
      "mentions": 3,
      "aliases": [
        "Natural Questions"
      ],
      "description": "A benchmark dataset used to evaluate question answering systems.",
      "pagerank": 0.002303114363975885,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.1469472744105261,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 43,
      "importance": 0.1353090254014083,
      "inferred_type": "dataset"
    },
    {
      "id": "CoQA",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "CoQA"
      ],
      "description": "A conversational question answering dataset used to evaluate models on dialogue-based tasks.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.07134581048692867,
      "inferred_type": "dataset"
    },
    {
      "id": "TriviaQA",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "llama",
        "longformer"
      ],
      "mentions": 4,
      "aliases": [
        "TriviaQA"
      ],
      "description": "A dataset for question answering that includes trivia questions.",
      "pagerank": 0.00298705138600981,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.16918039604071003,
      "in_degree": 4,
      "out_degree": 0,
      "total_degree": 4,
      "clustering_coefficient": 0.16666666666666666,
      "community": 88,
      "importance": 0.18788373116466134,
      "inferred_type": "dataset"
    },
    {
      "id": "log loss",
      "type": "metric",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "log loss"
      ],
      "description": "A performance metric that correlates well with many downstream NLP tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 130,
      "importance": 0.024999999999999998,
      "inferred_type": "metric"
    },
    {
      "id": "RWC+19",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "RWC+19"
      ],
      "description": "A citation for a paper discussing in-context learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.012152647435049728,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.0348843257139064,
      "inferred_type": "reference"
    },
    {
      "id": "YdC+19",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "YdC+19"
      ],
      "description": "A citation for a paper discussing generalization in models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 131,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "MPL19",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "MPL19"
      ],
      "description": "A citation for a paper related to model performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 132,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "GSL+18",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "GSL+18"
      ],
      "description": "A citation for a paper discussing the performance of fine-tuned models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 133,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "NK19",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "NK19"
      ],
      "description": "A citation for a paper related to model evaluation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 134,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "KMH+20",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "KMH+20"
      ],
      "description": "A citation for a paper discussing the correlation of log loss with model performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 135,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "RNSS18",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "RNSS18"
      ],
      "description": "A citation for a paper discussing the early transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 136,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "DCLT18",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "DCLT18"
      ],
      "description": "A citation for a paper discussing the scaling of transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 137,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "SPP+19",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "SPP+19"
      ],
      "description": "A citation for a paper discussing improvements in transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 138,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "RSR+19",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "RSR+19"
      ],
      "description": "A citation for a paper discussing the scaling of language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 139,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "Tur20",
      "type": "reference",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Tur20"
      ],
      "description": "A citation for a paper discussing the latest advancements in transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 140,
      "importance": 0.024999999999999998,
      "inferred_type": "reference"
    },
    {
      "id": "ANLI",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "ANLI"
      ],
      "description": "A natural language inference dataset used to evaluate model performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 141,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "RACE",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "llama"
      ],
      "mentions": 3,
      "aliases": [
        "RACE"
      ],
      "description": "A reading comprehension dataset for evaluating language models.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.08896321491447962,
      "inferred_type": "dataset"
    },
    {
      "id": "QuAC",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "QuAC"
      ],
      "description": "A dataset for question answering in a conversational context.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 142,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "CommonCrawl",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "llama"
      ],
      "mentions": 3,
      "aliases": [
        "CommonCrawl"
      ],
      "description": "A web corpus used for training language models.",
      "pagerank": 0.002303114363975885,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.1469472744105261,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 43,
      "importance": 0.1353090254014083,
      "inferred_type": "dataset"
    },
    {
      "id": "Data Contamination",
      "type": "concept",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Data Contamination",
        "data contamination"
      ],
      "description": "The issue of training models on datasets that may include content from test datasets.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 143,
      "importance": 0.024999999999999998,
      "inferred_type": "concept"
    },
    {
      "id": "Bias and Fairness",
      "type": "concept",
      "documents": [
        "gpt3_language_models"
      ],
      "mentions": 1,
      "aliases": [
        "Bias and Fairness"
      ],
      "description": "Concerns regarding the ethical implications and societal impacts of AI models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 144,
      "importance": 0.0,
      "inferred_type": "concept"
    },
    {
      "id": "Books1",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3_language_models",
        "gpt3",
        "gpt3"
      ],
      "mentions": 4,
      "aliases": [
        "Books2",
        "Books1"
      ],
      "description": "An internet-based corpus used in training language models.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.091371125441119,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 43,
      "importance": 0.13453763007265696,
      "inferred_type": "dataset"
    },
    {
      "id": "Wikipedia",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "llama",
        "palm"
      ],
      "mentions": 4,
      "aliases": [
        "Wikipedia"
      ],
      "description": "An English-language encyclopedia used in training language models.",
      "pagerank": 0.0028408683113150565,
      "centrality": 0,
      "betweenness": 0.00028180005202462497,
      "eigenvector": 0.1786057752885821,
      "in_degree": 3,
      "out_degree": 1,
      "total_degree": 4,
      "clustering_coefficient": 0.3333333333333333,
      "community": 43,
      "importance": 0.19385141474281103,
      "inferred_type": "dataset"
    },
    {
      "id": "Sparse Transformer",
      "type": "architecture",
      "documents": [
        "gpt3_language_models",
        "gpt3",
        "longformer"
      ],
      "mentions": 3,
      "aliases": [
        "Sparse Transformer"
      ],
      "description": "An architecture that uses alternating dense and locally banded sparse attention patterns.",
      "pagerank": 0.001637751006231779,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.03359364697238931,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 88,
      "importance": 0.07981508155402475,
      "inferred_type": "architecture"
    },
    {
      "id": "Validation loss",
      "type": "metric",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Validation loss"
      ],
      "description": "A measure used to evaluate the performance of models during training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.05942166417495598,
      "inferred_type": "metric"
    },
    {
      "id": "Scaling Laws for Neural Language Models",
      "type": "research",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Scaling Laws for Neural Language Models"
      ],
      "description": "A study that analyzes the relationship between model size and performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 145,
      "importance": 0.024999999999999998,
      "inferred_type": "research"
    },
    {
      "id": "Microsoft",
      "type": "organization",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Microsoft"
      ],
      "description": "Provider of high-bandwidth cluster for training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 146,
      "importance": 0.024999999999999998,
      "inferred_type": "organization"
    },
    {
      "id": "LAMBADA",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "LAMBADA"
      ],
      "description": "A dataset testing long-range dependencies in text.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 147,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "Storycloze",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Storycloze"
      ],
      "description": "A dataset used for evaluating story completion tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 148,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "Winograd",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "Winograd"
      ],
      "description": "A dataset for evaluating commonsense reasoning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 149,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "ARC",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "ARC"
      ],
      "description": "A dataset for evaluating reasoning capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 150,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "OpenBookQA",
      "type": "dataset",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "OpenBookQA"
      ],
      "description": "A dataset for evaluating question answering.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 151,
      "importance": 0.024999999999999998,
      "inferred_type": "dataset"
    },
    {
      "id": "F1 similarity score",
      "type": "metric",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "F1 similarity score"
      ],
      "description": "A metric for evaluating model performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 152,
      "importance": 0.024999999999999998,
      "inferred_type": "metric"
    },
    {
      "id": "K",
      "type": "parameter",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "K"
      ],
      "description": "The number of examples drawn from the training set for few-shot learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.012476515865209847,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.035018791537395594,
      "inferred_type": "parameter"
    },
    {
      "id": "model parallelism",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "model parallelism"
      ],
      "description": "A technique used to distribute model training across multiple GPUs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 153,
      "importance": 0.024999999999999998,
      "inferred_type": "method"
    },
    {
      "id": "gradient noise scaling",
      "type": "method",
      "documents": [
        "gpt3_language_models",
        "gpt3"
      ],
      "mentions": 2,
      "aliases": [
        "gradient noise scaling"
      ],
      "description": "A technique used to guide the choice of batch size during training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 154,
      "importance": 0.024999999999999998,
      "inferred_type": "method"
    },
    {
      "id": "task-specific fine-tuning",
      "type": "Method",
      "documents": [
        "gpt3"
      ],
      "mentions": 1,
      "aliases": [
        "task-specific fine-tuning"
      ],
      "description": "The process of adapting a pre-trained model to a specific task using a labeled dataset.",
      "pagerank": 0.002090277655680173,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07125219467670522,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.04634581048692867,
      "inferred_type": "Method"
    },
    {
      "id": "human evaluators",
      "type": "People",
      "documents": [
        "gpt3"
      ],
      "mentions": 1,
      "aliases": [
        "human evaluators"
      ],
      "description": "Individuals who assess the quality of generated text.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 155,
      "importance": 0.0,
      "inferred_type": "People"
    },
    {
      "id": "F1 Score",
      "type": "metric",
      "documents": [
        "gpt3",
        "longformer"
      ],
      "mentions": 2,
      "aliases": [
        "F1 Score"
      ],
      "description": "A measure of a model's accuracy on a dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 156,
      "importance": 0.024999999999999998,
      "inferred_type": "metric"
    },
    {
      "id": "accuracy",
      "type": "metric",
      "documents": [
        "gpt3",
        "segment_anything",
        "vision_transformer"
      ],
      "mentions": 3,
      "aliases": [
        "accuracy"
      ],
      "description": "The ratio of correctly predicted instances to the total instances.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 110,
      "importance": 0.06722051862198086,
      "inferred_type": "metric"
    },
    {
      "id": "LLaMA-I",
      "type": "Language Model",
      "documents": [
        "llama",
        "llama",
        "segment_anything"
      ],
      "mentions": 3,
      "aliases": [
        "LLaMA",
        "LLaMA-I"
      ],
      "description": "A collection of foundation language models ranging from 7B to 65B parameters.",
      "pagerank": 0.006257493143608254,
      "centrality": 0,
      "betweenness": 0.008430066620422555,
      "eigenvector": 0.481711146752691,
      "in_degree": 6,
      "out_degree": 25,
      "total_degree": 31,
      "clustering_coefficient": 0.017094017094017096,
      "community": 43,
      "importance": 0.5815983013644597,
      "inferred_type": "Language Model"
    },
    {
      "id": "Chinchilla",
      "type": "Language Model",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Chinchilla"
      ],
      "description": "A large language model with 70B parameters, used for performance comparison.",
      "pagerank": 0.0016877980836513065,
      "centrality": 0,
      "betweenness": 3.1837986647012924e-05,
      "eigenvector": 0.08486128423377133,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.08201497395733845,
      "inferred_type": "Language Model"
    },
    {
      "id": "PaLM",
      "type": "Language Model",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "PaLM"
      ],
      "description": "A large language model with 540B parameters, used for performance comparison.",
      "pagerank": 0.0021763737628616884,
      "centrality": 0,
      "betweenness": 0.0038618800398855456,
      "eigenvector": 0.2771730372386443,
      "in_degree": 3,
      "out_degree": 16,
      "total_degree": 19,
      "clustering_coefficient": 0.006535947712418301,
      "community": 2,
      "importance": 0.29856816843129114,
      "inferred_type": "Language Model"
    },
    {
      "id": "MetaAI",
      "type": "Organization",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "MetaAI"
      ],
      "description": "The organization behind the development of LLaMA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 157,
      "importance": 0.0,
      "inferred_type": "Organization"
    },
    {
      "id": "Hoffmann et al. (2022)",
      "type": "Research Paper",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Hoffmann et al. (2022)"
      ],
      "description": "A study that discusses scaling laws for language models.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 158,
      "importance": 0.042220518621980854,
      "inferred_type": "Research Paper"
    },
    {
      "id": "C4",
      "type": "Dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "C4"
      ],
      "description": "A dataset used for training, consisting of web data filtered for quality.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "Dataset"
    },
    {
      "id": "Github",
      "type": "Dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Github"
      ],
      "description": "A dataset containing public GitHub repositories.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "Dataset"
    },
    {
      "id": "Books",
      "type": "Dataset",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Books"
      ],
      "description": "A dataset containing public domain books.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.06396321491447961,
      "inferred_type": "Dataset"
    },
    {
      "id": "ArXiv",
      "type": "Dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "ArXiv"
      ],
      "description": "A dataset containing scientific papers from arXiv.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "Dataset"
    },
    {
      "id": "StackExchange",
      "type": "Dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "StackExchange"
      ],
      "description": "A dataset containing questions and answers from Stack Exchange.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "Dataset"
    },
    {
      "id": "Transformer Architecture",
      "type": "Architecture",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Transformer Architecture"
      ],
      "description": "The neural network architecture used for training LLaMA.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0001625769530911298,
      "eigenvector": 0.08095623340969144,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.048240165855644425,
      "inferred_type": "Architecture"
    },
    {
      "id": "Performance Metrics",
      "type": "Metric",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Performance Metrics"
      ],
      "description": "Benchmarks used to evaluate the performance of language models.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "Metric"
    },
    {
      "id": "CCNet",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "CCNet"
      ],
      "description": "A dataset used for increasing consistency across papers.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 159,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "Stack Exchange",
      "type": "website",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Stack Exchange"
      ],
      "description": "A platform for high-quality questions and answers across various domains.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 160,
      "importance": 0.0,
      "inferred_type": "website"
    },
    {
      "id": "Google BigQuery",
      "type": "service",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Google BigQuery"
      ],
      "description": "A cloud-based data warehouse for querying large datasets.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 161,
      "importance": 0.0,
      "inferred_type": "service"
    },
    {
      "id": "Apache License",
      "type": "license",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Apache License"
      ],
      "description": "A permissive free software license.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 162,
      "importance": 0.0,
      "inferred_type": "license"
    },
    {
      "id": "BSD License",
      "type": "license",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "BSD License"
      ],
      "description": "A family of permissive free software licenses.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 163,
      "importance": 0.0,
      "inferred_type": "license"
    },
    {
      "id": "MIT License",
      "type": "license",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "MIT License"
      ],
      "description": "A permissive free software license.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 164,
      "importance": 0.0,
      "inferred_type": "license"
    },
    {
      "id": "Byte-Pair Encoding (BPE)",
      "type": "algorithm",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Byte-Pair Encoding (BPE)"
      ],
      "description": "A tokenization algorithm used for processing text.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 165,
      "importance": 0.0,
      "inferred_type": "algorithm"
    },
    {
      "id": "RMSNorm",
      "type": "normalization function",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "RMSNorm"
      ],
      "description": "A normalization function used to improve training stability.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 166,
      "importance": 0.0,
      "inferred_type": "normalization function"
    },
    {
      "id": "SwiGLU",
      "type": "activation function",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "SwiGLU"
      ],
      "description": "An activation function used to improve performance.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0493880006706006,
      "inferred_type": "activation function"
    },
    {
      "id": "Rotary Positional Embeddings (RoPE)",
      "type": "embedding technique",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Rotary Positional Embeddings (RoPE)"
      ],
      "description": "A technique for adding positional information to transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 167,
      "importance": 0.0,
      "inferred_type": "embedding technique"
    },
    {
      "id": "AdamW",
      "type": "optimizer",
      "documents": [
        "llama",
        "swin_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "AdamW"
      ],
      "description": "An optimization algorithm used for training models.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.03708866210407426,
      "inferred_type": "optimizer"
    },
    {
      "id": "CommonSense Reasoning Tasks",
      "type": "task",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "CommonSense Reasoning Tasks"
      ],
      "description": "A set of tasks for evaluating models on common sense reasoning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 168,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "Gopher",
      "type": "model",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Gopher"
      ],
      "description": "A large language model developed by DeepMind.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.1469472744105261,
      "in_degree": 1,
      "out_degree": 2,
      "total_degree": 3,
      "clustering_coefficient": 1.0,
      "community": 43,
      "importance": 0.10322358876685495,
      "inferred_type": "model"
    },
    {
      "id": "Table 2",
      "type": "table",
      "documents": [
        "llama",
        "llama"
      ],
      "mentions": 2,
      "aliases": [
        "Table 3",
        "Table 2"
      ],
      "description": "A table containing model sizes, architectures, and optimization hyper-parameters.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 169,
      "importance": 0.024999999999999998,
      "inferred_type": "table"
    },
    {
      "id": "OPT",
      "type": "model",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "OPT"
      ],
      "description": "Open-sourced language models compared with LLaMA.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "model"
    },
    {
      "id": "OPT-IML",
      "type": "model",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "OPT-IML"
      ],
      "description": "An instruction-tuned model compared with LLaMA.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "model"
    },
    {
      "id": "HumanEval",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "HumanEval"
      ],
      "description": "A benchmark for evaluating code generation from natural language.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 170,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "MBPP",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "MBPP"
      ],
      "description": "A benchmark for evaluating code generation from natural language.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 171,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "MATH",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "MATH"
      ],
      "description": "A dataset of mathematical problems for evaluation.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "dataset"
    },
    {
      "id": "GSM8k",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "GSM8k"
      ],
      "description": "A dataset of middle school mathematical problems.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "dataset"
    },
    {
      "id": "BoolQ",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "BoolQ"
      ],
      "description": "A common sense reasoning benchmark.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "dataset"
    },
    {
      "id": "Flan-PaLM",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Flan-PaLM"
      ],
      "description": "An instruction-tuned model benchmark.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "dataset"
    },
    {
      "id": "WinoGrande",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "WinoGrande"
      ],
      "description": "A common sense reasoning benchmark.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 172,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "ARCeasy",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "ARCeasy"
      ],
      "description": "A common sense reasoning benchmark.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 173,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "ARChallenge",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "ARChallenge"
      ],
      "description": "A common sense reasoning benchmark.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 174,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "Exact Match",
      "type": "metric",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Exact Match"
      ],
      "description": "A performance metric for evaluating question answering.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03896321491447961,
      "inferred_type": "metric"
    },
    {
      "id": "Zero-shot",
      "type": "evaluation setting",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Zero-shot"
      ],
      "description": "An evaluation setting where models are tested without prior examples.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 175,
      "importance": 0.0,
      "inferred_type": "evaluation setting"
    },
    {
      "id": "Few-shot",
      "type": "evaluation setting",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Few-shot"
      ],
      "description": "An evaluation setting where models are tested with a few examples.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 176,
      "importance": 0.0,
      "inferred_type": "evaluation setting"
    },
    {
      "id": "pass@",
      "type": "metric",
      "documents": [
        "llama",
        "llama"
      ],
      "mentions": 2,
      "aliases": [
        "pass@",
        "pass@1"
      ],
      "description": "A performance metric for evaluating code generation.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.06396321491447961,
      "inferred_type": "metric"
    },
    {
      "id": "MMLU",
      "type": "benchmark",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "MMLU"
      ],
      "description": "Massive multitask language understanding benchmark.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 177,
      "importance": 0.0,
      "inferred_type": "benchmark"
    },
    {
      "id": "LaMDA",
      "type": "model",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "LaMDA"
      ],
      "description": "A language model developed by Google, compared with LLaMA.",
      "pagerank": 0.0013620809641777182,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.06396321491447961,
      "inferred_type": "model"
    },
    {
      "id": "LLaMA-65B",
      "type": "model",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "LLaMA-65B"
      ],
      "description": "A large language model with 65 billion parameters.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.05833097352102553,
      "in_degree": 0,
      "out_degree": 3,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.038734366004454374,
      "inferred_type": "model"
    },
    {
      "id": "Gutenberg",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Gutenberg"
      ],
      "description": "A digital library of free eBooks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 178,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "Books3",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Books3"
      ],
      "description": "A dataset used in training language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 179,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "pass@100",
      "type": "metric",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "pass@100"
      ],
      "description": "A metric indicating the percentage of correct answers within 100 attempts.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 180,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "pass@80",
      "type": "metric",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "pass@80"
      ],
      "description": "A metric indicating the percentage of correct answers within 80 attempts.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 181,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "RealToxicityPrompts",
      "type": "benchmark",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "RealToxicityPrompts"
      ],
      "description": "A benchmark for evaluating the toxicity of language models.",
      "pagerank": 0.0014749613753555944,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.009166204499950481,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.012771664355625718,
      "inferred_type": "benchmark"
    },
    {
      "id": "PerspectiveAPI",
      "type": "API",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "PerspectiveAPI"
      ],
      "description": "An API used to evaluate the toxicity of generated text.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 182,
      "importance": 0.0,
      "inferred_type": "API"
    },
    {
      "id": "Chen et al. (2021)",
      "type": "citation",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Chen et al. (2021)"
      ],
      "description": "A reference to a paper discussing methods for unbiased estimates.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 183,
      "importance": 0.0,
      "inferred_type": "citation"
    },
    {
      "id": "Chowdhery et al. (2022)",
      "type": "citation",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Chowdhery et al. (2022)"
      ],
      "description": "A reference to a paper discussing the PaLM-Coder model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 184,
      "importance": 0.0,
      "inferred_type": "citation"
    },
    {
      "id": "Iyer et al. (2022)",
      "type": "citation",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Iyer et al. (2022)"
      ],
      "description": "A reference to a paper discussing instruction-tuned models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 185,
      "importance": 0.0,
      "inferred_type": "citation"
    },
    {
      "id": "Zhang et al. (2022)",
      "type": "citation",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Zhang et al. (2022)"
      ],
      "description": "A reference to a paper discussing toxic content generation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 158,
      "importance": 0.004838709677419354,
      "inferred_type": "citation"
    },
    {
      "id": "WinoGender",
      "type": "dataset",
      "documents": [
        "llama",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Winogender",
        "WinoGender"
      ],
      "description": "A dataset for evaluating co-reference resolution and biases related to gender.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0493880006706006,
      "inferred_type": "dataset"
    },
    {
      "id": "CrowS-Pairs",
      "type": "dataset",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "CrowS-Pairs"
      ],
      "description": "A dataset used to measure biases in various categories including gender and religion.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 186,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "TruthfulQA",
      "type": "benchmark",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "TruthfulQA"
      ],
      "description": "A benchmark for measuring the truthfulness of language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 187,
      "importance": 0.0,
      "inferred_type": "benchmark"
    },
    {
      "id": "Rae et al. (2021)",
      "type": "publication",
      "documents": [
        "llama",
        "longformer",
        "palm"
      ],
      "mentions": 3,
      "aliases": [
        "Rae et al. (2020)",
        "Rae et al. (2021)"
      ],
      "description": "A reference to previous work on gender bias in models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 188,
      "importance": 0.049999999999999996,
      "inferred_type": "publication"
    },
    {
      "id": "API",
      "type": "technology",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "API"
      ],
      "description": "Application Programming Interface used for model interaction.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 189,
      "importance": 0.0,
      "inferred_type": "technology"
    },
    {
      "id": "Carbon Emission",
      "type": "metric",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Carbon Emission"
      ],
      "description": "A measure of the carbon footprint associated with model training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 190,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Perplexity",
      "type": "metric",
      "documents": [
        "llama",
        "transformer_xl",
        "transformer_xl"
      ],
      "mentions": 3,
      "aliases": [
        "perplexity (PPL)",
        "perplexity",
        "Perplexity"
      ],
      "description": "A measure used to evaluate the performance of language models.",
      "pagerank": 0.002471051328056027,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.07833552714628704,
      "inferred_type": "metric"
    },
    {
      "id": "Co-reference resolution",
      "type": "method",
      "documents": [
        "llama"
      ],
      "mentions": 1,
      "aliases": [
        "Co-reference resolution"
      ],
      "description": "A task in natural language processing to determine which words refer to the same entity.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 192,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Longformer",
      "type": "architecture",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Longformer"
      ],
      "description": "A modified Transformer architecture with a self-attention operation that scales linearly with sequence length.",
      "pagerank": 0.008041616010407168,
      "centrality": 0,
      "betweenness": 0.004917952831006676,
      "eigenvector": 0.10896728727935771,
      "in_degree": 5,
      "out_degree": 14,
      "total_degree": 19,
      "clustering_coefficient": 0.014705882352941176,
      "community": 88,
      "importance": 0.29269266408736677,
      "inferred_type": "architecture"
    },
    {
      "id": "Allen Institute for Artificial Intelligence",
      "type": "organization",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Allen Institute for Artificial Intelligence"
      ],
      "description": "The organization where the authors of the paper are affiliated.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 193,
      "importance": 0.0,
      "inferred_type": "organization"
    },
    {
      "id": "text8",
      "type": "dataset",
      "documents": [
        "longformer",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "text8"
      ],
      "description": "A benchmark dataset used for evaluating language models.",
      "pagerank": 0.0030304233869745596,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.02031797681268491,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 88,
      "importance": 0.06695021519164732,
      "inferred_type": "dataset"
    },
    {
      "id": "enwik8",
      "type": "dataset",
      "documents": [
        "longformer",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "enwik8"
      ],
      "description": "Another benchmark dataset used for evaluating language models.",
      "pagerank": 0.001637751006231779,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.02031797681268491,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 1.0,
      "community": 88,
      "importance": 0.04930320156154448,
      "inferred_type": "dataset"
    },
    {
      "id": "WikiHop",
      "type": "dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "WikiHop"
      ],
      "description": "A dataset used for evaluating long document tasks.",
      "pagerank": 0.0018331812779159317,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022233121630183945,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.027574705763253084,
      "inferred_type": "dataset"
    },
    {
      "id": "Longformer-Encoder-Decoder (LED)",
      "type": "architecture",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Longformer-Encoder-Decoder (LED)"
      ],
      "description": "A variant of Longformer designed for long document generative sequence-to-sequence tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 194,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "RoBERTa",
      "type": "architecture",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "RoBERTa"
      ],
      "description": "A pre-trained Transformer model that Longformer outperforms on long document tasks.",
      "pagerank": 0.002614902364652543,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017558527135712976,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.03553932405248168,
      "inferred_type": "architecture"
    },
    {
      "id": "Transformer-XL",
      "type": "architecture",
      "documents": [
        "longformer",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "Transformer-XL"
      ],
      "description": "A model that addresses the computational efficiency of Transformers on long sequences.",
      "pagerank": 0.012433556432777474,
      "centrality": 0,
      "betweenness": 0.003316569843059048,
      "eigenvector": 0.029256909087692407,
      "in_degree": 8,
      "out_degree": 8,
      "total_degree": 16,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.30353286317982503,
      "inferred_type": "architecture"
    },
    {
      "id": "Dai et al. (2019)",
      "type": "reference",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Dai et al. (2019)"
      ],
      "description": "A reference to prior work on generative language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 195,
      "importance": 0.0,
      "inferred_type": "reference"
    },
    {
      "id": "Radford et al. (2019)",
      "type": "reference",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Radford et al. (2019)"
      ],
      "description": "A reference to prior work on generative language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 196,
      "importance": 0.0,
      "inferred_type": "reference"
    },
    {
      "id": "Cohan et al. (2018)",
      "type": "reference",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Cohan et al. (2018)"
      ],
      "description": "A reference to prior work on summarization datasets.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 197,
      "importance": 0.0,
      "inferred_type": "reference"
    },
    {
      "id": "BlockSparse",
      "type": "method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "BlockSparse"
      ],
      "description": "A method implemented in C++ for specific versions of TensorFlow.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 198,
      "importance": 0.004838709677419354,
      "inferred_type": "method"
    },
    {
      "id": "CUDA",
      "type": "technology",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "CUDA"
      ],
      "description": "A parallel computing platform and application programming interface model created by NVIDIA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 199,
      "importance": 0.0,
      "inferred_type": "technology"
    },
    {
      "id": "TensorFlow",
      "type": "framework",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "TensorFlow"
      ],
      "description": "An open-source machine learning framework.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 198,
      "importance": 0.01722051862198086,
      "inferred_type": "framework"
    },
    {
      "id": "QA",
      "type": "task",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "QA"
      ],
      "description": "Question Answering, a natural language processing task.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 200,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "coreference resolution",
      "type": "task",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "coreference resolution"
      ],
      "description": "The task of determining when different expressions refer to the same entity.",
      "pagerank": 0.0023218258859869236,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0052375994342850766,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.026710171046047358,
      "inferred_type": "task"
    },
    {
      "id": "LED",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "LED"
      ],
      "description": "Longformer-Encoder-Decoder, a variant of Longformer for sequence-to-sequence learning.",
      "pagerank": 0.001637751006231779,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017124855898013484,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.022977460633107655,
      "inferred_type": "model"
    },
    {
      "id": "BP-Transformer",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "BP-Transformer"
      ],
      "description": "A transformer model evaluated on machine translation tasks.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 201,
      "importance": 0.01722051862198086,
      "inferred_type": "model"
    },
    {
      "id": "CPC loss",
      "type": "metric",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "CPC loss"
      ],
      "description": "Contrastive Predictive Coding loss, an additional training objective.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005622200565216767,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.007172971860521041,
      "inferred_type": "metric"
    },
    {
      "id": "BigBird",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "BigBird"
      ],
      "description": "An extension of ETC for long document tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 202,
      "importance": 0.004838709677419354,
      "inferred_type": "model"
    },
    {
      "id": "ETC",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "ETC"
      ],
      "description": "A transformer model that uses local + global attention.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 202,
      "importance": 0.01722051862198086,
      "inferred_type": "model"
    },
    {
      "id": "SQuAD",
      "type": "dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "SQuAD"
      ],
      "description": "Stanford Question Answering Dataset, typically fits within the 512 limit.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 203,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "MRQA",
      "type": "dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "MRQA"
      ],
      "description": "Machine Reading for Question Answering, constructed by dropping long-document examples.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 204,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "dilated CNNs",
      "type": "method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "dilated CNNs"
      ],
      "description": "Convolutional neural networks that use dilated convolutions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 205,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Sutskever et al. (2014)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Sutskever et al. (2014)"
      ],
      "description": "The paper introducing sequence-to-sequence learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 206,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Xie et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Xie et al. (2019)"
      ],
      "description": "Research on truncating documents for classification.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 207,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Clark and Gardner (2017)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Clark and Gardner (2017)"
      ],
      "description": "Research on two-stage models for open domain QA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 208,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Chen et al. (2017)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Chen et al. (2017)"
      ],
      "description": "Research related to answer extraction in QA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 209,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Gupta and Berant (2020)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Gupta and Berant (2020)"
      ],
      "description": "Research on using global memory in models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 210,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Kovaleva et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Kovaleva et al. (2019)"
      ],
      "description": "Research on the importance of local context.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 211,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Josh et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Josh et al. (2019)"
      ],
      "description": "Research on processing document chunks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 212,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Wu et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Wu et al. (2019)"
      ],
      "description": "Research on CNNs and their representation capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 213,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "vandenOord et al. (2016)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "vandenOord et al. (2016)"
      ],
      "description": "Research on dilated convolutions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 214,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Ye et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Ye et al. (2019)"
      ],
      "description": "Research on evaluating transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 201,
      "importance": 0.004838709677419354,
      "inferred_type": "publication"
    },
    {
      "id": "BERT",
      "type": "model",
      "documents": [
        "longformer",
        "palm",
        "reformer"
      ],
      "mentions": 3,
      "aliases": [
        "BERT"
      ],
      "description": "A state-of-the-art model for natural language processing tasks.",
      "pagerank": 0.004569205081494072,
      "centrality": 0,
      "betweenness": 5.4192317697043266e-05,
      "eigenvector": 0.019002170947805132,
      "in_degree": 4,
      "out_degree": 1,
      "total_degree": 5,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.1261697461339757,
      "inferred_type": "model"
    },
    {
      "id": "Global Attention",
      "type": "method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Global Attention"
      ],
      "description": "An attention mechanism that allows certain tokens to attend to all tokens in the sequence.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 215,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Masked Language Modeling (MLM)",
      "type": "task",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Masked Language Modeling (MLM)"
      ],
      "description": "A task where the model predicts masked words in a sequence.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 216,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "Question Answering (QA)",
      "type": "task",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Question Answering (QA)"
      ],
      "description": "A task where the model answers questions based on a given document.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 217,
      "importance": 0.0,
      "inferred_type": "task"
    },
    {
      "id": "Adaptive Span",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Adaptive Span"
      ],
      "description": "A model that adapts the attention span based on the input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017124855898013484,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.011948719903861274,
      "inferred_type": "model"
    },
    {
      "id": "Compressive Transformer",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Compressive Transformer"
      ],
      "description": "A transformer model that compresses information to handle longer sequences.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0003170250585277031,
      "eigenvector": 0.017558527135712976,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.03374436075794925,
      "inferred_type": "model"
    },
    {
      "id": "Reformer",
      "type": "model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Reformer"
      ],
      "description": "A transformer model that uses reversible layers to reduce memory usage.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 218,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "Dataset text8",
      "type": "dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Dataset text8"
      ],
      "description": "A dataset used for training language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 219,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "Dataset enwik8",
      "type": "dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Dataset enwik8"
      ],
      "description": "A dataset used for evaluating language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 220,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "TVM",
      "type": "technology",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "TVM"
      ],
      "description": "An open-source deep learning compiler stack.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 221,
      "importance": 0.0,
      "inferred_type": "technology"
    },
    {
      "id": "PyTorch",
      "type": "library",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "PyTorch"
      ],
      "description": "An open-source machine learning library used for deep learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 222,
      "importance": 0.0,
      "inferred_type": "library"
    },
    {
      "id": "Sukhbaatar et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Sukhbaatar et al. (2019)"
      ],
      "description": "A reference to a paper that discusses adaptive attention spans.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 223,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Child et al. (2019)",
      "type": "publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Child et al. (2019)"
      ],
      "description": "A reference to a paper that discusses sparse transformers.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 224,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Al-Rfou et al. (2018)",
      "type": "publication",
      "documents": [
        "longformer",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "Al-Rfou et al. (2018)"
      ],
      "description": "A reference to a paper that discusses the T12 dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 225,
      "importance": 0.024999999999999998,
      "inferred_type": "publication"
    },
    {
      "id": "MLM",
      "type": "Method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "MLM"
      ],
      "description": "Masked Language Modeling, a pretraining objective for language models.",
      "pagerank": 0.001637751006231779,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017124855898013484,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.018138750955688298,
      "inferred_type": "Method"
    },
    {
      "id": "Attention Pattern",
      "type": "Method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Attention Pattern"
      ],
      "description": "A technique for configuring attention mechanisms in transformer models.",
      "pagerank": 0.001637751006231779,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017124855898013484,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.018138750955688298,
      "inferred_type": "Method"
    },
    {
      "id": "Gradient Updates",
      "type": "Metric",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Gradient Updates"
      ],
      "description": "The number of updates applied during model training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 226,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "HotpotQA",
      "type": "Dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "HotpotQA"
      ],
      "description": "A multi-hop question answering dataset that requires evidence extraction.",
      "pagerank": 0.0013446745275661593,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005108265732170462,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.009435954807564793,
      "inferred_type": "Dataset"
    },
    {
      "id": "fairseq",
      "type": "Framework",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "fairseq"
      ],
      "description": "A sequence-to-sequence learning toolkit for training models.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 227,
      "importance": 0.01722051862198086,
      "inferred_type": "Framework"
    },
    {
      "id": "Clark and Gardner",
      "type": "People",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Clark and Gardner"
      ],
      "description": "Researchers who proposed a loss function for question answering.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 228,
      "importance": 0.0,
      "inferred_type": "People"
    },
    {
      "id": "Ott et al.",
      "type": "People",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Ott et al."
      ],
      "description": "Researchers who contributed to the fairseq framework.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 227,
      "importance": 0.004838709677419354,
      "inferred_type": "People"
    },
    {
      "id": "Devlin et al.",
      "type": "People",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Devlin et al."
      ],
      "description": "Researchers who developed the BERT model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.002986333398558319,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.006078595205786209,
      "inferred_type": "People"
    },
    {
      "id": "Liu et al.",
      "type": "People",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Liu et al."
      ],
      "description": "Researchers who introduced the RoBERTa model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0027594496769719297,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.005984396131043655,
      "inferred_type": "People"
    },
    {
      "id": "Rae et al.",
      "type": "People",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Rae et al."
      ],
      "description": "Researchers who discussed configurations for transformer models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0027594496769719297,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.005984396131043655,
      "inferred_type": "People"
    },
    {
      "id": "Clark et al.",
      "type": "People",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Clark et al."
      ],
      "description": "Researchers who analyzed BERT's attention heads.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 229,
      "importance": 0.0,
      "inferred_type": "People"
    },
    {
      "id": "RoBERTa-large",
      "type": "Model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "RoBERTa-large"
      ],
      "description": "A transformer-based model for natural language processing.",
      "pagerank": 0.0013446745275661593,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005108265732170462,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.009435954807564793,
      "inferred_type": "Model"
    },
    {
      "id": "Longformer-large",
      "type": "Model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Longformer-large"
      ],
      "description": "A transformer model designed for long document processing.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0325074716845155,
      "in_degree": 0,
      "out_degree": 5,
      "total_degree": 5,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.03769021413944846,
      "inferred_type": "Model"
    },
    {
      "id": "Frozen RoBERTa Weights",
      "type": "Method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Frozen RoBERTa Weights"
      ],
      "description": "A technique where the weights of the RoBERTa model are kept constant during training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 230,
      "importance": 0.0,
      "inferred_type": "Method"
    },
    {
      "id": "OntoNotes",
      "type": "Dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "OntoNotes"
      ],
      "description": "A dataset used for coreference resolution.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0008230423664951386,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.005180425816663143,
      "inferred_type": "Dataset"
    },
    {
      "id": "IMDB",
      "type": "Dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "IMDB"
      ],
      "description": "A dataset for sentiment classification based on movie reviews.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 231,
      "importance": 0.0,
      "inferred_type": "Dataset"
    },
    {
      "id": "Hyperpartisan",
      "type": "Dataset",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Hyperpartisan"
      ],
      "description": "A dataset for detecting hyperpartisan news.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 232,
      "importance": 0.0,
      "inferred_type": "Dataset"
    },
    {
      "id": "BPC",
      "type": "Metric",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "BPC"
      ],
      "description": "Bits per character, a measure of model performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 233,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "GNN",
      "type": "Method",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "GNN"
      ],
      "description": "Graph Neural Networks, used for reasoning over entities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 234,
      "importance": 0.0,
      "inferred_type": "Method"
    },
    {
      "id": "Joshi et al. (2019)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Joshi et al. (2019)"
      ],
      "description": "A paper that presents a model for coreference resolution.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 235,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "BART",
      "type": "Model",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "BART"
      ],
      "description": "A pre-trained encoder-decoder model for sequence-to-sequence tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 236,
      "importance": 0.0,
      "inferred_type": "Model"
    },
    {
      "id": "T5",
      "type": "Model",
      "documents": [
        "longformer",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "T5"
      ],
      "description": "A text-to-text transfer transformer model.",
      "pagerank": 0.005034668248696935,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.002986333398558319,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 88,
      "importance": 0.08031209125843566,
      "inferred_type": "Model"
    },
    {
      "id": "Lee et al. (2018)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Lee et al. (2018)"
      ],
      "description": "A paper that discusses replacing ELMo with BERT.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 237,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Fang et al. (2020)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Fang et al. (2020)"
      ],
      "description": "A paper that presents a state-of-the-art model for HotpotQA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 238,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Tu et al. (2019)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Tu et al. (2019)"
      ],
      "description": "A paper that discusses a model for question answering.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 239,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Shao et al. (2020)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Shao et al. (2020)"
      ],
      "description": "A paper that presents a model for question answering.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 240,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Kipf and Welling (2017)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Kipf and Welling (2017)"
      ],
      "description": "A paper that discusses Graph Neural Networks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 241,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Glaß et al. (2019)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Glaß et al. (2019)"
      ],
      "description": "A paper that presents a non-GNN method for question answering.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 242,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Groeneveld et al. (2020)",
      "type": "Publication",
      "documents": [
        "longformer"
      ],
      "mentions": 1,
      "aliases": [
        "Groeneveld et al. (2020)"
      ],
      "description": "A paper that discusses a model for question answering.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 243,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Pathways",
      "type": "System",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Pathways"
      ],
      "description": "A new ML system enabling efficient training across multiple TPU Pods.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "System"
    },
    {
      "id": "TPUv4",
      "type": "Hardware",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "TPUv4"
      ],
      "description": "Tensor Processing Unit version 4 used for training the model.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Hardware"
    },
    {
      "id": "BIG-bench",
      "type": "Benchmark",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "BIG-bench"
      ],
      "description": "A benchmark suite for evaluating language models.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Benchmark"
    },
    {
      "id": "Aakanksha Chowdhery",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Aakanksha Chowdhery"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 244,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Sharan Narang",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Sharan Narang"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 245,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Jacob Devlin",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Jacob Devlin"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 246,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Sanjay Ghemawat",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Sanjay Ghemawat"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 247,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Ethical Considerations",
      "type": "Topic",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Ethical Considerations"
      ],
      "description": "Discussion on the ethical implications of large language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 248,
      "importance": 0.0,
      "inferred_type": "Topic"
    },
    {
      "id": "Bias and Toxicity",
      "type": "Analysis",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Bias and Toxicity"
      ],
      "description": "Analysis of bias and toxicity in model outputs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 249,
      "importance": 0.0,
      "inferred_type": "Analysis"
    },
    {
      "id": "Language Models (LMs)",
      "type": "Technology",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Language Models (LMs)"
      ],
      "description": "Models used for few-shot predictions based on natural language task descriptions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 250,
      "importance": 0.0,
      "inferred_type": "Technology"
    },
    {
      "id": "Few-shot predictions",
      "type": "Method",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Few-shot predictions"
      ],
      "description": "A prediction method where the model is given a task description and a few exemplars.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 251,
      "importance": 0.0,
      "inferred_type": "Method"
    },
    {
      "id": "GLaM",
      "type": "Model",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "GLaM"
      ],
      "description": "A post-GPT-3 language model that achieved few-shot state-of-the-art results.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 252,
      "importance": 0.0,
      "inferred_type": "Model"
    },
    {
      "id": "Megatron–Turing NLG",
      "type": "Model",
      "documents": [
        "palm",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Megatron-Turing NLG 530B",
        "Megatron–Turing NLG"
      ],
      "description": "A post-GPT-3 language model developed by Smith et al. in 2022.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 253,
      "importance": 0.024999999999999998,
      "inferred_type": "Model"
    },
    {
      "id": "Transformer architecture",
      "type": "Architecture",
      "documents": [
        "palm",
        "swin_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "Transformer architecture"
      ],
      "description": "The architecture used in GPT-3 and its variants.",
      "pagerank": 0.008545848835985975,
      "centrality": 0,
      "betweenness": 0.01094522240527183,
      "eigenvector": 0.10480011714394742,
      "in_degree": 4,
      "out_degree": 1,
      "total_degree": 5,
      "clustering_coefficient": 0.1,
      "community": 88,
      "importance": 0.33816875482728587,
      "inferred_type": "Architecture"
    },
    {
      "id": "TPU v4 Pods",
      "type": "Hardware",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "TPU v4 Pods"
      ],
      "description": "A type of hardware used for training large models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 254,
      "importance": 0.0,
      "inferred_type": "Hardware"
    },
    {
      "id": "FLOPs utilization",
      "type": "Metric",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "FLOPs utilization"
      ],
      "description": "A measure of efficiency in model and hardware performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 255,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "Wei et al. (2022b)",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Wei et al. (2022b)"
      ],
      "description": "Researchers who contributed to chain-of-thought prompting.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 256,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Du et al. (2021)",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Du et al. (2021)"
      ],
      "description": "Researchers who developed GLaM.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 257,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Smith et al. (2022)",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Smith et al. (2022)"
      ],
      "description": "Researchers who developed Megatron–Turing NLG.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 258,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Thoppilan et al. (2022)",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Thoppilan et al. (2022)"
      ],
      "description": "Researchers who developed LaMDA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 259,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "Brown et al. (2020)",
      "type": "Person",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Brown et al. (2020)"
      ],
      "description": "Researchers who developed GPT-3.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 260,
      "importance": 0.0,
      "inferred_type": "Person"
    },
    {
      "id": "RoPE",
      "type": "embedding",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "RoPE"
      ],
      "description": "Rotary Position Embeddings used for better performance on long sequence lengths.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "embedding"
    },
    {
      "id": "SentencePiece",
      "type": "tokenization method",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "SentencePiece"
      ],
      "description": "A method used for generating a vocabulary of tokens from the training data.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "tokenization method"
    },
    {
      "id": "Kaplan et al., 2020",
      "type": "publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Kaplan et al., 2020"
      ],
      "description": "A reference discussing the power law in neural network scaling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 261,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Shazeer, 2020",
      "type": "publication",
      "documents": [
        "palm",
        "palm"
      ],
      "mentions": 2,
      "aliases": [
        "Shazeer, 2020",
        "Shazeer, 2019"
      ],
      "description": "A reference discussing the effectiveness of SwiGLU activations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 262,
      "importance": 0.024999999999999998,
      "inferred_type": "publication"
    },
    {
      "id": "Wang & Komatsuzaki, 2021",
      "type": "publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Wang & Komatsuzaki, 2021"
      ],
      "description": "A reference discussing the parallel formulation in Transformer blocks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 263,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Kudo & Richardson, 2018a",
      "type": "publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Kudo & Richardson, 2018a"
      ],
      "description": "A reference discussing the SentencePiece vocabulary generation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 264,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "540B parameters",
      "type": "Model Scale",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "540B parameters"
      ],
      "description": "The largest model scale in the comparison.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Model Scale"
    },
    {
      "id": "62B parameters",
      "type": "Model Scale",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "62B parameters"
      ],
      "description": "The medium model scale in the comparison.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Model Scale"
    },
    {
      "id": "8B parameters",
      "type": "Model Scale",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "8B parameters"
      ],
      "description": "The smallest model scale in the comparison.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Model Scale"
    },
    {
      "id": "FLOPs",
      "type": "Metric",
      "documents": [
        "palm",
        "swin_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "FLOPs"
      ],
      "description": "Floating Point Operations per second, approximately equal to the number of parameters.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.03708866210407426,
      "inferred_type": "Metric"
    },
    {
      "id": "Transformers",
      "type": "Architecture",
      "documents": [
        "palm",
        "vision_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "Transformers"
      ],
      "description": "A type of model architecture used for natural language processing.",
      "pagerank": 0.003066973796123852,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.022019081172381175,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 44,
      "importance": 0.06328092355959203,
      "inferred_type": "Architecture"
    },
    {
      "id": "JAX",
      "type": "Framework",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "JAX"
      ],
      "description": "A framework used for training and evaluation codebase.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Framework"
    },
    {
      "id": "T5X",
      "type": "Framework",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "T5X"
      ],
      "description": "A framework used for training and evaluation codebase.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.04355566490620611,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 2,
      "importance": 0.0243880006706006,
      "inferred_type": "Framework"
    },
    {
      "id": "780 billion tokens",
      "type": "Dataset Size",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "780 billion tokens"
      ],
      "description": "The total size of the pretraining dataset.",
      "pagerank": 0.0012649040620569207,
      "centrality": 0,
      "betweenness": 0.00022196270123413968,
      "eigenvector": 0.07343473010420389,
      "in_degree": 1,
      "out_degree": 2,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.04954798020540731,
      "inferred_type": "Dataset Size"
    },
    {
      "id": "Filtered webpages",
      "type": "Data Source",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Filtered webpages"
      ],
      "description": "One of the sources used in the training dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 265,
      "importance": 0.0,
      "inferred_type": "Data Source"
    },
    {
      "id": "GitHub",
      "type": "Data Source",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "GitHub"
      ],
      "description": "Source of code included in the training dataset.",
      "pagerank": 0.0016869982032211774,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.011539570113642235,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.016443842860301518,
      "inferred_type": "Data Source"
    },
    {
      "id": "Social media conversations",
      "type": "Data Source",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Social media conversations"
      ],
      "description": "One of the sources used in the training dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 266,
      "importance": 0.0,
      "inferred_type": "Data Source"
    },
    {
      "id": "News articles",
      "type": "Data Source",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "News articles"
      ],
      "description": "One of the sources used in the training dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 267,
      "importance": 0.0,
      "inferred_type": "Data Source"
    },
    {
      "id": "Levenshtein distance",
      "type": "Metric",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Levenshtein distance"
      ],
      "description": "A method used to remove duplicate files in the dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 268,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "Mitchell et al., 2019",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Mitchell et al., 2019"
      ],
      "description": "Authors of the Model Card for PaLM.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 269,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Thoppilan et al., 2022",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Thoppilan et al., 2022"
      ],
      "description": "Authors of the LaMDA model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 270,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Du et al., 2021",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Du et al., 2021"
      ],
      "description": "Authors of the GLaM model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 271,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Bradbury et al., 2018",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Bradbury et al., 2018"
      ],
      "description": "Authors of the JAX framework.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 272,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Roberts et al., 2022",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Roberts et al., 2022"
      ],
      "description": "Authors of the T5X framework.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 273,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Jouppi et al., 2020",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Jouppi et al., 2020"
      ],
      "description": "Authors of the TPU v4 Pods.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 274,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Xu et al., 2021",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Xu et al., 2021"
      ],
      "description": "Authors discussing model and data parallelism.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 275,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Huang et al., 2019",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Huang et al., 2019"
      ],
      "description": "Authors discussing pipeline parallelism.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 276,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Lopes et al., 2017",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Lopes et al., 2017"
      ],
      "description": "Authors discussing duplicate files in source code.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 277,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Allamanis, 2019",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Allamanis, 2019"
      ],
      "description": "Authors discussing duplicate files in source code.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 278,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Gebru et al., 2021",
      "type": "Publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Gebru et al., 2021"
      ],
      "description": "Authors of the datasheet providing additional information.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 279,
      "importance": 0.0,
      "inferred_type": "Publication"
    },
    {
      "id": "Pathways system",
      "type": "system",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Pathways system"
      ],
      "description": "A system designed to scale training across TPU pods using two-way data parallelism.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 9.714842283575237e-42,
      "in_degree": 0,
      "out_degree": 2,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 280,
      "importance": 0.009677419354838708,
      "inferred_type": "system"
    },
    {
      "id": "TPU v4",
      "type": "hardware",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "TPU v4"
      ],
      "description": "A type of Tensor Processing Unit used for accelerating machine learning workloads.",
      "pagerank": 0.0016378199350923885,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 6.869430856873855e-42,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 280,
      "importance": 0.011029614149700107,
      "inferred_type": "hardware"
    },
    {
      "id": "JAX/XLA",
      "type": "software",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "JAX/XLA"
      ],
      "description": "A framework for high-performance numerical computing and machine learning.",
      "pagerank": 0.0016378199350923885,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 6.869430856873855e-42,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 280,
      "importance": 0.011029614149700107,
      "inferred_type": "software"
    },
    {
      "id": "Python client",
      "type": "software",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Python client"
      ],
      "description": "A client that constructs a sharded dataflow program for executing computations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 281,
      "importance": 0.0,
      "inferred_type": "software"
    },
    {
      "id": "Gradient transfer",
      "type": "method",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Gradient transfer"
      ],
      "description": "The process of transferring computed gradients between TPU pods.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 282,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Model FLOPs utilization (MFU)",
      "type": "metric",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Model FLOPs utilization (MFU)"
      ],
      "description": "A metric for measuring training efficiency based on observed throughput relative to theoretical maximum throughput.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 283,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Hardware FLOPs utilization (HFU)",
      "type": "metric",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Hardware FLOPs utilization (HFU)"
      ],
      "description": "A metric reflecting the ratio of FLOPs observed on a device to its theoretical peak FLOPs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 284,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Rematerialization",
      "type": "method",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Rematerialization"
      ],
      "description": "A technique to trade off memory usage with compute by recomputing certain operations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 285,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Megatron-Turing NLG",
      "type": "model",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Megatron-Turing NLG"
      ],
      "description": "A large language model with 530 billion parameters.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 286,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "Google datacenter network",
      "type": "infrastructure",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Google datacenter network"
      ],
      "description": "The network infrastructure connecting TPU pods for data transfer.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 287,
      "importance": 0.0,
      "inferred_type": "infrastructure"
    },
    {
      "id": "Barham et al., 2022",
      "type": "publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Barham et al., 2022"
      ],
      "description": "A reference for the Pathways system and its design.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 288,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Singh et al., 2015",
      "type": "publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Singh et al., 2015"
      ],
      "description": "A reference for the Google datacenter network.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 289,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Narayanan et al., 2021b",
      "type": "publication",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Narayanan et al., 2021b"
      ],
      "description": "A reference for analytical accounting of hardware FLOPs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 290,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Appendix B",
      "type": "document section",
      "documents": [
        "palm"
      ],
      "mentions": 1,
      "aliases": [
        "Appendix B"
      ],
      "description": "Section detailing the mathematical formula to compute MFU.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 291,
      "importance": 0.0,
      "inferred_type": "document section"
    },
    {
      "id": "Classification Token",
      "type": "Model Component",
      "documents": [
        "reformer"
      ],
      "mentions": 1,
      "aliases": [
        "Classification Token"
      ],
      "description": "A learnable embedding added to the input sequence for classification tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 292,
      "importance": 0.0,
      "inferred_type": "Model Component"
    },
    {
      "id": "2D image topology",
      "type": "concept",
      "documents": [
        "reformer"
      ],
      "mentions": 1,
      "aliases": [
        "2D image topology"
      ],
      "description": "The spatial arrangement of pixels in a two-dimensional image.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 293,
      "importance": 0.01722051862198086,
      "inferred_type": "component"
    },
    {
      "id": "masked patch prediction",
      "type": "method",
      "documents": [
        "reformer",
        "vision_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "masked patch prediction"
      ],
      "description": "A self-supervised learning task that involves predicting missing parts of an image.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 294,
      "importance": 0.024999999999999998,
      "inferred_type": "method"
    },
    {
      "id": "LLaVA",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LLaVA"
      ],
      "description": "Large Language and Vision Assistant, an end-to-end trained large multimodal model.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.11427834743674357,
      "in_degree": 0,
      "out_degree": 10,
      "total_degree": 10,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.095833932168025,
      "inferred_type": "model"
    },
    {
      "id": "Vicuna",
      "type": "model",
      "documents": [
        "segment_anything",
        "segment_anything"
      ],
      "mentions": 2,
      "aliases": [
        "Vicuna",
        "Vicuna-v0"
      ],
      "description": "Language decoder connected to the visual encoder in LLaVA.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017957619529923058,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.0385326530764299,
      "inferred_type": "model"
    },
    {
      "id": "Science QA",
      "type": "dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "Science QA"
      ],
      "description": "A multimodal reasoning dataset used for evaluating LLaVA.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017957619529923058,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.0135326530764299,
      "inferred_type": "dataset"
    },
    {
      "id": "LLaVA-Bench",
      "type": "benchmark",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LLaVA-Bench"
      ],
      "description": "A set of two challenging benchmarks for evaluating multimodal instruction-following.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 2.7096158848521633e-06,
      "eigenvector": 0.01841226627696135,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.01859769066017883,
      "inferred_type": "benchmark"
    },
    {
      "id": "ChatGPT",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "ChatGPT"
      ],
      "description": "A conversational AI model developed by OpenAI, demonstrating the power of aligned LLMs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 296,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "Alpaca",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "Alpaca"
      ],
      "description": "An open-source LLM that matches the performance of GPT-3.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 297,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "InstructPix2Pix",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "InstructPix2Pix"
      ],
      "description": "An image editing model that follows human instructions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 298,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "Flamingo",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "Flamingo"
      ],
      "description": "A multimodal model known for strong performance on zero-shot task transfer.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.017957619529923058,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.0135326530764299,
      "inferred_type": "model"
    },
    {
      "id": "BLIP-2",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "BLIP-2"
      ],
      "description": "A model trained on image-text pairs for multimodal tasks.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 2.7096158848521633e-06,
      "eigenvector": 0.018449636138443245,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.01861320612488576,
      "inferred_type": "model"
    },
    {
      "id": "FROMAGe",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "FROMAGe"
      ],
      "description": "A model for multimodal tasks, trained on image-text pairs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0004920166085201903,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.005042988366769589,
      "inferred_type": "model"
    },
    {
      "id": "KOSMOS-1",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "KOSMOS-1"
      ],
      "description": "A multimodal model trained on image-text pairs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0004920166085201903,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.005042988366769589,
      "inferred_type": "model"
    },
    {
      "id": "PaLM-E",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "PaLM-E"
      ],
      "description": "A large multimodal model for embodied AI.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0004920166085201903,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.005042988366769589,
      "inferred_type": "model"
    },
    {
      "id": "OpenFlamingo",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "OpenFlamingo"
      ],
      "description": "An open-source effort enabling LLaMA to use image inputs.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 5.825674152432151e-05,
      "eigenvector": 0.09365269926374392,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.0506065841269285,
      "inferred_type": "model"
    },
    {
      "id": "LangChain",
      "type": "framework",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LangChain"
      ],
      "description": "A system that coordinates various models via LLMs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 299,
      "importance": 0.0,
      "inferred_type": "framework"
    },
    {
      "id": "Visual ChatGPT",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "Visual ChatGPT"
      ],
      "description": "A multimodal model that integrates visual and language capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 300,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "X-GPT",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "X-GPT"
      ],
      "description": "A multimodal model that integrates visual and language capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 301,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "MM-REACT",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "MM-REACT"
      ],
      "description": "A multimodal model that integrates visual and language capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 302,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "VisProg",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "VisProg"
      ],
      "description": "A multimodal model that integrates visual and language capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 303,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "ViperGPT",
      "type": "model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "ViperGPT"
      ],
      "description": "A multimodal model that integrates visual and language capabilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 304,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "LMM",
      "type": "Technology",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LMM"
      ],
      "description": "Large Multimodal Model used for zero-shot task transfer and in-context learning.",
      "pagerank": 0.005140941409967469,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0031311004926347073,
      "in_degree": 4,
      "out_degree": 0,
      "total_degree": 4,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.07123494811732231,
      "inferred_type": "Technology"
    },
    {
      "id": "LLaMA-Adapter",
      "type": "Model",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LLaMA-Adapter"
      ],
      "description": "An open-source adaptation of LLaMA for image inputs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.07569507973382086,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 43,
      "importance": 0.03626629039424362,
      "inferred_type": "Model"
    },
    {
      "id": "visual instruction tuning",
      "type": "Method",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "visual instruction tuning"
      ],
      "description": "A method aimed at improving model instruction-following abilities.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 305,
      "importance": 0.0,
      "inferred_type": "Method"
    },
    {
      "id": "visual prompt tuning",
      "type": "Method",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "visual prompt tuning"
      ],
      "description": "A method aimed at improving parameter efficiency in model adaptation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 306,
      "importance": 0.0,
      "inferred_type": "Method"
    },
    {
      "id": "COCO",
      "type": "Dataset",
      "documents": [
        "segment_anything",
        "swin_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "COCO"
      ],
      "description": "A dataset used for generating instruction-following data.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 307,
      "importance": 0.024999999999999998,
      "inferred_type": "Dataset"
    },
    {
      "id": "CC",
      "type": "Dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "CC"
      ],
      "description": "A public multimodal dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 308,
      "importance": 0.0,
      "inferred_type": "Dataset"
    },
    {
      "id": "LAION",
      "type": "Dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LAION"
      ],
      "description": "Another public multimodal dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 309,
      "importance": 0.0,
      "inferred_type": "Dataset"
    },
    {
      "id": "image-text pairs",
      "type": "Data Type",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "image-text pairs"
      ],
      "description": "Data consisting of images and their corresponding textual descriptions.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 310,
      "importance": 0.0,
      "inferred_type": "Data Type"
    },
    {
      "id": "visual feature Z",
      "type": "data",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "visual feature Z"
      ],
      "description": "The output feature representation from the visual encoder.",
      "pagerank": 0.0024369083363361154,
      "centrality": 0,
      "betweenness": 0.00022489811844272954,
      "eigenvector": 0.008615665885402937,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.03752752503187741,
      "inferred_type": "data"
    },
    {
      "id": "language embedding tokens H",
      "type": "data",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "language embedding tokens H"
      ],
      "description": "Tokens representing language embeddings in the model.",
      "pagerank": 0.0032213391862991533,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.001353996541728007,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.03165707295518624,
      "inferred_type": "data"
    },
    {
      "id": "projection matrix W",
      "type": "method",
      "documents": [
        "segment_anything",
        "segment_anything"
      ],
      "mentions": 2,
      "aliases": [
        "projection matrix W",
        "projection matrix"
      ],
      "description": "A trainable matrix used to convert visual features into language embedding tokens.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.001353996541728007,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 54,
      "importance": 0.030400870860881918,
      "inferred_type": "method"
    },
    {
      "id": "CC3M",
      "type": "dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "CC3M"
      ],
      "description": "A dataset containing image-text pairs used for training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 311,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "multi-turn conversation data",
      "type": "data",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "multi-turn conversation data"
      ],
      "description": "Data format used for training the model with multiple conversational turns.",
      "pagerank": 0.0012469593917240828,
      "centrality": 0,
      "betweenness": 5.419231769704327e-06,
      "eigenvector": 0.01889053203392295,
      "in_degree": 1,
      "out_degree": 2,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.02367253453970585,
      "inferred_type": "data"
    },
    {
      "id": "instruction tokens X",
      "type": "data",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "instruction tokens X"
      ],
      "description": "Tokens representing instructions given to the model.",
      "pagerank": 0.001679365795293591,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.002968438343545741,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.01278851126521904,
      "inferred_type": "data"
    },
    {
      "id": "assistant answers X",
      "type": "data",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "assistant answers X"
      ],
      "description": "Tokens representing the responses generated by the assistant.",
      "pagerank": 0.001679365795293591,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.002968438343545741,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.01278851126521904,
      "inferred_type": "data"
    },
    {
      "id": "LLM",
      "type": "technology",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LLM"
      ],
      "description": "Large Language Model used in conjunction with visual encoders.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 312,
      "importance": 0.0,
      "inferred_type": "technology"
    },
    {
      "id": "visual encoder",
      "type": "architecture",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "visual encoder"
      ],
      "description": "A component that processes visual inputs.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 313,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "ScienceQA",
      "type": "dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "ScienceQA"
      ],
      "description": "A large-scale multimodal science question dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 314,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "LLaVA-Instruct-158K",
      "type": "dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "LLaVA-Instruct-158K"
      ],
      "description": "A dataset used for fine-tuning LLaVA.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 315,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "A100",
      "type": "hardware",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "A100"
      ],
      "description": "NVIDIA A100 GPUs used for training the models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 316,
      "importance": 0.0,
      "inferred_type": "hardware"
    },
    {
      "id": "instruction-following capability",
      "type": "metric",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "instruction-following capability"
      ],
      "description": "A measure of how well the model follows user instructions.",
      "pagerank": 0.002224110750144847,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.01841226627696135,
      "in_degree": 2,
      "out_degree": 0,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.030941934923054047,
      "inferred_type": "metric"
    },
    {
      "id": "training data",
      "type": "method",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "training data"
      ],
      "description": "Data organized as single-turn conversations for training.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 317,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "evaluation metric",
      "type": "metric",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "evaluation metric"
      ],
      "description": "A quantitative measure to assess model performance.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0028932845227117726,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.006039962561939868,
      "inferred_type": "metric"
    },
    {
      "id": "COCO-Val-2014",
      "type": "dataset",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "COCO-Val-2014"
      ],
      "description": "A dataset used to generate questions for evaluating models.",
      "pagerank": 0.0022094873347051755,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0028932845227117726,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 295,
      "importance": 0.01947465425391959,
      "inferred_type": "dataset"
    },
    {
      "id": "MM-CoT",
      "type": "method",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "MM-CoT"
      ],
      "description": "Multimodal chain-of-thought method used as a state-of-the-art approach on the ScienceQA dataset.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 318,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "mean±std",
      "type": "metric",
      "documents": [
        "segment_anything"
      ],
      "mentions": 1,
      "aliases": [
        "mean±std"
      ],
      "description": "Statistical representation of model performance results.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 319,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Swin Transformer",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Swin Transformer"
      ],
      "description": "A hierarchical vision transformer designed for computer vision tasks.",
      "pagerank": 0.0061253449377267274,
      "centrality": 0,
      "betweenness": 0.005171572877828839,
      "eigenvector": 0.03887107407145946,
      "in_degree": 3,
      "out_degree": 14,
      "total_degree": 17,
      "clustering_coefficient": 0.007352941176470588,
      "community": 67,
      "importance": 0.23314658611245886,
      "inferred_type": "architecture"
    },
    {
      "id": "Microsoft Research Asia",
      "type": "organization",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Microsoft Research Asia"
      ],
      "description": "The organization where the authors of the paper are affiliated.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 6.426736456908581e-25,
      "in_degree": 0,
      "out_degree": 8,
      "total_degree": 8,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.038709677419354833,
      "inferred_type": "organization"
    },
    {
      "id": "Ze Liu",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Ze Liu"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Yutong Lin",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Yutong Lin"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Yue Cao",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Yue Cao"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Han Hu",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Han Hu"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Yixuan Wei",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Yixuan Wei"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Zheng Zhang",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Zheng Zhang"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Stephen Lin",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Stephen Lin"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "Baining Guo",
      "type": "person",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Baining Guo"
      ],
      "description": "One of the authors of the paper.",
      "pagerank": 0.0012713881756846019,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 2.272194464789431e-25,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 320,
      "importance": 0.0063864357954895425,
      "inferred_type": "person"
    },
    {
      "id": "ADE20K",
      "type": "dataset",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "ADE20K"
      ],
      "description": "A dataset used for semantic segmentation tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 321,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "feature pyramid networks (FPN)",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "feature pyramid networks (FPN)"
      ],
      "description": "A technique for dense predictions in computer vision.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.007375875474473507,
      "inferred_type": "method"
    },
    {
      "id": "U-Net",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "U-Net"
      ],
      "description": "A convolutional network architecture for semantic segmentation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.007375875474473507,
      "inferred_type": "method"
    },
    {
      "id": "ViT/DeiT",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "ViT/DeiT"
      ],
      "description": "Previous vision transformer architectures that Swin Transformer outperforms.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "architecture"
    },
    {
      "id": "Copy-paste",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Copy-paste"
      ],
      "description": "A technique used in object detection that Swin Transformer improves upon.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 322,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "DetectoRS",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "DetectoRS"
      ],
      "description": "A method for object detection that Swin Transformer surpasses.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "method"
    },
    {
      "id": "SETR",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "SETR"
      ],
      "description": "A method for semantic segmentation that Swin Transformer outperforms.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "method"
    },
    {
      "id": "mIoU",
      "type": "metric",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "mIoU"
      ],
      "description": "Mean Intersection over Union, a metric for evaluating segmentation tasks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 323,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "AlexNet",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "AlexNet"
      ],
      "description": "A pioneering CNN that popularized deep learning in computer vision.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 324,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "VGG",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "VGG"
      ],
      "description": "A deep CNN architecture known for its simplicity and depth.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 325,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "GoogleNet",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "GoogleNet"
      ],
      "description": "A CNN architecture that introduced the inception module.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 326,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "DenseNet",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "DenseNet"
      ],
      "description": "A CNN architecture that connects each layer to every other layer.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 327,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "HRNet",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "HRNet"
      ],
      "description": "High-Resolution Network for maintaining high-resolution representations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 328,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "ViT",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "ViT"
      ],
      "description": "Vision Transformer, a model that applies transformer architecture to image classification.",
      "pagerank": 0.007876293496778682,
      "centrality": 0,
      "betweenness": 0.005696154513136217,
      "eigenvector": 0.03742727913248348,
      "in_degree": 5,
      "out_degree": 5,
      "total_degree": 10,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.2281355463840795,
      "inferred_type": "architecture"
    },
    {
      "id": "patch merging",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "patch merging"
      ],
      "description": "A technique used in Swin Transformer to reduce the number of tokens.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 329,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "linear embedding",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "linear embedding"
      ],
      "description": "A process to project raw pixel values into an arbitrary dimension.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 330,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "DeiT",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "DeiT"
      ],
      "description": "Data-efficient image Transformers, which introduces training strategies for ViT.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "architecture"
    },
    {
      "id": "MLP",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "MLP"
      ],
      "description": "Multi-layer perceptron, a type of neural network.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 331,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "W-MSA",
      "type": "module",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "W-MSA"
      ],
      "description": "Window-based multi-head self-attention module.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 332,
      "importance": 0.0,
      "inferred_type": "module"
    },
    {
      "id": "SW-MSA",
      "type": "module",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "SW-MSA"
      ],
      "description": "Shifted window-based multi-head self-attention module.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 333,
      "importance": 0.0,
      "inferred_type": "module"
    },
    {
      "id": "Stage 1",
      "type": "process",
      "documents": [
        "swin_transformer",
        "swin_transformer",
        "swin_transformer",
        "swin_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "Stage 3",
        "Stage 2",
        "Stage 1",
        "Stage 4"
      ],
      "description": "The first stage of the Swin Transformer architecture.",
      "pagerank": 0.006998360151430364,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 334,
      "importance": 0.15879350678506476,
      "inferred_type": "process"
    },
    {
      "id": "RegNet",
      "type": "architecture",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "RegNet"
      ],
      "description": "A family of convolutional neural networks optimized through architecture search.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "architecture"
    },
    {
      "id": "Relative Position Bias",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Relative Position Bias"
      ],
      "description": "A technique used in self-attention mechanisms to incorporate positional information.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "method"
    },
    {
      "id": "Top-1 Accuracy",
      "type": "metric",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Top-1 Accuracy"
      ],
      "description": "A metric used to evaluate the performance of classification models.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "metric"
    },
    {
      "id": "Batch Size",
      "type": "parameter",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Batch Size"
      ],
      "description": "The number of training examples utilized in one iteration.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 335,
      "importance": 0.0,
      "inferred_type": "parameter"
    },
    {
      "id": "Cyclic Shifting",
      "type": "method",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Cyclic Shifting"
      ],
      "description": "A technique proposed to improve batch computation efficiency in window partitioning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 336,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Swin-T",
      "type": "model variant",
      "documents": [
        "swin_transformer",
        "swin_transformer",
        "swin_transformer",
        "swin_transformer"
      ],
      "mentions": 4,
      "aliases": [
        "Swin-T",
        "Swin-B",
        "Swin-S",
        "Swin-L"
      ],
      "description": "A variant of the Swin Transformer with specific hyperparameters.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0002872192837943293,
      "eigenvector": 0.01387988881706339,
      "in_degree": 1,
      "out_degree": 3,
      "total_degree": 4,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.11648124737891022,
      "inferred_type": "model variant"
    },
    {
      "id": "Cascade Mask R-CNN",
      "type": "framework",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Cascade Mask R-CNN"
      ],
      "description": "An object detection framework used for instance segmentation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0021819464929649026,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.005744624563414345,
      "inferred_type": "framework"
    },
    {
      "id": "ATSS",
      "type": "framework",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "ATSS"
      ],
      "description": "An object detection framework used for instance segmentation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 337,
      "importance": 0.0,
      "inferred_type": "framework"
    },
    {
      "id": "RepPoints v2",
      "type": "framework",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "RepPoints v2"
      ],
      "description": "An object detection framework used for instance segmentation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 338,
      "importance": 0.0,
      "inferred_type": "framework"
    },
    {
      "id": "Sparse RCNN",
      "type": "framework",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Sparse RCNN"
      ],
      "description": "An object detection framework used for instance segmentation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 339,
      "importance": 0.0,
      "inferred_type": "framework"
    },
    {
      "id": "HTC++",
      "type": "Framework",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "HTC++"
      ],
      "description": "An improved version of the HTC framework for object detection.",
      "pagerank": 0.0015211693823058235,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.006110905228003305,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 67,
      "importance": 0.012088662104074256,
      "inferred_type": "Framework"
    },
    {
      "id": "APbox",
      "type": "Metric",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "APbox"
      ],
      "description": "Average Precision for bounding box detection.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 340,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "APmask",
      "type": "Metric",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "APmask"
      ],
      "description": "Average Precision for mask prediction in instance segmentation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 341,
      "importance": 0.0,
      "inferred_type": "Metric"
    },
    {
      "id": "DeiT-S",
      "type": "Model",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "DeiT-S"
      ],
      "description": "Data-efficient image Transformers, a model architecture for image classification.",
      "pagerank": 0.0017519337766969442,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0021819464929649026,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.013381503369662248,
      "inferred_type": "Model"
    },
    {
      "id": "EfficientDet-D7",
      "type": "Model",
      "documents": [
        "swin_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "EfficientDet-D7"
      ],
      "description": "A model architecture for object detection that balances accuracy and efficiency.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 342,
      "importance": 0.0,
      "inferred_type": "Model"
    },
    {
      "id": "LSTM",
      "type": "architecture",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "LSTM"
      ],
      "description": "Long Short-Term Memory networks, a standard solution for language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.006747761666827309,
      "inferred_type": "architecture"
    },
    {
      "id": "RNN",
      "type": "architecture",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "RNN"
      ],
      "description": "Recurrent Neural Networks, a type of neural network for sequential data.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0003387019856065204,
      "eigenvector": 0.004714509333809899,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.028712214481946358,
      "inferred_type": "architecture"
    },
    {
      "id": "Carnegie Mellon University",
      "type": "organization",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Carnegie Mellon University"
      ],
      "description": "An academic institution where some authors are affiliated.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 343,
      "importance": 0.0,
      "inferred_type": "organization"
    },
    {
      "id": "WikiText-103",
      "type": "dataset",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "WikiText-103"
      ],
      "description": "A dataset used for language modeling.",
      "pagerank": 0.002471051328056027,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.02349681746886769,
      "inferred_type": "dataset"
    },
    {
      "id": "One Billion Word",
      "type": "dataset",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "One Billion Word"
      ],
      "description": "A large dataset for language modeling.",
      "pagerank": 0.002471051328056027,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.02349681746886769,
      "inferred_type": "dataset"
    },
    {
      "id": "context fragmentation",
      "type": "problem",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "context fragmentation"
      ],
      "description": "The issue of losing contextual information due to fixed-length segments.",
      "pagerank": 0.002471051328056027,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.02349681746886769,
      "inferred_type": "problem"
    },
    {
      "id": "gradient vanishing",
      "type": "problem",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "gradient vanishing"
      ],
      "description": "A common issue in training RNNs that affects optimization.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 344,
      "importance": 0.0,
      "inferred_type": "problem"
    },
    {
      "id": "gradient explosion",
      "type": "problem",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "gradient explosion"
      ],
      "description": "A common issue in training RNNs that affects optimization.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 345,
      "importance": 0.0,
      "inferred_type": "problem"
    },
    {
      "id": "Softmax function",
      "type": "function",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Softmax function"
      ],
      "description": "A function that converts logits into a categorical probability distribution.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 346,
      "importance": 0.0,
      "inferred_type": "function"
    },
    {
      "id": "Bengio et al. (2003)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Bengio et al. (2003)"
      ],
      "description": "A foundational paper in language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 347,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Mikolov et al. (2010)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Mikolov et al. (2010)"
      ],
      "description": "A significant work on RNNs and language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 348,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Merity et al. (2016)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Merity et al. (2016)"
      ],
      "description": "Research on language modeling techniques.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 349,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Galand Ghahramani (2016)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Galand Ghahramani (2016)"
      ],
      "description": "Work on improving optimization algorithms.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 350,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Grave et al. (2016a)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Grave et al. (2016a)"
      ],
      "description": "Research on speeding up Softmax computation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 351,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Yang et al. (2017)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Yang et al. (2017)"
      ],
      "description": "Work on enriching output distribution families.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 352,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Peters et al. (2018)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Peters et al. (2018)"
      ],
      "description": "Research on efficiency in language modeling.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 353,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "Devlin et al. (2018)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Devlin et al. (2018)"
      ],
      "description": "Contributions to language modeling techniques.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 354,
      "importance": 0.0,
      "inferred_type": "publication"
    },
    {
      "id": "SG function",
      "type": "function",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "SG function"
      ],
      "description": "Stop-gradient function used to prevent gradient flow.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.006747761666827309,
      "inferred_type": "function"
    },
    {
      "id": "positional encodings",
      "type": "concept",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "positional encodings"
      ],
      "description": "Encodings that provide sequence order information in Transformers.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 355,
      "importance": 0.0,
      "inferred_type": "concept"
    },
    {
      "id": "hidden state",
      "type": "concept",
      "documents": [
        "transformer_xl",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "hidden state",
        "hidden states"
      ],
      "description": "The internal state representation in neural networks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.03174776166682731,
      "inferred_type": "concept"
    },
    {
      "id": "logits",
      "type": "concept",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "logits"
      ],
      "description": "Raw output scores from the model before applying Softmax.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 356,
      "importance": 0.0,
      "inferred_type": "concept"
    },
    {
      "id": "long-term dependency",
      "type": "concept",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "long-term dependency"
      ],
      "description": "The ability to model dependencies over long sequences.",
      "pagerank": 0.002471051328056027,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.028335527146287044,
      "inferred_type": "concept"
    },
    {
      "id": "BPTT",
      "type": "method",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "BPTT"
      ],
      "description": "Backpropagation Through Time, a technique for training recurrent neural networks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.0007409478112026825,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.005146341259622738,
      "inferred_type": "method"
    },
    {
      "id": "enwiki8",
      "type": "dataset",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "enwiki8"
      ],
      "description": "A dataset used for evaluating language models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 357,
      "importance": 0.0,
      "inferred_type": "dataset"
    },
    {
      "id": "GPU memory",
      "type": "resource",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "GPU memory"
      ],
      "description": "Graphics Processing Unit memory used for storing data during model training and evaluation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 358,
      "importance": 0.0,
      "inferred_type": "resource"
    },
    {
      "id": "memory augmented neural networks",
      "type": "architecture",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "memory augmented neural networks"
      ],
      "description": "Neural networks that utilize external memory to enhance learning.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 359,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "attention score",
      "type": "metric",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "attention score"
      ],
      "description": "A score that determines the importance of different elements in a sequence.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 360,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "query vector",
      "type": "concept",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "query vector"
      ],
      "description": "A vector representing the current input in the attention mechanism.",
      "pagerank": 0.003103546972723535,
      "centrality": 0,
      "betweenness": 5.419231769704327e-06,
      "eigenvector": 3.1574392278118618e-37,
      "in_degree": 2,
      "out_degree": 1,
      "total_degree": 3,
      "clustering_coefficient": 0,
      "community": 361,
      "importance": 0.039354876284753634,
      "inferred_type": "component"
    },
    {
      "id": "key vector",
      "type": "concept",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "key vector"
      ],
      "description": "A vector used in the attention mechanism to determine relevance.",
      "pagerank": 0.0037882300223508695,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.8229483881270626e-37,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 361,
      "importance": 0.03827817357034528,
      "inferred_type": "component"
    },
    {
      "id": "content-based addressing",
      "type": "method",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "content-based addressing"
      ],
      "description": "A method of addressing memory based on the content of the input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 362,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "location-based key vectors",
      "type": "concept",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "location-based key vectors"
      ],
      "description": "Key vectors that are influenced by the position of the input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 363,
      "importance": 0.0,
      "inferred_type": "concept"
    },
    {
      "id": "Shaw et al. (2018)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Shaw et al. (2018)"
      ],
      "description": "A paper that explored relative positional encodings in machine translation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.006747761666827309,
      "inferred_type": "publication"
    },
    {
      "id": "Huang et al. (2018)",
      "type": "publication",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Huang et al. (2018)"
      ],
      "description": "A paper that applied relative positional encodings in music generation.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.004164650509609902,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.006567816648551173,
      "inferred_type": "publication"
    },
    {
      "id": "Masked-Softmax",
      "type": "method",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Masked-Softmax"
      ],
      "description": "A variant of the softmax function used in attention mechanisms.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 364,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "LayerNorm",
      "type": "method",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "LayerNorm"
      ],
      "description": "A normalization technique applied to layers in neural networks.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 365,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "Positionwise-Feed-Forward",
      "type": "method",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Positionwise-Feed-Forward"
      ],
      "description": "A feed-forward neural network applied independently to each position in the input.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 366,
      "importance": 0.0,
      "inferred_type": "method"
    },
    {
      "id": "absolute positional embedding",
      "type": "concept",
      "documents": [
        "transformer_xl",
        "transformer_xl"
      ],
      "mentions": 2,
      "aliases": [
        "absolute positional embedding",
        "relative positional embedding"
      ],
      "description": "A method for encoding the position of tokens in a sequence.",
      "pagerank": 0.006998360151430364,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 1,
      "total_degree": 2,
      "clustering_coefficient": 0,
      "community": 367,
      "importance": 0.10879350678506476,
      "inferred_type": "concept"
    },
    {
      "id": "u",
      "type": "parameter",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "u"
      ],
      "description": "A trainable parameter introduced to replace the query vector in the attention mechanism.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.8229483881270626e-37,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 361,
      "importance": 0.004838709677419354,
      "inferred_type": "parameter"
    },
    {
      "id": "v",
      "type": "parameter",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "v"
      ],
      "description": "A trainable parameter added to substitute the query vector in the attention mechanism.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.8229483881270626e-37,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 361,
      "importance": 0.004838709677419354,
      "inferred_type": "parameter"
    },
    {
      "id": "bits per character (bpc)",
      "type": "metric",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "bits per character (bpc)"
      ],
      "description": "A metric used to evaluate language models based on the number of bits needed to encode characters.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 368,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Baevski and Auli",
      "type": "people",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Baevski and Auli"
      ],
      "description": "Researchers who contributed to the development of adaptive input representations.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 369,
      "importance": 0.0,
      "inferred_type": "people"
    },
    {
      "id": "Grave et al. (2016)",
      "type": "citation",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Grave et al. (2016)"
      ],
      "description": "Authors of a paper discussing neural cache mechanisms.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 370,
      "importance": 0.0,
      "inferred_type": "citation"
    },
    {
      "id": "Dauphin et al. (2016)",
      "type": "citation",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Dauphin et al. (2016)"
      ],
      "description": "Authors of a paper discussing GCNN architectures.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 371,
      "importance": 0.0,
      "inferred_type": "citation"
    },
    {
      "id": "AWD-LSTM",
      "type": "architecture",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "AWD-LSTM"
      ],
      "description": "A type of LSTM model that incorporates dropout and weight averaging.",
      "pagerank": 0.002471051328056027,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.00459805811514106,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 0,
      "importance": 0.02349681746886769,
      "inferred_type": "architecture"
    },
    {
      "id": "MoS",
      "type": "architecture",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "MoS"
      ],
      "description": "A method proposed by Yang et al. for fine-tuning models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 372,
      "importance": 0.0,
      "inferred_type": "architecture"
    },
    {
      "id": "Khandelwal et al. (2018)",
      "type": "person",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Khandelwal et al. (2018)"
      ],
      "description": "Proposed a method to evaluate Effective Context Length (ECL).",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 373,
      "importance": 0.004838709677419354,
      "inferred_type": "person"
    },
    {
      "id": "Effective Context Length (ECL)",
      "type": "metric",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Effective Context Length (ECL)"
      ],
      "description": "The longest length to which increasing context span leads to performance gain.",
      "pagerank": 0.0021263956143027707,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 1,
      "out_degree": 0,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 373,
      "importance": 0.01722051862198086,
      "inferred_type": "metric"
    },
    {
      "id": "Relative Effective Context Length (RECL)",
      "type": "metric",
      "documents": [
        "transformer_xl"
      ],
      "mentions": 1,
      "aliases": [
        "Relative Effective Context Length (RECL)"
      ],
      "description": "A metric to measure the relative improvement of context length in models.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 374,
      "importance": 0.0,
      "inferred_type": "metric"
    },
    {
      "id": "Layer Normalization (LN)",
      "type": "Technique",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Layer Normalization (LN)"
      ],
      "description": "A technique applied before every block in the Transformer encoder.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 375,
      "importance": 0.0,
      "inferred_type": "Technique"
    },
    {
      "id": "Residual Connections",
      "type": "Technique",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Residual Connections"
      ],
      "description": "Connections added after every block in the Transformer encoder.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 376,
      "importance": 0.0,
      "inferred_type": "Technique"
    },
    {
      "id": "Self-Supervision",
      "type": "Learning Method",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Self-Supervision"
      ],
      "description": "A method explored for training the Vision Transformer.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 66,
      "importance": 0.004838709677419354,
      "inferred_type": "Learning Method"
    },
    {
      "id": "R50x1",
      "type": "model",
      "documents": [
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "R50x1",
        "R50x2"
      ],
      "description": "ResNet model variant",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 377,
      "importance": 0.024999999999999998,
      "inferred_type": "model"
    },
    {
      "id": "R101x1",
      "type": "model",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "R101x1"
      ],
      "description": "ResNet model variant",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 378,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "R152x1",
      "type": "model",
      "documents": [
        "vision_transformer",
        "vision_transformer"
      ],
      "mentions": 2,
      "aliases": [
        "R152x1",
        "R152x2"
      ],
      "description": "ResNet model variant",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 42,
      "importance": 0.024999999999999998,
      "inferred_type": "model"
    },
    {
      "id": "R200x3",
      "type": "model",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "R200x3"
      ],
      "description": "ResNet model variant",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 126,
      "importance": 0.0,
      "inferred_type": "model"
    },
    {
      "id": "hybrids",
      "type": "architecture",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "hybrids"
      ],
      "description": "Combination of ResNet and Vision Transformer architectures",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 0.005882631281630402,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 100,
      "importance": 0.007281099197066708,
      "inferred_type": "architecture"
    },
    {
      "id": "position embeddings",
      "type": "concept",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "position embeddings"
      ],
      "description": "Learned representations that encode the spatial information of image patches.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.3206806712577511e-48,
      "in_degree": 0,
      "out_degree": 1,
      "total_degree": 1,
      "clustering_coefficient": 0,
      "community": 293,
      "importance": 0.004838709677419354,
      "inferred_type": "concept"
    },
    {
      "id": "Google",
      "type": "organization",
      "documents": [
        "vision_transformer"
      ],
      "mentions": 1,
      "aliases": [
        "Google"
      ],
      "description": "The organization where the research was conducted.",
      "pagerank": 0.0011492442558820063,
      "centrality": 0,
      "betweenness": 0.0,
      "eigenvector": 1.365551808297413e-73,
      "in_degree": 0,
      "out_degree": 0,
      "total_degree": 0,
      "clustering_coefficient": 0,
      "community": 191,
      "importance": 0.0,
      "inferred_type": "organization"
    }
  ],
  "edges": [
    {
      "source": "Transformer",
      "target": "Attention Mechanism",
      "relations": [
        "based_on"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Transformer",
      "target": "BLEU",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 0.9
    },
    {
      "source": "Transformer",
      "target": "WMT 2014 English-to-German",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "trained_on"
    },
    {
      "source": "Transformer",
      "target": "WMT 2014 English-to-French",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "trained_on"
    },
    {
      "source": "Transformer",
      "target": "Self-Attention",
      "relations": [
        "is_based_on"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Transformer",
      "target": "Transformer Encoder",
      "relation": "uses",
      "weight": 0.7212033271789551,
      "inferred": true
    },
    {
      "source": "Transformer",
      "target": "Attention Pattern",
      "relation": "uses",
      "weight": 0.7163671851158142,
      "inferred": true
    },
    {
      "source": "Transformer",
      "target": "Transformer Architecture",
      "relation": "uses",
      "weight": 0.7068883776664734,
      "inferred": true
    },
    {
      "source": "Transformer",
      "target": "Hybrid models",
      "relation": "uses",
      "weight": 0.6653774380683899,
      "inferred": true
    },
    {
      "source": "Transformer",
      "target": "hybrids",
      "relation": "uses",
      "weight": 0.6651410460472107,
      "inferred": true
    },
    {
      "source": "Attention Mechanism",
      "target": "Attention(Q, K, V)",
      "relation": "part_of",
      "weight": 0.7541435956954956,
      "inferred": true
    },
    {
      "source": "Attention Mechanism",
      "target": "local, restricted attention mechanisms",
      "relation": "uses",
      "weight": 0.7457857131958008,
      "inferred": true
    },
    {
      "source": "Attention Mechanism",
      "target": "Self-Attention",
      "relation": "uses",
      "weight": 0.7097106575965881,
      "inferred": true
    },
    {
      "source": "Attention Mechanism",
      "target": "self-attention",
      "relation": "uses",
      "weight": 0.7060629725456238,
      "inferred": true
    },
    {
      "source": "Attention Mechanism",
      "target": "attention-based models",
      "relation": "uses",
      "weight": 0.7006927132606506,
      "inferred": true
    },
    {
      "source": "BLEU",
      "target": "BLEU Score",
      "relation": "compared_to",
      "weight": 0.8384098410606384,
      "inferred": true
    },
    {
      "source": "WMT 2014 English-to-German",
      "target": "WMT 2014 English-to-French",
      "relation": "evaluated_on",
      "weight": 0.9430638551712036,
      "inferred": true
    },
    {
      "source": "WMT 2014 English-to-German",
      "target": "WMT2014",
      "relation": "evaluated_on",
      "weight": 0.9087347984313965,
      "inferred": true
    },
    {
      "source": "WMT 2014 English-to-German",
      "target": "WikiText-103",
      "relation": "evaluated_on",
      "weight": 0.652509331703186,
      "inferred": true
    },
    {
      "source": "WMT 2014 English-to-French",
      "target": "WMT2014",
      "relation": "evaluated_on",
      "weight": 0.8725728392601013,
      "inferred": true
    },
    {
      "source": "Google Brain",
      "target": "Ashish Vaswani",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Noam Shazeer",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Niki Parmar",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Jakob Uszkoreit",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Llion Jones",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Aidan N. Gomez",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Łukasz Kaiser",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Illia Polosukhin",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Google Brain",
      "target": "Google",
      "relation": "part_of",
      "weight": 0.6882954835891724,
      "inferred": true
    },
    {
      "source": "Google Research",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "authored"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "vision_transformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Google Research",
      "target": "PaLM",
      "relations": [
        "developed"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "Google Research",
      "target": "Google",
      "relation": "part_of",
      "weight": 0.8129014372825623,
      "inferred": true
    },
    {
      "source": "Ashish Vaswani",
      "target": "Rae et al.",
      "relation": "authored_by",
      "weight": 0.7289313673973083,
      "inferred": true
    },
    {
      "source": "Ashish Vaswani",
      "target": "Niki Parmar",
      "relation": "authored_by",
      "weight": 0.6936880946159363,
      "inferred": true
    },
    {
      "source": "Ashish Vaswani",
      "target": "Illia Polosukhin",
      "relation": "authored_by",
      "weight": 0.6583512425422668,
      "inferred": true
    },
    {
      "source": "Noam Shazeer",
      "target": "Scaled Dot-Product Attention",
      "relation": "uses",
      "weight": 0.7178444862365723,
      "inferred": true
    },
    {
      "source": "Niki Parmar",
      "target": "Rae et al.",
      "relation": "authored_by",
      "weight": 0.666759192943573,
      "inferred": true
    },
    {
      "source": "Niki Parmar",
      "target": "Illia Polosukhin",
      "relation": "authored_by",
      "weight": 0.6585824489593506,
      "inferred": true
    },
    {
      "source": "Jakob Uszkoreit",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Aidan N. Gomez",
      "target": "Łukasz Kaiser",
      "relation": "authored_by",
      "weight": 0.8694811463356018,
      "inferred": true
    },
    {
      "source": "Encoder",
      "target": "Multi-Head Attention",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Encoder",
      "target": "Decoder",
      "relation": "uses",
      "weight": 0.7878592610359192,
      "inferred": true
    },
    {
      "source": "Encoder",
      "target": "visual encoder",
      "relation": "uses",
      "weight": 0.7214633226394653,
      "inferred": true
    },
    {
      "source": "Encoder",
      "target": "Transformer Encoder",
      "relation": "uses",
      "weight": 0.7089172601699829,
      "inferred": true
    },
    {
      "source": "Decoder",
      "target": "Multi-Head Attention",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Decoder",
      "target": "Transformer Encoder",
      "relation": "uses",
      "weight": 0.7223502993583679,
      "inferred": true
    },
    {
      "source": "Decoder",
      "target": "visual encoder",
      "relation": "uses",
      "weight": 0.7037339210510254,
      "inferred": true
    },
    {
      "source": "Decoder",
      "target": "BART",
      "relation": "part_of",
      "weight": 0.6863356232643127,
      "inferred": true
    },
    {
      "source": "Decoder",
      "target": "Longformer-Encoder-Decoder (LED)",
      "relation": "uses",
      "weight": 0.6737117171287537,
      "inferred": true
    },
    {
      "source": "Multi-Head Attention",
      "target": "Attention Function",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Scaled Dot-Product Attention",
      "target": "Attention Function",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Scaled Dot-Product Attention",
      "target": "attention score",
      "relation": "uses",
      "weight": 0.6507177352905273,
      "inferred": true
    },
    {
      "source": "Residual Connection",
      "target": "Encoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Residual Connection",
      "target": "Decoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Layer Normalization",
      "target": "Encoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Layer Normalization",
      "target": "Decoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Layer Normalization",
      "target": "LayerNorm",
      "relation": "uses",
      "weight": 0.8096245527267456,
      "inferred": true
    },
    {
      "source": "Layer Normalization",
      "target": "Layer Normalization (LN)",
      "relation": "uses",
      "weight": 0.7138181328773499,
      "inferred": true
    },
    {
      "source": "Feed-Forward Network",
      "target": "Encoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Feed-Forward Network",
      "target": "Decoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Feed-Forward Network",
      "target": "Positionwise-Feed-Forward",
      "relation": "uses",
      "weight": 0.686059832572937,
      "inferred": true
    },
    {
      "source": "Embedding Layer",
      "target": "Position Embeddings",
      "relation": "uses",
      "weight": 0.7192005515098572,
      "inferred": true
    },
    {
      "source": "Embedding Layer",
      "target": "Classification Token",
      "relation": "part_of",
      "weight": 0.6862131357192993,
      "inferred": true
    },
    {
      "source": "Softmax Function",
      "target": "Decoder",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "Softmax Function",
      "target": "Softmax function",
      "relation": "uses",
      "weight": 0.8280977010726929,
      "inferred": true
    },
    {
      "source": "Softmax Function",
      "target": "logits",
      "relation": "uses",
      "weight": 0.6602149605751038,
      "inferred": true
    },
    {
      "source": "Attention Function",
      "target": "v",
      "relation": "uses",
      "weight": 0.7610126733779907,
      "inferred": true
    },
    {
      "source": "Attention Function",
      "target": "u",
      "relation": "uses",
      "weight": 0.7444425821304321,
      "inferred": true
    },
    {
      "source": "Attention Function",
      "target": "Attention(Q, K, V)",
      "relation": "part_of",
      "weight": 0.6904163360595703,
      "inferred": true
    },
    {
      "source": "Attention Function",
      "target": "self-attention",
      "relation": "uses",
      "weight": 0.6683701872825623,
      "inferred": true
    },
    {
      "source": "Attention Function",
      "target": "attention score",
      "relation": "evaluated_on",
      "weight": 0.6594928503036499,
      "inferred": true
    },
    {
      "source": "Attention(Q, K, V)",
      "target": "Attention",
      "relation": "based_on",
      "weight": 0.676521360874176,
      "inferred": true
    },
    {
      "source": "Attention(Q, K, V)",
      "target": "local, restricted attention mechanisms",
      "relation": "uses",
      "weight": 0.6751890778541565,
      "inferred": true
    },
    {
      "source": "r",
      "target": "self-attention",
      "relation": "uses",
      "weight": 0.6627129912376404,
      "inferred": true
    },
    {
      "source": "r",
      "target": "attention distance",
      "relation": "compared_to",
      "weight": 0.6584138870239258,
      "inferred": true
    },
    {
      "source": "Self-Attention",
      "target": "Transformer",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Self-Attention",
      "target": "self-attention",
      "relation": "uses",
      "weight": 0.8842253088951111,
      "inferred": true
    },
    {
      "source": "Self-Attention",
      "target": "Global Attention",
      "relation": "uses",
      "weight": 0.720845639705658,
      "inferred": true
    },
    {
      "source": "Recurrent Neural Network (RNN)",
      "target": "RNN",
      "relation": "uses",
      "weight": 0.942781925201416,
      "inferred": true
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "target": "CNN",
      "relation": "uses",
      "weight": 0.8800655007362366,
      "inferred": true
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "target": "ResNet-50",
      "relation": "part_of",
      "weight": 0.7220489382743835,
      "inferred": true
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "target": "EfficientNet",
      "relation": "uses",
      "weight": 0.7010082006454468,
      "inferred": true
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "target": "RegNet",
      "relation": "uses",
      "weight": 0.6759514212608337,
      "inferred": true
    },
    {
      "source": "Convolutional Neural Network (CNN)",
      "target": "DenseNet",
      "relation": "uses",
      "weight": 0.6708697080612183,
      "inferred": true
    },
    {
      "source": "Positional Encoding",
      "target": "Transformer",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Positional Encoding",
      "target": "Transformer-XL",
      "relations": [
        "is used in"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Positional Encoding",
      "target": "Positional Encoding",
      "relations": [
        "extends"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Positional Encoding",
      "target": "absolute positional embedding",
      "relation": "part_of",
      "weight": 0.7900352478027344,
      "inferred": true
    },
    {
      "source": "Positional Encoding",
      "target": "positional encodings",
      "relation": "uses",
      "weight": 0.7781039476394653,
      "inferred": true
    },
    {
      "source": "Adam Optimizer",
      "target": "Adam",
      "relation": "semantically_related",
      "weight": 0.9547815918922424,
      "inferred": true
    },
    {
      "source": "Adam Optimizer",
      "target": "AdamW",
      "relation": "compared_to",
      "weight": 0.9255989193916321,
      "inferred": true
    },
    {
      "source": "Adam Optimizer",
      "target": "Adam optimizer",
      "relation": "uses",
      "weight": 0.9152991771697998,
      "inferred": true
    },
    {
      "source": "Byte-Pair Encoding",
      "target": "Byte-Pair Encoding (BPE)",
      "relation": "compared_to",
      "weight": 0.7318257689476013,
      "inferred": true
    },
    {
      "source": "Word-Piece Vocabulary",
      "target": "SentencePiece",
      "relation": "uses",
      "weight": 0.7553209066390991,
      "inferred": true
    },
    {
      "source": "Residual Dropout",
      "target": "dropout",
      "relation": "part_of",
      "weight": 0.6941553354263306,
      "inferred": true
    },
    {
      "source": "Label Smoothing",
      "target": "label smoothing",
      "relation": "part_of",
      "weight": 0.6825858950614929,
      "inferred": true
    },
    {
      "source": "NVIDIA P100 GPU",
      "target": "P100 GPUs",
      "relation": "compared_to",
      "weight": 0.8412156701087952,
      "inferred": true
    },
    {
      "source": "NVIDIA P100 GPU",
      "target": "V100 GPU",
      "relation": "compared_to",
      "weight": 0.7613207697868347,
      "inferred": true
    },
    {
      "source": "NVIDIA P100 GPU",
      "target": "GPU memory",
      "relation": "part_of",
      "weight": 0.6658787727355957,
      "inferred": true
    },
    {
      "source": "P100 GPUs",
      "target": "V100 GPU",
      "relation": "compared_to",
      "weight": 0.738069474697113,
      "inferred": true
    },
    {
      "source": "Penn Treebank",
      "target": "Wall Street Journal",
      "relations": [
        "contains"
      ],
      "mentions": 1,
      "sources": [
        "attention_is_all_you_need"
      ],
      "confidence": 0.9,
      "relation": "evaluated_on"
    },
    {
      "source": "Attention Heads",
      "target": "Attention Pattern",
      "relation": "uses",
      "weight": 0.6646472811698914,
      "inferred": true
    },
    {
      "source": "Attention Heads",
      "target": "Attention",
      "relation": "part_of",
      "weight": 0.6538415551185608,
      "inferred": true
    },
    {
      "source": "Semi-supervised Setting",
      "target": "Self-Supervised Learning",
      "relation": "uses",
      "weight": 0.7739365100860596,
      "inferred": true
    },
    {
      "source": "Semi-supervised Setting",
      "target": "self-supervised pre-training",
      "relation": "uses",
      "weight": 0.7582687735557556,
      "inferred": true
    },
    {
      "source": "Vinyals & Kaiser (2014)",
      "target": "Dyer et al. (2016)",
      "relation": "authored_by",
      "weight": 0.9135938286781311,
      "inferred": true
    },
    {
      "source": "Vinyals & Kaiser (2014)",
      "target": "Zhu et al. (2013)",
      "relation": "authored_by",
      "weight": 0.8914799690246582,
      "inferred": true
    },
    {
      "source": "Vinyals & Kaiser (2014)",
      "target": "Luong et al. (2015)",
      "relation": "authored_by",
      "weight": 0.8861629366874695,
      "inferred": true
    },
    {
      "source": "Vinyals & Kaiser (2014)",
      "target": "McClosky et al. (2006)",
      "relation": "authored_by",
      "weight": 0.8746878504753113,
      "inferred": true
    },
    {
      "source": "Vinyals & Kaiser (2014)",
      "target": "Huang & Harper (2009)",
      "relation": "authored_by",
      "weight": 0.8315191268920898,
      "inferred": true
    },
    {
      "source": "Dyer et al. (2016)",
      "target": "Zhu et al. (2013)",
      "relation": "authored_by",
      "weight": 0.9508126378059387,
      "inferred": true
    },
    {
      "source": "Dyer et al. (2016)",
      "target": "Luong et al. (2015)",
      "relation": "authored_by",
      "weight": 0.9443243741989136,
      "inferred": true
    },
    {
      "source": "Dyer et al. (2016)",
      "target": "McClosky et al. (2006)",
      "relation": "authored_by",
      "weight": 0.9137487411499023,
      "inferred": true
    },
    {
      "source": "Dyer et al. (2016)",
      "target": "Huang & Harper (2009)",
      "relation": "authored_by",
      "weight": 0.8765565156936646,
      "inferred": true
    },
    {
      "source": "Zhu et al. (2013)",
      "target": "Luong et al. (2015)",
      "relation": "semantically_related",
      "weight": 0.9426431655883789,
      "inferred": true
    },
    {
      "source": "Zhu et al. (2013)",
      "target": "Huang & Harper (2009)",
      "relation": "authored_by",
      "weight": 0.926629900932312,
      "inferred": true
    },
    {
      "source": "Zhu et al. (2013)",
      "target": "McClosky et al. (2006)",
      "relation": "trained_on",
      "weight": 0.8991572260856628,
      "inferred": true
    },
    {
      "source": "Huang & Harper (2009)",
      "target": "Luong et al. (2015)",
      "relation": "semantically_related",
      "weight": 0.8638743162155151,
      "inferred": true
    },
    {
      "source": "Huang & Harper (2009)",
      "target": "McClosky et al. (2006)",
      "relation": "semantically_related",
      "weight": 0.8283951878547668,
      "inferred": true
    },
    {
      "source": "McClosky et al. (2006)",
      "target": "Luong et al. (2015)",
      "relation": "semantically_related",
      "weight": 0.9208025932312012,
      "inferred": true
    },
    {
      "source": "attention-based models",
      "target": "local, restricted attention mechanisms",
      "relation": "uses",
      "weight": 0.6531352400779724,
      "inferred": true
    },
    {
      "source": "images",
      "target": "video",
      "relation": "applied_to",
      "weight": 0.7321035861968994,
      "inferred": true
    },
    {
      "source": "images",
      "target": "image-text pairs",
      "relation": "applied_to",
      "weight": 0.719761073589325,
      "inferred": true
    },
    {
      "source": "images",
      "target": "Visual Genome",
      "relation": "evaluated_on",
      "weight": 0.6699697971343994,
      "inferred": true
    },
    {
      "source": "local, restricted attention mechanisms",
      "target": "Attention",
      "relation": "part_of",
      "weight": 0.6664206385612488,
      "inferred": true
    },
    {
      "source": "code repository",
      "target": "GitHub",
      "relation": "uses",
      "weight": 0.775306224822998,
      "inferred": true
    },
    {
      "source": "CLIP",
      "target": "zero-shot transfer",
      "relations": [
        "achieves"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "CLIP",
      "target": "WIT",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "CLIP",
      "target": "zero-shot learning",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "CLIP",
      "target": "cross_entropy_loss",
      "relations": [
        "is_trained_with"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9
    },
    {
      "source": "CLIP",
      "target": "ImageNet",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "CLIP",
      "target": "ResNet-50",
      "relations": [
        "matches_performance_of"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.8
    },
    {
      "source": "CLIP",
      "target": "Visual N-Grams",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "CLIP",
      "target": "YFCC100M",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "CLIP",
      "target": "Yahoo",
      "relations": [
        "reduces_errors_on"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "CLIP",
      "target": "SUN",
      "relations": [
        "doubles_accuracy_on"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "CLIP",
      "target": "visual feature Z",
      "relations": [
        "provides"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "GPT-3",
      "target": "OpenAI",
      "relations": [
        "developed_by"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 1.0
    },
    {
      "source": "GPT-3",
      "target": "few-shot learning",
      "relations": [
        "applies_to"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "GPT-3",
      "target": "translation",
      "relations": [
        "achieves_performance_on"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "applied_to"
    },
    {
      "source": "GPT-3",
      "target": "question-answering",
      "relations": [
        "achieves_performance_on"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "applied_to"
    },
    {
      "source": "GPT-3",
      "target": "cloze tasks",
      "relations": [
        "achieves_performance_on"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "applied_to"
    },
    {
      "source": "GPT-3",
      "target": "Natural Questions",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "CoQA",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "TriviaQA",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "in-context learning",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "GPT-3",
      "target": "V100 GPU",
      "relations": [
        "uses"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.9
    },
    {
      "source": "GPT-3",
      "target": "task-specific fine-tuning",
      "relations": [
        "requires"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "GPT-3",
      "target": "CommonCrawl",
      "relations": [
        "is trained on"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "Common Crawl",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "WebText",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "Books1",
      "relations": [
        "uses"
      ],
      "mentions": 2,
      "sources": [
        "gpt3",
        "gpt3"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "Wikipedia",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "GPT-3",
      "target": "LLaMA-I",
      "relations": [
        "is_compared_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "GPT-3",
      "target": "GLaM",
      "relation": "semantically_related",
      "weight": 0.7586455345153809,
      "inferred": true
    },
    {
      "source": "GPT-3",
      "target": "X-GPT",
      "relation": "semantically_related",
      "weight": 0.7125504612922668,
      "inferred": true
    },
    {
      "source": "GPT-3",
      "target": "Alpaca",
      "relation": "semantically_related",
      "weight": 0.6523198485374451,
      "inferred": true
    },
    {
      "source": "ImageNet",
      "target": "ImageNet-21k",
      "relations": [
        "is_a_subset_of"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.85,
      "relation": "evaluated_on"
    },
    {
      "source": "ImageNet",
      "target": "fine-tuning",
      "relations": [
        "used_for"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "ImageNet",
      "target": "ILSVRC-2012 ImageNet",
      "relation": "evaluated_on",
      "weight": 0.8054940700531006,
      "inferred": true
    },
    {
      "source": "ImageNet",
      "target": "ResNet-50",
      "relation": "semantically_related",
      "weight": 0.7329657077789307,
      "inferred": true
    },
    {
      "source": "ImageNet",
      "target": "Yahoo",
      "relation": "evaluated_on",
      "weight": 0.7266128063201904,
      "inferred": true
    },
    {
      "source": "ImageNet",
      "target": "AlexNet",
      "relation": "uses",
      "weight": 0.710264265537262,
      "inferred": true
    },
    {
      "source": "ImageNet",
      "target": "SUN",
      "relation": "evaluated_on",
      "weight": 0.682630717754364,
      "inferred": true
    },
    {
      "source": "ResNet-50",
      "target": "CLIP",
      "relations": [
        "matches accuracy of"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.85
    },
    {
      "source": "ResNet-50",
      "target": "CNN",
      "relation": "uses",
      "weight": 0.7537044882774353,
      "inferred": true
    },
    {
      "source": "ResNet-50",
      "target": "AlexNet",
      "relation": "uses",
      "weight": 0.6976622939109802,
      "inferred": true
    },
    {
      "source": "ResNet-50",
      "target": "Inception-V4",
      "relation": "uses",
      "weight": 0.6962071061134338,
      "inferred": true
    },
    {
      "source": "VirTex",
      "target": "ConVIRT",
      "relation": "semantically_related",
      "weight": 0.7436332702636719,
      "inferred": true
    },
    {
      "source": "ICMLM",
      "target": "MLM",
      "relation": "uses",
      "weight": 0.7057267427444458,
      "inferred": true
    },
    {
      "source": "ICMLM",
      "target": "Masked Language Modeling (MLM)",
      "relation": "applied_to",
      "weight": 0.6978846192359924,
      "inferred": true
    },
    {
      "source": "ICMLM",
      "target": "masked language modeling",
      "relation": "uses",
      "weight": 0.6617890000343323,
      "inferred": true
    },
    {
      "source": "Dai & Le",
      "target": "Peters et al.",
      "relation": "semantically_related",
      "weight": 0.8377844095230103,
      "inferred": true
    },
    {
      "source": "Peters et al.",
      "target": "pre-training",
      "relation": "uses",
      "weight": 0.6898380517959595,
      "inferred": true
    },
    {
      "source": "OpenAI",
      "target": "CLIP",
      "relations": [
        "developed"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.9
    },
    {
      "source": "OpenAI",
      "target": "Johns Hopkins University",
      "relations": [
        "associated_with"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.7
    },
    {
      "source": "OpenAI",
      "target": "GPT-3",
      "relations": [
        "developed"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.9
    },
    {
      "source": "zero-shot transfer",
      "target": "meta-learning",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "zero-shot transfer",
      "target": "zero-shot learning",
      "relation": "uses",
      "weight": 0.7816219925880432,
      "inferred": true
    },
    {
      "source": "zero-shot transfer",
      "target": "Zero-shot",
      "relation": "semantically_related",
      "weight": 0.7058413028717041,
      "inferred": true
    },
    {
      "source": "zero-shot transfer",
      "target": "MOTIVATION",
      "relation": "semantically_related",
      "weight": 0.7004518508911133,
      "inferred": true
    },
    {
      "source": "zero-shot transfer",
      "target": "Visual N-Grams",
      "relation": "uses",
      "weight": 0.6797206997871399,
      "inferred": true
    },
    {
      "source": "JFT-300M",
      "target": "JFT",
      "relation": "evaluated_on",
      "weight": 0.7463864684104919,
      "inferred": true
    },
    {
      "source": "GPT",
      "target": "Transformers",
      "relation": "uses",
      "weight": 0.7036779522895813,
      "inferred": true
    },
    {
      "source": "GPT",
      "target": "RoBERTa-large",
      "relation": "semantically_related",
      "weight": 0.659295380115509,
      "inferred": true
    },
    {
      "source": "Li et al. (2017)",
      "target": "Lei Ba et al. (2015)",
      "relation": "semantically_related",
      "weight": 0.7031431198120117,
      "inferred": true
    },
    {
      "source": "Li et al. (2017)",
      "target": "Elhoseiny et al. (2013)",
      "relation": "semantically_related",
      "weight": 0.678229808807373,
      "inferred": true
    },
    {
      "source": "Kolesnikov et al. (2019)",
      "target": "Dosovitskiy et al. (2020)",
      "relation": "authored_by",
      "weight": 0.7816900014877319,
      "inferred": true
    },
    {
      "source": "Kolesnikov et al. (2019)",
      "target": "Kaplan et al. (2020)",
      "relation": "authored_by",
      "weight": 0.7800456881523132,
      "inferred": true
    },
    {
      "source": "Kolesnikov et al. (2019)",
      "target": "Hestness et al. (2017)",
      "relation": "authored_by",
      "weight": 0.712921679019928,
      "inferred": true
    },
    {
      "source": "Dosovitskiy et al. (2020)",
      "target": "Kaplan et al. (2020)",
      "relation": "authored_by",
      "weight": 0.7515876889228821,
      "inferred": true
    },
    {
      "source": "Dosovitskiy et al. (2020)",
      "target": "Hestness et al. (2017)",
      "relation": "authored_by",
      "weight": 0.7230144143104553,
      "inferred": true
    },
    {
      "source": "Hestness et al. (2017)",
      "target": "Kaplan et al. (2020)",
      "relation": "authored_by",
      "weight": 0.8390889763832092,
      "inferred": true
    },
    {
      "source": "McCann et al. (2017)",
      "target": "Chen et al. (2020)",
      "relation": "authored_by",
      "weight": 0.7997933626174927,
      "inferred": true
    },
    {
      "source": "McCann et al. (2017)",
      "target": "Oord et al. (2018)",
      "relation": "authored_by",
      "weight": 0.7412461042404175,
      "inferred": true
    },
    {
      "source": "McCann et al. (2017)",
      "target": "Tian et al. (2019)",
      "relation": "authored_by",
      "weight": 0.7034665942192078,
      "inferred": true
    },
    {
      "source": "McCann et al. (2017)",
      "target": "Zhang et al. (2020)",
      "relation": "authored_by",
      "weight": 0.6585061550140381,
      "inferred": true
    },
    {
      "source": "WIT",
      "target": "WebText",
      "relation": "evaluated_on",
      "weight": 0.6969284415245056,
      "inferred": true
    },
    {
      "source": "WIT",
      "target": "CC3M",
      "relation": "evaluated_on",
      "weight": 0.6554567813873291,
      "inferred": true
    },
    {
      "source": "WIT",
      "target": "Yahoo",
      "relation": "evaluated_on",
      "weight": 0.6528169512748718,
      "inferred": true
    },
    {
      "source": "Vision Transformer",
      "target": "Vision Transformer (ViT)",
      "relation": "uses",
      "weight": 0.7863581776618958,
      "inferred": true
    },
    {
      "source": "Vision Transformer",
      "target": "ViT",
      "relation": "uses",
      "weight": 0.7733107209205627,
      "inferred": true
    },
    {
      "source": "Vision Transformer",
      "target": "Hybrid models",
      "relation": "uses",
      "weight": 0.756807804107666,
      "inferred": true
    },
    {
      "source": "Vision Transformer",
      "target": "hybrids",
      "relation": "uses",
      "weight": 0.7288452982902527,
      "inferred": true
    },
    {
      "source": "Vision Transformer",
      "target": "DeiT-S",
      "relation": "semantically_related",
      "weight": 0.6609932780265808,
      "inferred": true
    },
    {
      "source": "EfficientNet",
      "target": "ResNet-101",
      "relations": [
        "is_related_to"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.6,
      "relation": "uses"
    },
    {
      "source": "EfficientNet",
      "target": "RegNet",
      "relation": "uses",
      "weight": 0.809444785118103,
      "inferred": true
    },
    {
      "source": "EfficientNet",
      "target": "CNN",
      "relation": "uses",
      "weight": 0.724365234375,
      "inferred": true
    },
    {
      "source": "EfficientNet",
      "target": "AlexNet",
      "relation": "uses",
      "weight": 0.7004059553146362,
      "inferred": true
    },
    {
      "source": "EfficientNet",
      "target": "DenseNet",
      "relation": "uses",
      "weight": 0.699659526348114,
      "inferred": true
    },
    {
      "source": "WebText",
      "target": "Common Crawl",
      "relations": [
        "augments"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.85,
      "relation": "evaluated_on"
    },
    {
      "source": "Zhang et al. (2020)",
      "target": "Chen et al. (2020)",
      "relation": "authored_by",
      "weight": 0.7432700395584106,
      "inferred": true
    },
    {
      "source": "Zhang et al. (2020)",
      "target": "Oord et al. (2018)",
      "relation": "authored_by",
      "weight": 0.7351587414741516,
      "inferred": true
    },
    {
      "source": "Zhang et al. (2020)",
      "target": "Tian et al. (2019)",
      "relation": "authored_by",
      "weight": 0.7259626388549805,
      "inferred": true
    },
    {
      "source": "Oord et al. (2018)",
      "target": "Tian et al. (2019)",
      "relation": "authored_by",
      "weight": 0.7768535017967224,
      "inferred": true
    },
    {
      "source": "Oord et al. (2018)",
      "target": "Chen et al. (2020)",
      "relation": "authored_by",
      "weight": 0.7707624435424805,
      "inferred": true
    },
    {
      "source": "Oord et al. (2018)",
      "target": "Hénaff et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.6622297167778015,
      "inferred": true
    },
    {
      "source": "Chen et al. (2020)",
      "target": "Tian et al. (2019)",
      "relation": "authored_by",
      "weight": 0.7703877687454224,
      "inferred": true
    },
    {
      "source": "Tian et al. (2019)",
      "target": "Hénaff et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.6675143241882324,
      "inferred": true
    },
    {
      "source": "ResNet-101",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is compared_with",
        "is_compared_with"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "reformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "ResNet-101",
      "target": "ViT",
      "relations": [
        "is_used_as_baseline_for"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "ResNet-101",
      "target": "Transformer",
      "relations": [
        "applies_before"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.7,
      "relation": "uses"
    },
    {
      "source": "ResNet-101",
      "target": "DenseNet",
      "relation": "uses",
      "weight": 0.6618047952651978,
      "inferred": true
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "Transformer",
      "relations": [
        "is based_on",
        "is_based_on"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "reformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ImageNet-21k",
      "relations": [
        "is trained_on",
        "is_pretrained_on"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "JFT-300M",
      "relations": [
        "is trained_on",
        "is_pretrained_on"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ImageNet",
      "relations": [
        "is evaluated_on"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "CIFAR-10",
      "relations": [
        "is evaluated_on"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "VTAB",
      "relations": [
        "is evaluated_on"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "Transformer Encoder",
      "relations": [
        "is_based_on"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "MLP Head",
      "relations": [
        "uses"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ResNet-101",
      "relations": [
        "is_compared_with"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "self-attention",
      "relations": [
        "uses"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "Transformers",
      "relations": [
        "is based_on"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ViT",
      "relation": "uses",
      "weight": 0.9625636339187622,
      "inferred": true
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ViT-Base",
      "relation": "semantically_related",
      "weight": 0.771498441696167,
      "inferred": true
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ViT-B/32",
      "relation": "semantically_related",
      "weight": 0.7473771572113037,
      "inferred": true
    },
    {
      "source": "Vision Transformer (ViT)",
      "target": "ViT-Large",
      "relation": "semantically_related",
      "weight": 0.7453644275665283,
      "inferred": true
    },
    {
      "source": "Adam optimizer",
      "target": "Adam",
      "relation": "semantically_related",
      "weight": 0.9104635119438171,
      "inferred": true
    },
    {
      "source": "Adam optimizer",
      "target": "AdamW",
      "relation": "semantically_related",
      "weight": 0.8778825998306274,
      "inferred": true
    },
    {
      "source": "zero-shot learning",
      "target": "Zero-shot",
      "relation": "semantically_related",
      "weight": 0.7239775061607361,
      "inferred": true
    },
    {
      "source": "zero-shot learning",
      "target": "Lei Ba et al. (2015)",
      "relation": "semantically_related",
      "weight": 0.7202097773551941,
      "inferred": true
    },
    {
      "source": "zero-shot learning",
      "target": "Visual N-Grams",
      "relation": "uses",
      "weight": 0.7110934257507324,
      "inferred": true
    },
    {
      "source": "zero-shot learning",
      "target": "few-shot learning",
      "relation": "uses",
      "weight": 0.7011927366256714,
      "inferred": true
    },
    {
      "source": "CIFAR-10",
      "target": "TinyImages",
      "relation": "evaluated_on",
      "weight": 0.8276534676551819,
      "inferred": true
    },
    {
      "source": "CIFAR-10",
      "target": "Yahoo",
      "relation": "evaluated_on",
      "weight": 0.6810511350631714,
      "inferred": true
    },
    {
      "source": "Visual N-Grams",
      "target": "CLIP",
      "relations": [
        "compares_to"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.85
    },
    {
      "source": "Visual N-Grams",
      "target": "Elhoseiny et al. (2013)",
      "relations": [
        "dates_back_to"
      ],
      "mentions": 1,
      "sources": [
        "clip"
      ],
      "confidence": 0.7
    },
    {
      "source": "hypernetwork",
      "target": "DenseNet",
      "relation": "uses",
      "weight": 0.6647126078605652,
      "inferred": true
    },
    {
      "source": "Inception-V4",
      "target": "GoogleNet",
      "relation": "uses",
      "weight": 0.6883752942085266,
      "inferred": true
    },
    {
      "source": "Lei Ba et al. (2015)",
      "target": "Elhoseiny et al. (2013)",
      "relation": "semantically_related",
      "weight": 0.8001221418380737,
      "inferred": true
    },
    {
      "source": "Flowers102",
      "target": "Oxford Flowers-102",
      "relation": "evaluated_on",
      "weight": 0.9247950911521912,
      "inferred": true
    },
    {
      "source": "SUN",
      "target": "Yahoo",
      "relation": "evaluated_on",
      "weight": 0.6867550015449524,
      "inferred": true
    },
    {
      "source": "Yahoo",
      "target": "StackExchange",
      "relation": "evaluated_on",
      "weight": 0.6608465909957886,
      "inferred": true
    },
    {
      "source": "Alexey Dosovitskiy",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Lucas Beyer",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Alexander Kolesnikov",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Alexander Kolesnikov",
      "target": "Dirk Weissenborn",
      "relation": "authored_by",
      "weight": 0.6820568442344666,
      "inferred": true
    },
    {
      "source": "Dirk Weissenborn",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Dirk Weissenborn",
      "target": "Stephen Lin",
      "relation": "authored_by",
      "weight": 0.6974961757659912,
      "inferred": true
    },
    {
      "source": "Dirk Weissenborn",
      "target": "Matthias Minderer",
      "relation": "authored_by",
      "weight": 0.6621803641319275,
      "inferred": true
    },
    {
      "source": "Dirk Weissenborn",
      "target": "Yutong Lin",
      "relation": "authored_by",
      "weight": 0.6517271995544434,
      "inferred": true
    },
    {
      "source": "Xiaohua Zhai",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Xiaohua Zhai",
      "target": "Ze Liu",
      "relation": "authored_by",
      "weight": 0.6801357269287109,
      "inferred": true
    },
    {
      "source": "Thomas Unterthiner",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Mostafa Dehghani",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Matthias Minderer",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Georg Heigold",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Sylvain Gelly",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Neil Houlsby",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is a co-author of"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "ImageNet-21k",
      "target": "ILSVRC-2012 ImageNet",
      "relations": [
        "is_a_superset_of"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "evaluated_on"
    },
    {
      "source": "ImageNet-21k",
      "target": "ImageNet",
      "relations": [
        "is_a_superset_of"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.85,
      "relation": "evaluated_on"
    },
    {
      "source": "ImageNet-21k",
      "target": "pre-training",
      "relations": [
        "used_for"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "MLP Head",
      "target": "MLP",
      "relation": "uses",
      "weight": 0.7587341070175171,
      "inferred": true
    },
    {
      "source": "Transformer Encoder",
      "target": "Vaswani et al. (2017)",
      "relations": [
        "is_inspired_by"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Transformer Encoder",
      "target": "Longformer",
      "relation": "uses",
      "weight": 0.7063381671905518,
      "inferred": true
    },
    {
      "source": "Transformer Encoder",
      "target": "visual encoder",
      "relation": "uses",
      "weight": 0.7058371901512146,
      "inferred": true
    },
    {
      "source": "Multi-Head Self-Attention (MSA)",
      "target": "SW-MSA",
      "relation": "semantically_related",
      "weight": 0.7245185375213623,
      "inferred": true
    },
    {
      "source": "Multi-Head Self-Attention (MSA)",
      "target": "W-MSA",
      "relation": "semantically_related",
      "weight": 0.6998966336250305,
      "inferred": true
    },
    {
      "source": "Position Embeddings",
      "target": "Longformer",
      "relations": [
        "supports"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "Position Embeddings",
      "target": "absolute positional embedding",
      "relation": "semantically_related",
      "weight": 0.6846851706504822,
      "inferred": true
    },
    {
      "source": "Self-Supervised Learning",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is_used_in",
        "is_applied_to"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "reformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Self-Supervised Learning",
      "target": "self-supervised pre-training",
      "relation": "uses",
      "weight": 0.8988746404647827,
      "inferred": true
    },
    {
      "source": "ICLR 2021",
      "target": "ICLR2021",
      "relation": "semantically_related",
      "weight": 0.9482333064079285,
      "inferred": true
    },
    {
      "source": "Sun et al. (2017)",
      "target": "Djolonga et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.7070634961128235,
      "inferred": true
    },
    {
      "source": "Sun et al. (2017)",
      "target": "Kolesnikov et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.6599224209785461,
      "inferred": true
    },
    {
      "source": "Sun et al. (2017)",
      "target": "Wu et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.6574233770370483,
      "inferred": true
    },
    {
      "source": "Kolesnikov et al. (2020)",
      "target": "Djolonga et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.9077143669128418,
      "inferred": true
    },
    {
      "source": "Vaswani et al. (2017)",
      "target": "Transformer architecture",
      "relations": [
        "introduces",
        "proposed",
        "introduced"
      ],
      "mentions": 5,
      "sources": [
        "deit",
        "transformer_xl",
        "transformer_xl",
        "transformer_xl",
        "vision_transformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Vaswani et al. (2017)",
      "target": "SPP+19",
      "relation": "semantically_related",
      "weight": 0.7914090752601624,
      "inferred": true
    },
    {
      "source": "Vaswani et al. (2017)",
      "target": "Tur20",
      "relation": "semantically_related",
      "weight": 0.7784999012947083,
      "inferred": true
    },
    {
      "source": "Vaswani et al. (2017)",
      "target": "RNSS18",
      "relation": "semantically_related",
      "weight": 0.7651750445365906,
      "inferred": true
    },
    {
      "source": "Vaswani et al. (2017)",
      "target": "Ye et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7623119950294495,
      "inferred": true
    },
    {
      "source": "Vaswani et al. (2017)",
      "target": "Rae et al.",
      "relation": "semantically_related",
      "weight": 0.7420013546943665,
      "inferred": true
    },
    {
      "source": "ILSVRC-2012 ImageNet",
      "target": "ImageNet-21k",
      "relations": [
        "is_a_subset_of"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "evaluated_on"
    },
    {
      "source": "Oxford-IIIT Pets",
      "target": "Oxford Flowers-102",
      "relation": "evaluated_on",
      "weight": 0.6568697690963745,
      "inferred": true
    },
    {
      "source": "ViT-Base",
      "target": "BERT",
      "relations": [
        "is_based_on"
      ],
      "mentions": 3,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "ViT-Base",
      "target": "ViT-Large",
      "relation": "semantically_related",
      "weight": 0.9579446911811829,
      "inferred": true
    },
    {
      "source": "ViT-Base",
      "target": "ViT-B/32",
      "relation": "semantically_related",
      "weight": 0.8616853356361389,
      "inferred": true
    },
    {
      "source": "ViT-Base",
      "target": "ViT",
      "relation": "uses",
      "weight": 0.7724475264549255,
      "inferred": true
    },
    {
      "source": "ViT-Base",
      "target": "ViT-H/14",
      "relation": "semantically_related",
      "weight": 0.7677029967308044,
      "inferred": true
    },
    {
      "source": "ViT-Large",
      "target": "BERT",
      "relations": [
        "is_based_on"
      ],
      "mentions": 6,
      "sources": [
        "deit",
        "deit",
        "reformer",
        "reformer",
        "vision_transformer",
        "vision_transformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "ViT-Large",
      "target": "ViT-B/32",
      "relation": "semantically_related",
      "weight": 0.8380758166313171,
      "inferred": true
    },
    {
      "source": "ViT-Large",
      "target": "ViT-H/14",
      "relation": "semantically_related",
      "weight": 0.7760536074638367,
      "inferred": true
    },
    {
      "source": "ViT-Large",
      "target": "ViT/DeiT",
      "relation": "uses",
      "weight": 0.7328646779060364,
      "inferred": true
    },
    {
      "source": "Adam",
      "target": "AdamW",
      "relation": "semantically_related",
      "weight": 0.9547156095504761,
      "inferred": true
    },
    {
      "source": "SGD",
      "target": "fine-tuning",
      "relations": [
        "is_used_for"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "reformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "SGD",
      "target": "AdamW",
      "relation": "semantically_related",
      "weight": 0.6937338709831238,
      "inferred": true
    },
    {
      "source": "Noisy Student",
      "target": "ViT-H/14",
      "relations": [
        "is_compared_with",
        "is_compared_to"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.85
    },
    {
      "source": "Noisy Student",
      "target": "S4L",
      "relation": "uses",
      "weight": 0.6806342005729675,
      "inferred": true
    },
    {
      "source": "Big Transfer (BiT)",
      "target": "ViT-H/14",
      "relations": [
        "is_compared_with",
        "is_compared_to"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.85
    },
    {
      "source": "ViT-L/16",
      "target": "Big Transfer (BiT)",
      "relations": [
        "compares_to"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.9
    },
    {
      "source": "ViT-L/16",
      "target": "ImageNet-21k",
      "relations": [
        "pre_trained_on",
        "is_pretrained_on"
      ],
      "mentions": 4,
      "sources": [
        "deit",
        "reformer",
        "vision_transformer",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "ViT-L/16",
      "target": "TPUv3",
      "relations": [
        "trained_using"
      ],
      "mentions": 2,
      "sources": [
        "deit",
        "reformer"
      ],
      "confidence": 0.8
    },
    {
      "source": "ViT-L/16",
      "target": "JFT-300M",
      "relations": [
        "is_pretrained_on"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "ViT-L/16",
      "target": "VTAB",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "ViT-L/16",
      "target": "ViT-H/14",
      "relation": "semantically_related",
      "weight": 0.684158205986023,
      "inferred": true
    },
    {
      "source": "TPUv3",
      "target": "TPUv4",
      "relation": "semantically_related",
      "weight": 0.7064898014068604,
      "inferred": true
    },
    {
      "source": "TPUv3",
      "target": "TPU v4 Pods",
      "relation": "semantically_related",
      "weight": 0.6996963024139404,
      "inferred": true
    },
    {
      "source": "TPUv3",
      "target": "TPU v4",
      "relation": "semantically_related",
      "weight": 0.6608239412307739,
      "inferred": true
    },
    {
      "source": "BiT",
      "target": "ViT",
      "relations": [
        "compared_to"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.75,
      "relation": "uses"
    },
    {
      "source": "BiT",
      "target": "VIVI",
      "relation": "uses",
      "weight": 0.7597715258598328,
      "inferred": true
    },
    {
      "source": "ViT-H/14",
      "target": "Noisy Student",
      "relations": [
        "compares_to"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.9
    },
    {
      "source": "ViT-H/14",
      "target": "BiT",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "ViT-H/14",
      "target": "ViT-B/32",
      "relation": "semantically_related",
      "weight": 0.7406438589096069,
      "inferred": true
    },
    {
      "source": "ViT-H/14",
      "target": "ViT/DeiT",
      "relation": "uses",
      "weight": 0.6895323991775513,
      "inferred": true
    },
    {
      "source": "ViT-B/32",
      "target": "ViT",
      "relation": "uses",
      "weight": 0.7737232446670532,
      "inferred": true
    },
    {
      "source": "Hybrid models",
      "target": "ViT",
      "relations": [
        "outperform"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.6,
      "relation": "uses"
    },
    {
      "source": "Hybrid models",
      "target": "hybrids",
      "relation": "uses",
      "weight": 0.9152467846870422,
      "inferred": true
    },
    {
      "source": "Attention",
      "target": "Attention Pattern",
      "relation": "uses",
      "weight": 0.7179822325706482,
      "inferred": true
    },
    {
      "source": "self-attention",
      "target": "Swin Transformer",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "CNN",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is_compared_with"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "CNN",
      "target": "AlexNet",
      "relation": "uses",
      "weight": 0.7224823236465454,
      "inferred": true
    },
    {
      "source": "CNN",
      "target": "RegNet",
      "relation": "uses",
      "weight": 0.7063250541687012,
      "inferred": true
    },
    {
      "source": "masked language modeling",
      "target": "Devlin et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.8139153122901917,
      "inferred": true
    },
    {
      "source": "masked language modeling",
      "target": "Masked Language Modeling (MLM)",
      "relation": "semantically_related",
      "weight": 0.7699177861213684,
      "inferred": true
    },
    {
      "source": "masked language modeling",
      "target": "MLM",
      "relation": "uses",
      "weight": 0.7625402808189392,
      "inferred": true
    },
    {
      "source": "self-supervised pre-training",
      "target": "accuracy",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.85
    },
    {
      "source": "attention distance",
      "target": "attention score",
      "relation": "semantically_related",
      "weight": 0.6866554617881775,
      "inferred": true
    },
    {
      "source": "Devlin et al. (2019)",
      "target": "Devlin et al.",
      "relation": "semantically_related",
      "weight": 0.6804788708686829,
      "inferred": true
    },
    {
      "source": "Devlin et al. (2019)",
      "target": "BERT",
      "relation": "semantically_related",
      "weight": 0.6671301126480103,
      "inferred": true
    },
    {
      "source": "Devlin et al. (2019)",
      "target": "MLM",
      "relation": "uses",
      "weight": 0.6587162613868713,
      "inferred": true
    },
    {
      "source": "Chen et al. (2020b)",
      "target": "Hénaff et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.7987942695617676,
      "inferred": true
    },
    {
      "source": "Carion et al. (2020)",
      "target": "DetectoRS",
      "relation": "uses",
      "weight": 0.6531751751899719,
      "inferred": true
    },
    {
      "source": "NLP",
      "target": "language tasks",
      "relations": [
        "contains"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.9
    },
    {
      "source": "NLP",
      "target": "few-shot learning",
      "relations": [
        "contains"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 1.0,
      "relation": "uses"
    },
    {
      "source": "few-shot learning",
      "target": "meta-learning",
      "relations": [
        "is_a"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3_language_models"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "few-shot learning",
      "target": "Few-shot predictions",
      "relation": "uses",
      "weight": 0.8540292978286743,
      "inferred": true
    },
    {
      "source": "few-shot learning",
      "target": "Few-shot",
      "relation": "semantically_related",
      "weight": 0.7720261216163635,
      "inferred": true
    },
    {
      "source": "pre-training",
      "target": "fine-tuning",
      "relations": [
        "followed_by"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "pre-training",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is_applied_to"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "fine-tuning",
      "target": "Vision Transformer (ViT)",
      "relations": [
        "is_applied_to"
      ],
      "mentions": 1,
      "sources": [
        "reformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "fine-tuning",
      "target": "task-specific fine-tuning",
      "relation": "uses",
      "weight": 0.9220865964889526,
      "inferred": true
    },
    {
      "source": "language tasks",
      "target": "translation",
      "relation": "semantically_related",
      "weight": 0.673836350440979,
      "inferred": true
    },
    {
      "source": "question-answering",
      "target": "Question Answering (QA)",
      "relation": "semantically_related",
      "weight": 0.7994958758354187,
      "inferred": true
    },
    {
      "source": "question-answering",
      "target": "QA",
      "relation": "semantically_related",
      "weight": 0.759384036064148,
      "inferred": true
    },
    {
      "source": "question-answering",
      "target": "Tu et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7452992796897888,
      "inferred": true
    },
    {
      "source": "question-answering",
      "target": "Shao et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.7294355034828186,
      "inferred": true
    },
    {
      "source": "question-answering",
      "target": "Groeneveld et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.701578676700592,
      "inferred": true
    },
    {
      "source": "Common Crawl",
      "target": "GPT-3",
      "relations": [
        "used_for_training"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.95
    },
    {
      "source": "Common Crawl",
      "target": "CommonCrawl",
      "relation": "evaluated_on",
      "weight": 0.7026355862617493,
      "inferred": true
    },
    {
      "source": "SuperGLUE",
      "target": "BIG-bench",
      "relation": "semantically_related",
      "weight": 0.7140769958496094,
      "inferred": true
    },
    {
      "source": "societal impacts",
      "target": "GPT-3",
      "relations": [
        "discussed_in"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.9
    },
    {
      "source": "in-context learning",
      "target": "meta-learning",
      "relations": [
        "is_a"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Natural Questions",
      "target": "OpenBookQA",
      "relation": "evaluated_on",
      "weight": 0.8081251382827759,
      "inferred": true
    },
    {
      "source": "Natural Questions",
      "target": "CoQA",
      "relation": "evaluated_on",
      "weight": 0.7557932734489441,
      "inferred": true
    },
    {
      "source": "Natural Questions",
      "target": "COCO-Val-2014",
      "relation": "evaluated_on",
      "weight": 0.7286874651908875,
      "inferred": true
    },
    {
      "source": "Natural Questions",
      "target": "MRQA",
      "relation": "evaluated_on",
      "weight": 0.7285969853401184,
      "inferred": true
    },
    {
      "source": "Natural Questions",
      "target": "QuAC",
      "relation": "evaluated_on",
      "weight": 0.721881091594696,
      "inferred": true
    },
    {
      "source": "CoQA",
      "target": "QuAC",
      "relation": "evaluated_on",
      "weight": 0.818228006362915,
      "inferred": true
    },
    {
      "source": "CoQA",
      "target": "OpenBookQA",
      "relation": "evaluated_on",
      "weight": 0.7566007971763611,
      "inferred": true
    },
    {
      "source": "CoQA",
      "target": "COCO-Val-2014",
      "relation": "evaluated_on",
      "weight": 0.6808841824531555,
      "inferred": true
    },
    {
      "source": "CoQA",
      "target": "HotpotQA",
      "relation": "evaluated_on",
      "weight": 0.6795018911361694,
      "inferred": true
    },
    {
      "source": "TriviaQA",
      "target": "StackExchange",
      "relation": "evaluated_on",
      "weight": 0.7346193194389343,
      "inferred": true
    },
    {
      "source": "TriviaQA",
      "target": "HotpotQA",
      "relation": "evaluated_on",
      "weight": 0.6500163078308105,
      "inferred": true
    },
    {
      "source": "RWC+19",
      "target": "in-context learning",
      "relations": [
        "discusses"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "YdC+19",
      "target": "NK19",
      "relation": "semantically_related",
      "weight": 0.6527082324028015,
      "inferred": true
    },
    {
      "source": "MPL19",
      "target": "NK19",
      "relation": "semantically_related",
      "weight": 0.7328703999519348,
      "inferred": true
    },
    {
      "source": "MPL19",
      "target": "GSL+18",
      "relation": "semantically_related",
      "weight": 0.7104612588882446,
      "inferred": true
    },
    {
      "source": "MPL19",
      "target": "mean±std",
      "relation": "semantically_related",
      "weight": 0.6754704713821411,
      "inferred": true
    },
    {
      "source": "GSL+18",
      "target": "MoS",
      "relation": "uses",
      "weight": 0.6861267685890198,
      "inferred": true
    },
    {
      "source": "GSL+18",
      "target": "Iyer et al. (2022)",
      "relation": "semantically_related",
      "weight": 0.669113039970398,
      "inferred": true
    },
    {
      "source": "NK19",
      "target": "evaluation metric",
      "relation": "semantically_related",
      "weight": 0.7313917279243469,
      "inferred": true
    },
    {
      "source": "RNSS18",
      "target": "SPP+19",
      "relation": "semantically_related",
      "weight": 0.857875645160675,
      "inferred": true
    },
    {
      "source": "RNSS18",
      "target": "Tur20",
      "relation": "semantically_related",
      "weight": 0.8151362538337708,
      "inferred": true
    },
    {
      "source": "RNSS18",
      "target": "DCLT18",
      "relation": "semantically_related",
      "weight": 0.7814105153083801,
      "inferred": true
    },
    {
      "source": "RNSS18",
      "target": "Ye et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7713160514831543,
      "inferred": true
    },
    {
      "source": "DCLT18",
      "target": "SPP+19",
      "relation": "semantically_related",
      "weight": 0.8181347250938416,
      "inferred": true
    },
    {
      "source": "DCLT18",
      "target": "Tur20",
      "relation": "semantically_related",
      "weight": 0.7817934155464172,
      "inferred": true
    },
    {
      "source": "DCLT18",
      "target": "Rae et al.",
      "relation": "semantically_related",
      "weight": 0.6878014802932739,
      "inferred": true
    },
    {
      "source": "SPP+19",
      "target": "Tur20",
      "relation": "semantically_related",
      "weight": 0.8394327163696289,
      "inferred": true
    },
    {
      "source": "SPP+19",
      "target": "Ye et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.8247072100639343,
      "inferred": true
    },
    {
      "source": "RSR+19",
      "target": "Hoffmann et al. (2022)",
      "relation": "semantically_related",
      "weight": 0.8518679141998291,
      "inferred": true
    },
    {
      "source": "RSR+19",
      "target": "Scaling Laws for Neural Language Models",
      "relation": "semantically_related",
      "weight": 0.7085639238357544,
      "inferred": true
    },
    {
      "source": "Tur20",
      "target": "Ye et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.8072529435157776,
      "inferred": true
    },
    {
      "source": "ANLI",
      "target": "text8",
      "relation": "evaluated_on",
      "weight": 0.8172937631607056,
      "inferred": true
    },
    {
      "source": "ANLI",
      "target": "enwik8",
      "relation": "evaluated_on",
      "weight": 0.760891854763031,
      "inferred": true
    },
    {
      "source": "ANLI",
      "target": "Books1",
      "relation": "evaluated_on",
      "weight": 0.6937363147735596,
      "inferred": true
    },
    {
      "source": "ANLI",
      "target": "RACE",
      "relation": "evaluated_on",
      "weight": 0.6859527826309204,
      "inferred": true
    },
    {
      "source": "ANLI",
      "target": "COCO-Val-2014",
      "relation": "evaluated_on",
      "weight": 0.6799296140670776,
      "inferred": true
    },
    {
      "source": "QuAC",
      "target": "MRQA",
      "relation": "evaluated_on",
      "weight": 0.7304362654685974,
      "inferred": true
    },
    {
      "source": "QuAC",
      "target": "OpenBookQA",
      "relation": "evaluated_on",
      "weight": 0.7248139977455139,
      "inferred": true
    },
    {
      "source": "QuAC",
      "target": "HotpotQA",
      "relation": "evaluated_on",
      "weight": 0.6882629990577698,
      "inferred": true
    },
    {
      "source": "CommonCrawl",
      "target": "Books1",
      "relation": "evaluated_on",
      "weight": 0.8136371374130249,
      "inferred": true
    },
    {
      "source": "CommonCrawl",
      "target": "One Billion Word",
      "relation": "evaluated_on",
      "weight": 0.7680723667144775,
      "inferred": true
    },
    {
      "source": "CommonCrawl",
      "target": "WikiText-103",
      "relation": "evaluated_on",
      "weight": 0.7175000905990601,
      "inferred": true
    },
    {
      "source": "Books1",
      "target": "Common Crawl",
      "relations": [
        "augments"
      ],
      "mentions": 2,
      "sources": [
        "gpt3_language_models",
        "gpt3_language_models"
      ],
      "confidence": 0.85,
      "relation": "evaluated_on"
    },
    {
      "source": "Books1",
      "target": "Books3",
      "relation": "evaluated_on",
      "weight": 0.8656834363937378,
      "inferred": true
    },
    {
      "source": "Books1",
      "target": "Wikipedia",
      "relation": "evaluated_on",
      "weight": 0.8218334913253784,
      "inferred": true
    },
    {
      "source": "Books1",
      "target": "One Billion Word",
      "relation": "evaluated_on",
      "weight": 0.7687016725540161,
      "inferred": true
    },
    {
      "source": "Wikipedia",
      "target": "Common Crawl",
      "relations": [
        "augments"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.85,
      "relation": "evaluated_on"
    },
    {
      "source": "Wikipedia",
      "target": "Books3",
      "relation": "evaluated_on",
      "weight": 0.8951708674430847,
      "inferred": true
    },
    {
      "source": "Wikipedia",
      "target": "Dataset text8",
      "relation": "evaluated_on",
      "weight": 0.8620537519454956,
      "inferred": true
    },
    {
      "source": "Wikipedia",
      "target": "enwiki8",
      "relation": "evaluated_on",
      "weight": 0.8325626254081726,
      "inferred": true
    },
    {
      "source": "Wikipedia",
      "target": "One Billion Word",
      "relation": "evaluated_on",
      "weight": 0.80424964427948,
      "inferred": true
    },
    {
      "source": "Sparse Transformer",
      "target": "Transformer architecture",
      "relations": [
        "extends"
      ],
      "mentions": 1,
      "sources": [
        "gpt3_language_models"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Validation loss",
      "target": "GPT-3",
      "relations": [
        "is_measured_for"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.9
    },
    {
      "source": "Scaling Laws for Neural Language Models",
      "target": "Hoffmann et al. (2022)",
      "relation": "semantically_related",
      "weight": 0.8218292593955994,
      "inferred": true
    },
    {
      "source": "Winograd",
      "target": "CommonSense Reasoning Tasks",
      "relation": "semantically_related",
      "weight": 0.7587000131607056,
      "inferred": true
    },
    {
      "source": "Winograd",
      "target": "WinoGrande",
      "relation": "evaluated_on",
      "weight": 0.7144811749458313,
      "inferred": true
    },
    {
      "source": "Winograd",
      "target": "ARChallenge",
      "relation": "evaluated_on",
      "weight": 0.6798375248908997,
      "inferred": true
    },
    {
      "source": "Winograd",
      "target": "ARC",
      "relation": "evaluated_on",
      "weight": 0.6611892580986023,
      "inferred": true
    },
    {
      "source": "ARC",
      "target": "ARCeasy",
      "relation": "evaluated_on",
      "weight": 0.7870421409606934,
      "inferred": true
    },
    {
      "source": "ARC",
      "target": "ARChallenge",
      "relation": "evaluated_on",
      "weight": 0.7363322973251343,
      "inferred": true
    },
    {
      "source": "ARC",
      "target": "WinoGrande",
      "relation": "evaluated_on",
      "weight": 0.7040299773216248,
      "inferred": true
    },
    {
      "source": "ARC",
      "target": "BoolQ",
      "relation": "evaluated_on",
      "weight": 0.6850444078445435,
      "inferred": true
    },
    {
      "source": "OpenBookQA",
      "target": "MRQA",
      "relation": "evaluated_on",
      "weight": 0.7574999332427979,
      "inferred": true
    },
    {
      "source": "OpenBookQA",
      "target": "Exact Match",
      "relation": "semantically_related",
      "weight": 0.7267770171165466,
      "inferred": true
    },
    {
      "source": "F1 similarity score",
      "target": "F1 Score",
      "relation": "semantically_related",
      "weight": 0.7503622174263,
      "inferred": true
    },
    {
      "source": "F1 similarity score",
      "target": "evaluation metric",
      "relation": "semantically_related",
      "weight": 0.6882027387619019,
      "inferred": true
    },
    {
      "source": "F1 similarity score",
      "target": "mean±std",
      "relation": "semantically_related",
      "weight": 0.6794533133506775,
      "inferred": true
    },
    {
      "source": "K",
      "target": "few-shot learning",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "gpt3"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "K",
      "target": "Few-shot predictions",
      "relation": "uses",
      "weight": 0.6645063757896423,
      "inferred": true
    },
    {
      "source": "K",
      "target": "Few-shot",
      "relation": "semantically_related",
      "weight": 0.6535320281982422,
      "inferred": true
    },
    {
      "source": "accuracy",
      "target": "Top-1 Accuracy",
      "relation": "semantically_related",
      "weight": 0.7316063046455383,
      "inferred": true
    },
    {
      "source": "LLaMA-I",
      "target": "GPT-3",
      "relations": [
        "outperforms",
        "compares_with"
      ],
      "mentions": 2,
      "sources": [
        "llama",
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "Chinchilla",
      "relations": [
        "is_competitive_with",
        "compares_with"
      ],
      "mentions": 2,
      "sources": [
        "llama",
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "PaLM",
      "relations": [
        "is_competitive_with",
        "compares_with"
      ],
      "mentions": 2,
      "sources": [
        "llama",
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "CommonCrawl",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "C4",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "Github",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "Wikipedia",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "Books",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "ArXiv",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "StackExchange",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "Transformer Architecture",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "LLaMA-I",
      "target": "Performance Metrics",
      "relations": [
        "evaluated_by"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "Gopher",
      "relations": [
        "compares_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "OPT",
      "relations": [
        "compares_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "OPT-IML",
      "relations": [
        "compares_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.8
    },
    {
      "source": "LLaMA-I",
      "target": "LaMDA",
      "relations": [
        "compares_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.8
    },
    {
      "source": "LLaMA-I",
      "target": "Natural Questions",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "TriviaQA",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "MATH",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "GSM8k",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "RACE",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "BoolQ",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "Flan-PaLM",
      "relations": [
        "evaluates_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "LLaMA-I",
      "target": "Exact Match",
      "relations": [
        "achieves_performance"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "pass@",
      "relations": [
        "achieves_performance"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-I",
      "target": "LLaMA-65B",
      "relation": "semantically_related",
      "weight": 0.8770766854286194,
      "inferred": true
    },
    {
      "source": "Chinchilla",
      "target": "LLaMA-I",
      "relations": [
        "is_compared_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "Chinchilla",
      "target": "PaLM",
      "relation": "semantically_related",
      "weight": 0.6720594763755798,
      "inferred": true
    },
    {
      "source": "PaLM",
      "target": "LLaMA-I",
      "relations": [
        "is_compared_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "PaLM",
      "target": "Transformer",
      "relations": [
        "is_based_on",
        "based_on"
      ],
      "mentions": 2,
      "sources": [
        "palm",
        "palm"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "PaLM",
      "target": "Pathways",
      "relations": [
        "uses",
        "is based_on"
      ],
      "mentions": 2,
      "sources": [
        "palm",
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "PaLM",
      "target": "TPUv4",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "PaLM",
      "target": "GPT-3",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.8
    },
    {
      "source": "PaLM",
      "target": "BIG-bench",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "PaLM",
      "target": "SwiGLU",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "PaLM",
      "target": "RoPE",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "PaLM",
      "target": "SentencePiece",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "PaLM",
      "target": "WinoGender",
      "relations": [
        "evaluates"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "PaLM",
      "target": "540B parameters",
      "relations": [
        "has_model_scale"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0
    },
    {
      "source": "PaLM",
      "target": "62B parameters",
      "relations": [
        "has_model_scale"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0
    },
    {
      "source": "PaLM",
      "target": "8B parameters",
      "relations": [
        "has_model_scale"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0
    },
    {
      "source": "PaLM",
      "target": "780 billion tokens",
      "relations": [
        "trained_on"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0,
      "relation": "trained_on"
    },
    {
      "source": "PaLM",
      "target": "JAX",
      "relations": [
        "trained_using"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0
    },
    {
      "source": "PaLM",
      "target": "T5X",
      "relations": [
        "trained_using"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0
    },
    {
      "source": "PaLM",
      "target": "Flan-PaLM",
      "relation": "trained_on",
      "weight": 0.6552667617797852,
      "inferred": true
    },
    {
      "source": "Hoffmann et al. (2022)",
      "target": "Devlin et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.726465106010437,
      "inferred": true
    },
    {
      "source": "Github",
      "target": "GitHub",
      "relation": "semantically_related",
      "weight": 0.7770739197731018,
      "inferred": true
    },
    {
      "source": "Books",
      "target": "Gutenberg",
      "relation": "evaluated_on",
      "weight": 0.7384481430053711,
      "inferred": true
    },
    {
      "source": "StackExchange",
      "target": "SQuAD",
      "relation": "evaluated_on",
      "weight": 0.669312596321106,
      "inferred": true
    },
    {
      "source": "Transformer Architecture",
      "target": "Vaswani et al. (2017)",
      "relations": [
        "is_based_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "Transformer Architecture",
      "target": "Transformers",
      "relation": "uses",
      "weight": 0.6604865789413452,
      "inferred": true
    },
    {
      "source": "Performance Metrics",
      "target": "BIG-bench",
      "relation": "semantically_related",
      "weight": 0.8256290555000305,
      "inferred": true
    },
    {
      "source": "Performance Metrics",
      "target": "enwik8",
      "relation": "evaluated_on",
      "weight": 0.7481029629707336,
      "inferred": true
    },
    {
      "source": "Performance Metrics",
      "target": "Perplexity",
      "relation": "semantically_related",
      "weight": 0.7433815002441406,
      "inferred": true
    },
    {
      "source": "Performance Metrics",
      "target": "mean±std",
      "relation": "semantically_related",
      "weight": 0.6887049674987793,
      "inferred": true
    },
    {
      "source": "Performance Metrics",
      "target": "evaluation metric",
      "relation": "semantically_related",
      "weight": 0.6813730597496033,
      "inferred": true
    },
    {
      "source": "SwiGLU",
      "target": "Shazeer, 2020",
      "relation": "semantically_related",
      "weight": 0.7188297510147095,
      "inferred": true
    },
    {
      "source": "Gopher",
      "target": "LLaMA-I",
      "relations": [
        "is_compared_with"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "Gopher",
      "target": "GPT-3",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.85
    },
    {
      "source": "OPT",
      "target": "LLaMA-65B",
      "relation": "semantically_related",
      "weight": 0.7280755043029785,
      "inferred": true
    },
    {
      "source": "OPT",
      "target": "LaMDA",
      "relation": "semantically_related",
      "weight": 0.6754375100135803,
      "inferred": true
    },
    {
      "source": "HumanEval",
      "target": "MBPP",
      "relation": "evaluated_on",
      "weight": 0.8842802047729492,
      "inferred": true
    },
    {
      "source": "HumanEval",
      "target": "pass@",
      "relation": "semantically_related",
      "weight": 0.7166261672973633,
      "inferred": true
    },
    {
      "source": "MBPP",
      "target": "pass@",
      "relation": "semantically_related",
      "weight": 0.7092027068138123,
      "inferred": true
    },
    {
      "source": "MATH",
      "target": "GSM8k",
      "relation": "evaluated_on",
      "weight": 0.7331836223602295,
      "inferred": true
    },
    {
      "source": "BoolQ",
      "target": "WinoGrande",
      "relation": "evaluated_on",
      "weight": 0.8354949355125427,
      "inferred": true
    },
    {
      "source": "BoolQ",
      "target": "ARChallenge",
      "relation": "evaluated_on",
      "weight": 0.8269283175468445,
      "inferred": true
    },
    {
      "source": "BoolQ",
      "target": "ARCeasy",
      "relation": "evaluated_on",
      "weight": 0.823021411895752,
      "inferred": true
    },
    {
      "source": "WinoGrande",
      "target": "ARChallenge",
      "relation": "evaluated_on",
      "weight": 0.8904396891593933,
      "inferred": true
    },
    {
      "source": "WinoGrande",
      "target": "ARCeasy",
      "relation": "evaluated_on",
      "weight": 0.8843144774436951,
      "inferred": true
    },
    {
      "source": "ARCeasy",
      "target": "ARChallenge",
      "relation": "evaluated_on",
      "weight": 0.9092087745666504,
      "inferred": true
    },
    {
      "source": "Zero-shot",
      "target": "Few-shot",
      "relation": "semantically_related",
      "weight": 0.8351297378540039,
      "inferred": true
    },
    {
      "source": "Few-shot",
      "target": "Few-shot predictions",
      "relation": "uses",
      "weight": 0.7719706892967224,
      "inferred": true
    },
    {
      "source": "MMLU",
      "target": "BIG-bench",
      "relation": "semantically_related",
      "weight": 0.6956764459609985,
      "inferred": true
    },
    {
      "source": "LaMDA",
      "target": "LLaMA-65B",
      "relation": "semantically_related",
      "weight": 0.6772091388702393,
      "inferred": true
    },
    {
      "source": "LLaMA-65B",
      "target": "Chinchilla",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-65B",
      "target": "PaLM",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaMA-65B",
      "target": "RealToxicityPrompts",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.7
    },
    {
      "source": "LLaMA-65B",
      "target": "Megatron-Turing NLG",
      "relation": "semantically_related",
      "weight": 0.6736020445823669,
      "inferred": true
    },
    {
      "source": "Books3",
      "target": "Dataset text8",
      "relation": "evaluated_on",
      "weight": 0.8661251068115234,
      "inferred": true
    },
    {
      "source": "Books3",
      "target": "One Billion Word",
      "relation": "evaluated_on",
      "weight": 0.8011718988418579,
      "inferred": true
    },
    {
      "source": "Books3",
      "target": "Dataset enwik8",
      "relation": "evaluated_on",
      "weight": 0.7931773066520691,
      "inferred": true
    },
    {
      "source": "pass@100",
      "target": "pass@80",
      "relation": "semantically_related",
      "weight": 0.9657125473022461,
      "inferred": true
    },
    {
      "source": "Chowdhery et al. (2022)",
      "target": "Mitchell et al., 2019",
      "relation": "semantically_related",
      "weight": 0.7755725383758545,
      "inferred": true
    },
    {
      "source": "Zhang et al. (2022)",
      "target": "Hoffmann et al. (2022)",
      "relations": [
        "cites"
      ],
      "mentions": 1,
      "sources": [
        "llama"
      ],
      "confidence": 0.8
    },
    {
      "source": "TruthfulQA",
      "target": "BIG-bench",
      "relation": "semantically_related",
      "weight": 0.7080774307250977,
      "inferred": true
    },
    {
      "source": "Perplexity",
      "target": "Transformer-XL",
      "relations": [
        "is reduced by"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Co-reference resolution",
      "target": "coreference resolution",
      "relation": "semantically_related",
      "weight": 0.7249179482460022,
      "inferred": true
    },
    {
      "source": "Longformer",
      "target": "self-attention",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "RoBERTa",
      "relations": [
        "outperforms",
        "improves",
        "is_based_on"
      ],
      "mentions": 3,
      "sources": [
        "longformer",
        "longformer",
        "longformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "text8",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Longformer",
      "target": "enwik8",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Longformer",
      "target": "WikiHop",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "Longformer",
      "target": "TriviaQA",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "trained_on"
    },
    {
      "source": "Longformer",
      "target": "LED",
      "relations": [
        "used_in"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.8
    },
    {
      "source": "Longformer",
      "target": "Transformer architecture",
      "relations": [
        "based_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "Sparse Transformer",
      "relations": [
        "compared_to"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "Transformer",
      "relations": [
        "is_based_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "Attention Pattern",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "Position Embeddings",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "MLM",
      "relations": [
        "is_pretrained_with"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Longformer",
      "target": "BERT",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85
    },
    {
      "source": "Longformer",
      "target": "Longformer-large",
      "relation": "semantically_related",
      "weight": 0.7659521698951721,
      "inferred": true
    },
    {
      "source": "Longformer",
      "target": "Longformer-Encoder-Decoder (LED)",
      "relation": "uses",
      "weight": 0.728726863861084,
      "inferred": true
    },
    {
      "source": "text8",
      "target": "Dataset text8",
      "relation": "evaluated_on",
      "weight": 0.8751117587089539,
      "inferred": true
    },
    {
      "source": "text8",
      "target": "enwiki8",
      "relation": "evaluated_on",
      "weight": 0.8673997521400452,
      "inferred": true
    },
    {
      "source": "text8",
      "target": "Dataset enwik8",
      "relation": "evaluated_on",
      "weight": 0.8575928211212158,
      "inferred": true
    },
    {
      "source": "text8",
      "target": "One Billion Word",
      "relation": "evaluated_on",
      "weight": 0.7906455993652344,
      "inferred": true
    },
    {
      "source": "enwik8",
      "target": "text8",
      "relations": [
        "is compared with"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.7,
      "relation": "evaluated_on"
    },
    {
      "source": "enwik8",
      "target": "Dataset enwik8",
      "relation": "evaluated_on",
      "weight": 0.9044584631919861,
      "inferred": true
    },
    {
      "source": "enwik8",
      "target": "enwiki8",
      "relation": "evaluated_on",
      "weight": 0.8940914869308472,
      "inferred": true
    },
    {
      "source": "enwik8",
      "target": "BIG-bench",
      "relation": "semantically_related",
      "weight": 0.7881434559822083,
      "inferred": true
    },
    {
      "source": "Longformer-Encoder-Decoder (LED)",
      "target": "LED",
      "relation": "semantically_related",
      "weight": 0.8832454681396484,
      "inferred": true
    },
    {
      "source": "Longformer-Encoder-Decoder (LED)",
      "target": "BART",
      "relation": "semantically_related",
      "weight": 0.6638723611831665,
      "inferred": true
    },
    {
      "source": "Longformer-Encoder-Decoder (LED)",
      "target": "Longformer-large",
      "relation": "semantically_related",
      "weight": 0.6520397663116455,
      "inferred": true
    },
    {
      "source": "RoBERTa",
      "target": "Longformer-large",
      "relation": "semantically_related",
      "weight": 0.7951823472976685,
      "inferred": true
    },
    {
      "source": "RoBERTa",
      "target": "RoBERTa-large",
      "relation": "semantically_related",
      "weight": 0.668950080871582,
      "inferred": true
    },
    {
      "source": "RoBERTa",
      "target": "Transformers",
      "relation": "uses",
      "weight": 0.6553435325622559,
      "inferred": true
    },
    {
      "source": "Transformer-XL",
      "target": "Longformer",
      "relations": [
        "compared_to"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.75,
      "relation": "uses"
    },
    {
      "source": "Transformer-XL",
      "target": "long-term dependency",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9
    },
    {
      "source": "Transformer-XL",
      "target": "context fragmentation",
      "relations": [
        "resolves"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85
    },
    {
      "source": "Transformer-XL",
      "target": "Positional Encoding",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "Transformer-XL",
      "target": "Perplexity",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9
    },
    {
      "source": "Transformer-XL",
      "target": "AWD-LSTM",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "Transformer-XL",
      "target": "One Billion Word",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "Transformer-XL",
      "target": "WikiText-103",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.8,
      "relation": "trained_on"
    },
    {
      "source": "Transformer-XL",
      "target": "Compressive Transformer",
      "relation": "semantically_related",
      "weight": 0.6950331330299377,
      "inferred": true
    },
    {
      "source": "Transformer-XL",
      "target": "Longformer-large",
      "relation": "semantically_related",
      "weight": 0.6517878770828247,
      "inferred": true
    },
    {
      "source": "Dai et al. (2019)",
      "target": "Radford et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.9828694462776184,
      "inferred": true
    },
    {
      "source": "Dai et al. (2019)",
      "target": "Bengio et al. (2003)",
      "relation": "semantically_related",
      "weight": 0.7516748905181885,
      "inferred": true
    },
    {
      "source": "Dai et al. (2019)",
      "target": "Merity et al. (2016)",
      "relation": "semantically_related",
      "weight": 0.6769588589668274,
      "inferred": true
    },
    {
      "source": "Dai et al. (2019)",
      "target": "Mikolov et al. (2010)",
      "relation": "semantically_related",
      "weight": 0.6701159477233887,
      "inferred": true
    },
    {
      "source": "Radford et al. (2019)",
      "target": "Bengio et al. (2003)",
      "relation": "semantically_related",
      "weight": 0.751702606678009,
      "inferred": true
    },
    {
      "source": "Radford et al. (2019)",
      "target": "Mikolov et al. (2010)",
      "relation": "semantically_related",
      "weight": 0.6637857556343079,
      "inferred": true
    },
    {
      "source": "BlockSparse",
      "target": "TensorFlow",
      "relations": [
        "is_designed_for"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.8
    },
    {
      "source": "TensorFlow",
      "target": "PyTorch",
      "relation": "semantically_related",
      "weight": 0.75377357006073,
      "inferred": true
    },
    {
      "source": "QA",
      "target": "Question Answering (QA)",
      "relation": "semantically_related",
      "weight": 0.8793184757232666,
      "inferred": true
    },
    {
      "source": "QA",
      "target": "Chen et al. (2017)",
      "relation": "semantically_related",
      "weight": 0.71957927942276,
      "inferred": true
    },
    {
      "source": "coreference resolution",
      "target": "Joshi et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7692285776138306,
      "inferred": true
    },
    {
      "source": "LED",
      "target": "Longformer",
      "relations": [
        "is_a_variant_of"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "LED",
      "target": "BART",
      "relation": "semantically_related",
      "weight": 0.7637436985969543,
      "inferred": true
    },
    {
      "source": "BP-Transformer",
      "target": "Transformers",
      "relation": "uses",
      "weight": 0.6697120070457458,
      "inferred": true
    },
    {
      "source": "BP-Transformer",
      "target": "T5",
      "relation": "semantically_related",
      "weight": 0.6599416136741638,
      "inferred": true
    },
    {
      "source": "CPC loss",
      "target": "pre-training",
      "relations": [
        "is_used_for"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "BigBird",
      "target": "ETC",
      "relations": [
        "is_an_extension_of"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "ETC",
      "target": "Attention Pattern",
      "relation": "uses",
      "weight": 0.7388305068016052,
      "inferred": true
    },
    {
      "source": "SQuAD",
      "target": "MRQA",
      "relation": "evaluated_on",
      "weight": 0.7207693457603455,
      "inferred": true
    },
    {
      "source": "MRQA",
      "target": "HotpotQA",
      "relation": "evaluated_on",
      "weight": 0.6984127163887024,
      "inferred": true
    },
    {
      "source": "dilated CNNs",
      "target": "vandenOord et al. (2016)",
      "relation": "semantically_related",
      "weight": 0.719655454158783,
      "inferred": true
    },
    {
      "source": "Sutskever et al. (2014)",
      "target": "fairseq",
      "relation": "semantically_related",
      "weight": 0.6901391744613647,
      "inferred": true
    },
    {
      "source": "Chen et al. (2017)",
      "target": "Question Answering (QA)",
      "relation": "semantically_related",
      "weight": 0.7219241261482239,
      "inferred": true
    },
    {
      "source": "Chen et al. (2017)",
      "target": "Groeneveld et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.707344114780426,
      "inferred": true
    },
    {
      "source": "Ye et al. (2019)",
      "target": "BP-Transformer",
      "relations": [
        "discusses"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Ye et al. (2019)",
      "target": "Rae et al.",
      "relation": "semantically_related",
      "weight": 0.8077725768089294,
      "inferred": true
    },
    {
      "source": "BERT",
      "target": "T5",
      "relations": [
        "is_related_to"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.7
    },
    {
      "source": "BERT",
      "target": "RoBERTa-large",
      "relation": "semantically_related",
      "weight": 0.6843487024307251,
      "inferred": true
    },
    {
      "source": "Masked Language Modeling (MLM)",
      "target": "MLM",
      "relation": "uses",
      "weight": 0.8628824353218079,
      "inferred": true
    },
    {
      "source": "Question Answering (QA)",
      "target": "Tu et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7307670712471008,
      "inferred": true
    },
    {
      "source": "Question Answering (QA)",
      "target": "Shao et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.7219417095184326,
      "inferred": true
    },
    {
      "source": "Adaptive Span",
      "target": "Longformer",
      "relations": [
        "compared_to"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.7,
      "relation": "uses"
    },
    {
      "source": "Adaptive Span",
      "target": "Sukhbaatar et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7699987888336182,
      "inferred": true
    },
    {
      "source": "Compressive Transformer",
      "target": "Longformer",
      "relations": [
        "compared_to"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.7,
      "relation": "uses"
    },
    {
      "source": "Dataset text8",
      "target": "Dataset enwik8",
      "relation": "evaluated_on",
      "weight": 0.8536936044692993,
      "inferred": true
    },
    {
      "source": "Dataset text8",
      "target": "enwiki8",
      "relation": "evaluated_on",
      "weight": 0.8275092244148254,
      "inferred": true
    },
    {
      "source": "Dataset enwik8",
      "target": "enwiki8",
      "relation": "evaluated_on",
      "weight": 0.9664169549942017,
      "inferred": true
    },
    {
      "source": "MLM",
      "target": "Language Models (LMs)",
      "relation": "semantically_related",
      "weight": 0.6593711376190186,
      "inferred": true
    },
    {
      "source": "fairseq",
      "target": "BART",
      "relation": "semantically_related",
      "weight": 0.699725866317749,
      "inferred": true
    },
    {
      "source": "Ott et al.",
      "target": "fairseq",
      "relations": [
        "developed"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Devlin et al.",
      "target": "BERT",
      "relations": [
        "developed"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Devlin et al.",
      "target": "Clark et al.",
      "relation": "semantically_related",
      "weight": 0.7743104100227356,
      "inferred": true
    },
    {
      "source": "Devlin et al.",
      "target": "Liu et al.",
      "relation": "semantically_related",
      "weight": 0.678009033203125,
      "inferred": true
    },
    {
      "source": "Liu et al.",
      "target": "RoBERTa",
      "relations": [
        "developed"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Rae et al.",
      "target": "Compressive Transformer",
      "relations": [
        "discussed"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.8
    },
    {
      "source": "RoBERTa-large",
      "target": "Transformers",
      "relation": "uses",
      "weight": 0.7497743964195251,
      "inferred": true
    },
    {
      "source": "Longformer-large",
      "target": "RoBERTa-large",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Longformer-large",
      "target": "WikiHop",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.95,
      "relation": "trained_on"
    },
    {
      "source": "Longformer-large",
      "target": "TriviaQA",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.95,
      "relation": "trained_on"
    },
    {
      "source": "Longformer-large",
      "target": "HotpotQA",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.95,
      "relation": "trained_on"
    },
    {
      "source": "Longformer-large",
      "target": "coreference resolution",
      "relations": [
        "used_for"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.85,
      "relation": "applied_to"
    },
    {
      "source": "OntoNotes",
      "target": "coreference resolution",
      "relations": [
        "used_for"
      ],
      "mentions": 1,
      "sources": [
        "longformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "OntoNotes",
      "target": "Joshi et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7024286389350891,
      "inferred": true
    },
    {
      "source": "BPC",
      "target": "bits per character (bpc)",
      "relation": "semantically_related",
      "weight": 0.8990827798843384,
      "inferred": true
    },
    {
      "source": "Tu et al. (2019)",
      "target": "Shao et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.9749228358268738,
      "inferred": true
    },
    {
      "source": "Tu et al. (2019)",
      "target": "Groeneveld et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.9466460943222046,
      "inferred": true
    },
    {
      "source": "Tu et al. (2019)",
      "target": "Glaß et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7769291400909424,
      "inferred": true
    },
    {
      "source": "Shao et al. (2020)",
      "target": "Groeneveld et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.9351597428321838,
      "inferred": true
    },
    {
      "source": "Shao et al. (2020)",
      "target": "Glaß et al. (2019)",
      "relation": "semantically_related",
      "weight": 0.7602849006652832,
      "inferred": true
    },
    {
      "source": "Glaß et al. (2019)",
      "target": "Groeneveld et al. (2020)",
      "relation": "semantically_related",
      "weight": 0.7795904278755188,
      "inferred": true
    },
    {
      "source": "Pathways",
      "target": "Pathways system",
      "relation": "semantically_related",
      "weight": 0.8755174279212952,
      "inferred": true
    },
    {
      "source": "Pathways",
      "target": "TPU v4 Pods",
      "relation": "semantically_related",
      "weight": 0.6950275301933289,
      "inferred": true
    },
    {
      "source": "TPUv4",
      "target": "TPU v4",
      "relation": "semantically_related",
      "weight": 0.8044096827507019,
      "inferred": true
    },
    {
      "source": "Sharan Narang",
      "target": "Yixuan Wei",
      "relation": "authored_by",
      "weight": 0.6563596129417419,
      "inferred": true
    },
    {
      "source": "Jacob Devlin",
      "target": "Stephen Lin",
      "relation": "authored_by",
      "weight": 0.7353368997573853,
      "inferred": true
    },
    {
      "source": "Jacob Devlin",
      "target": "Yutong Lin",
      "relation": "authored_by",
      "weight": 0.6730045080184937,
      "inferred": true
    },
    {
      "source": "GLaM",
      "target": "X-GPT",
      "relation": "semantically_related",
      "weight": 0.7037206888198853,
      "inferred": true
    },
    {
      "source": "Megatron–Turing NLG",
      "target": "Megatron-Turing NLG",
      "relation": "semantically_related",
      "weight": 0.8856226205825806,
      "inferred": true
    },
    {
      "source": "Megatron–Turing NLG",
      "target": "Smith et al. (2022)",
      "relation": "authored_by",
      "weight": 0.7996851205825806,
      "inferred": true
    },
    {
      "source": "Transformer architecture",
      "target": "GPT-3",
      "relations": [
        "is used in"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.95
    },
    {
      "source": "TPU v4 Pods",
      "target": "Jouppi et al., 2020",
      "relation": "semantically_related",
      "weight": 0.8204099535942078,
      "inferred": true
    },
    {
      "source": "TPU v4 Pods",
      "target": "TPU v4",
      "relation": "semantically_related",
      "weight": 0.690976083278656,
      "inferred": true
    },
    {
      "source": "FLOPs utilization",
      "target": "Narayanan et al., 2021b",
      "relation": "semantically_related",
      "weight": 0.7484735250473022,
      "inferred": true
    },
    {
      "source": "FLOPs utilization",
      "target": "FLOPs",
      "relation": "semantically_related",
      "weight": 0.7386839985847473,
      "inferred": true
    },
    {
      "source": "FLOPs utilization",
      "target": "Model FLOPs utilization (MFU)",
      "relation": "semantically_related",
      "weight": 0.7349250316619873,
      "inferred": true
    },
    {
      "source": "FLOPs utilization",
      "target": "Hardware FLOPs utilization (HFU)",
      "relation": "semantically_related",
      "weight": 0.675588846206665,
      "inferred": true
    },
    {
      "source": "Du et al. (2021)",
      "target": "Du et al., 2021",
      "relation": "semantically_related",
      "weight": 0.7344109416007996,
      "inferred": true
    },
    {
      "source": "Smith et al. (2022)",
      "target": "Megatron-Turing NLG",
      "relation": "semantically_related",
      "weight": 0.7736201882362366,
      "inferred": true
    },
    {
      "source": "Thoppilan et al. (2022)",
      "target": "Thoppilan et al., 2022",
      "relation": "semantically_related",
      "weight": 0.7191895842552185,
      "inferred": true
    },
    {
      "source": "540B parameters",
      "target": "62B parameters",
      "relation": "semantically_related",
      "weight": 0.867846667766571,
      "inferred": true
    },
    {
      "source": "540B parameters",
      "target": "8B parameters",
      "relation": "semantically_related",
      "weight": 0.8304417729377747,
      "inferred": true
    },
    {
      "source": "62B parameters",
      "target": "8B parameters",
      "relation": "semantically_related",
      "weight": 0.8833149671554565,
      "inferred": true
    },
    {
      "source": "JAX",
      "target": "Bradbury et al., 2018",
      "relation": "semantically_related",
      "weight": 0.8085980415344238,
      "inferred": true
    },
    {
      "source": "JAX",
      "target": "JAX/XLA",
      "relation": "semantically_related",
      "weight": 0.680377185344696,
      "inferred": true
    },
    {
      "source": "T5X",
      "target": "Roberts et al., 2022",
      "relation": "semantically_related",
      "weight": 0.7079148888587952,
      "inferred": true
    },
    {
      "source": "780 billion tokens",
      "target": "GitHub",
      "relations": [
        "composed_of"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0
    },
    {
      "source": "780 billion tokens",
      "target": "Wikipedia",
      "relations": [
        "composed_of"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 1.0,
      "relation": "evaluated_on"
    },
    {
      "source": "Filtered webpages",
      "target": "News articles",
      "relation": "semantically_related",
      "weight": 0.729681134223938,
      "inferred": true
    },
    {
      "source": "Filtered webpages",
      "target": "GitHub",
      "relation": "semantically_related",
      "weight": 0.6924213171005249,
      "inferred": true
    },
    {
      "source": "GitHub",
      "target": "News articles",
      "relation": "semantically_related",
      "weight": 0.672943651676178,
      "inferred": true
    },
    {
      "source": "Social media conversations",
      "target": "News articles",
      "relation": "semantically_related",
      "weight": 0.7250563502311707,
      "inferred": true
    },
    {
      "source": "Social media conversations",
      "target": "training data",
      "relation": "uses",
      "weight": 0.6715022325515747,
      "inferred": true
    },
    {
      "source": "Xu et al., 2021",
      "target": "Huang et al., 2019",
      "relation": "semantically_related",
      "weight": 0.6627378463745117,
      "inferred": true
    },
    {
      "source": "Lopes et al., 2017",
      "target": "Allamanis, 2019",
      "relation": "semantically_related",
      "weight": 0.9712246656417847,
      "inferred": true
    },
    {
      "source": "Pathways system",
      "target": "TPU v4",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "Pathways system",
      "target": "JAX/XLA",
      "relations": [
        "executes"
      ],
      "mentions": 1,
      "sources": [
        "palm"
      ],
      "confidence": 0.9
    },
    {
      "source": "Model FLOPs utilization (MFU)",
      "target": "Hardware FLOPs utilization (HFU)",
      "relation": "semantically_related",
      "weight": 0.7423077821731567,
      "inferred": true
    },
    {
      "source": "Google datacenter network",
      "target": "Singh et al., 2015",
      "relation": "semantically_related",
      "weight": 0.7031381130218506,
      "inferred": true
    },
    {
      "source": "Classification Token",
      "target": "language embedding tokens H",
      "relation": "semantically_related",
      "weight": 0.6789727807044983,
      "inferred": true
    },
    {
      "source": "LLaVA",
      "target": "GPT-3",
      "relations": [
        "is_based_on",
        "compared_to",
        "is evaluated by"
      ],
      "mentions": 3,
      "sources": [
        "segment_anything",
        "segment_anything",
        "segment_anything"
      ],
      "confidence": 1.0
    },
    {
      "source": "LLaVA",
      "target": "CLIP",
      "relations": [
        "connects",
        "uses"
      ],
      "mentions": 2,
      "sources": [
        "segment_anything",
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaVA",
      "target": "Vicuna",
      "relations": [
        "connects",
        "is based on"
      ],
      "mentions": 2,
      "sources": [
        "segment_anything",
        "segment_anything"
      ],
      "confidence": 0.8
    },
    {
      "source": "LLaVA",
      "target": "Science QA",
      "relations": [
        "is_evaluated_on",
        "is_fine_tuned_on"
      ],
      "mentions": 2,
      "sources": [
        "segment_anything",
        "segment_anything"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "LLaVA",
      "target": "multi-turn conversation data",
      "relations": [
        "is trained on"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaVA",
      "target": "Flamingo",
      "relations": [
        "compares to"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.6
    },
    {
      "source": "LLaVA",
      "target": "BLIP-2",
      "relations": [
        "compares to",
        "compared_to",
        "compares_with"
      ],
      "mentions": 3,
      "sources": [
        "segment_anything",
        "segment_anything",
        "segment_anything"
      ],
      "confidence": 1.0
    },
    {
      "source": "LLaVA",
      "target": "OpenFlamingo",
      "relations": [
        "compared_to",
        "compares_with"
      ],
      "mentions": 2,
      "sources": [
        "segment_anything",
        "segment_anything"
      ],
      "confidence": 1.0
    },
    {
      "source": "LLaVA",
      "target": "LLaVA-Bench",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.95
    },
    {
      "source": "LLaVA",
      "target": "instruction-following capability",
      "relations": [
        "improves"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "LLaVA",
      "target": "KOSMOS-1",
      "relation": "semantically_related",
      "weight": 0.7239408493041992,
      "inferred": true
    },
    {
      "source": "LLaVA",
      "target": "FROMAGe",
      "relation": "semantically_related",
      "weight": 0.6531365513801575,
      "inferred": true
    },
    {
      "source": "Science QA",
      "target": "ScienceQA",
      "relation": "evaluated_on",
      "weight": 0.7013242244720459,
      "inferred": true
    },
    {
      "source": "Science QA",
      "target": "LLaVA-Instruct-158K",
      "relation": "evaluated_on",
      "weight": 0.6704220771789551,
      "inferred": true
    },
    {
      "source": "LLaVA-Bench",
      "target": "COCO-Val-2014",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9,
      "relation": "evaluated_on"
    },
    {
      "source": "Flamingo",
      "target": "LMM",
      "relation": "semantically_related",
      "weight": 0.6758987903594971,
      "inferred": true
    },
    {
      "source": "BLIP-2",
      "target": "LMM",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "BLIP-2",
      "target": "FROMAGe",
      "relation": "semantically_related",
      "weight": 0.8283402919769287,
      "inferred": true
    },
    {
      "source": "BLIP-2",
      "target": "KOSMOS-1",
      "relation": "semantically_related",
      "weight": 0.8268049955368042,
      "inferred": true
    },
    {
      "source": "FROMAGe",
      "target": "LMM",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "FROMAGe",
      "target": "KOSMOS-1",
      "relation": "semantically_related",
      "weight": 0.7891978025436401,
      "inferred": true
    },
    {
      "source": "KOSMOS-1",
      "target": "LMM",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "KOSMOS-1",
      "target": "CC",
      "relation": "trained_on",
      "weight": 0.6711516380310059,
      "inferred": true
    },
    {
      "source": "KOSMOS-1",
      "target": "ViperGPT",
      "relation": "semantically_related",
      "weight": 0.6523134112358093,
      "inferred": true
    },
    {
      "source": "PaLM-E",
      "target": "LMM",
      "relations": [
        "is_a"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.9
    },
    {
      "source": "OpenFlamingo",
      "target": "LLaMA-I",
      "relations": [
        "is_based_on"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.8
    },
    {
      "source": "OpenFlamingo",
      "target": "LLaMA-Adapter",
      "relation": "semantically_related",
      "weight": 0.8045874238014221,
      "inferred": true
    },
    {
      "source": "Visual ChatGPT",
      "target": "VisProg",
      "relation": "semantically_related",
      "weight": 0.8116380572319031,
      "inferred": true
    },
    {
      "source": "Visual ChatGPT",
      "target": "ViperGPT",
      "relation": "semantically_related",
      "weight": 0.7979646325111389,
      "inferred": true
    },
    {
      "source": "Visual ChatGPT",
      "target": "X-GPT",
      "relation": "semantically_related",
      "weight": 0.787146806716919,
      "inferred": true
    },
    {
      "source": "Visual ChatGPT",
      "target": "MM-REACT",
      "relation": "semantically_related",
      "weight": 0.6793935298919678,
      "inferred": true
    },
    {
      "source": "X-GPT",
      "target": "ViperGPT",
      "relation": "semantically_related",
      "weight": 0.8095012903213501,
      "inferred": true
    },
    {
      "source": "X-GPT",
      "target": "VisProg",
      "relation": "semantically_related",
      "weight": 0.7794328927993774,
      "inferred": true
    },
    {
      "source": "MM-REACT",
      "target": "VisProg",
      "relation": "semantically_related",
      "weight": 0.7085457444190979,
      "inferred": true
    },
    {
      "source": "MM-REACT",
      "target": "ViperGPT",
      "relation": "semantically_related",
      "weight": 0.6788339018821716,
      "inferred": true
    },
    {
      "source": "VisProg",
      "target": "ViperGPT",
      "relation": "semantically_related",
      "weight": 0.7927399277687073,
      "inferred": true
    },
    {
      "source": "LLaMA-Adapter",
      "target": "LLaMA-I",
      "relations": [
        "is_based_on"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.8
    },
    {
      "source": "visual instruction tuning",
      "target": "visual prompt tuning",
      "relation": "uses",
      "weight": 0.7181467413902283,
      "inferred": true
    },
    {
      "source": "CC",
      "target": "LAION",
      "relation": "evaluated_on",
      "weight": 0.8232038021087646,
      "inferred": true
    },
    {
      "source": "CC",
      "target": "ScienceQA",
      "relation": "evaluated_on",
      "weight": 0.7546223402023315,
      "inferred": true
    },
    {
      "source": "CC",
      "target": "CC3M",
      "relation": "evaluated_on",
      "weight": 0.7286485433578491,
      "inferred": true
    },
    {
      "source": "LAION",
      "target": "ScienceQA",
      "relation": "evaluated_on",
      "weight": 0.6991347074508667,
      "inferred": true
    },
    {
      "source": "visual feature Z",
      "target": "language embedding tokens H",
      "relations": [
        "is converted to"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.8
    },
    {
      "source": "visual feature Z",
      "target": "visual encoder",
      "relation": "uses",
      "weight": 0.7508613467216492,
      "inferred": true
    },
    {
      "source": "projection matrix W",
      "target": "visual feature Z",
      "relations": [
        "connects"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.85
    },
    {
      "source": "multi-turn conversation data",
      "target": "instruction tokens X",
      "relations": [
        "contains"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.8
    },
    {
      "source": "multi-turn conversation data",
      "target": "assistant answers X",
      "relations": [
        "contains"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 0.8
    },
    {
      "source": "multi-turn conversation data",
      "target": "training data",
      "relation": "uses",
      "weight": 0.7916978597640991,
      "inferred": true
    },
    {
      "source": "ScienceQA",
      "target": "COCO-Val-2014",
      "relation": "evaluated_on",
      "weight": 0.6537598967552185,
      "inferred": true
    },
    {
      "source": "evaluation metric",
      "target": "instruction-following capability",
      "relations": [
        "measures"
      ],
      "mentions": 1,
      "sources": [
        "segment_anything"
      ],
      "confidence": 1.0
    },
    {
      "source": "evaluation metric",
      "target": "mean±std",
      "relation": "semantically_related",
      "weight": 0.7962151765823364,
      "inferred": true
    },
    {
      "source": "Swin Transformer",
      "target": "ViT/DeiT",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "DetectoRS",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "SETR",
      "relations": [
        "outperforms"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "ResNet-50",
      "relations": [
        "is similar to"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.75
    },
    {
      "source": "Swin Transformer",
      "target": "ResNet-101",
      "relations": [
        "is similar to"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.75,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "ImageNet-21k",
      "relations": [
        "evaluated_on"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Swin Transformer",
      "target": "DeiT",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "EfficientNet",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "RegNet",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "AdamW",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Swin Transformer",
      "target": "Relative Position Bias",
      "relations": [
        "contains"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "Swin Transformer",
      "target": "Top-1 Accuracy",
      "relations": [
        "achieves"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Swin Transformer",
      "target": "FLOPs",
      "relations": [
        "has"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Swin Transformer",
      "target": "HTC++",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9
    },
    {
      "source": "Swin Transformer",
      "target": "Copy-paste",
      "relation": "uses",
      "weight": 0.696668267250061,
      "inferred": true
    },
    {
      "source": "Swin Transformer",
      "target": "Swin-T",
      "relation": "semantically_related",
      "weight": 0.67667156457901,
      "inferred": true
    },
    {
      "source": "Swin Transformer",
      "target": "Stage 1",
      "relation": "semantically_related",
      "weight": 0.6725982427597046,
      "inferred": true
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Ze Liu",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Yutong Lin",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Yue Cao",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Han Hu",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Yixuan Wei",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Zheng Zhang",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Stephen Lin",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Microsoft Research Asia",
      "target": "Baining Guo",
      "relations": [
        "affiliated_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 1.0,
      "relation": "authored_by"
    },
    {
      "source": "Ze Liu",
      "target": "Baining Guo",
      "relation": "authored_by",
      "weight": 0.7447261214256287,
      "inferred": true
    },
    {
      "source": "Ze Liu",
      "target": "Yixuan Wei",
      "relation": "authored_by",
      "weight": 0.7280870079994202,
      "inferred": true
    },
    {
      "source": "Ze Liu",
      "target": "Han Hu",
      "relation": "authored_by",
      "weight": 0.7017406225204468,
      "inferred": true
    },
    {
      "source": "Ze Liu",
      "target": "Stephen Lin",
      "relation": "authored_by",
      "weight": 0.6944493651390076,
      "inferred": true
    },
    {
      "source": "Yutong Lin",
      "target": "Stephen Lin",
      "relation": "authored_by",
      "weight": 0.9072965383529663,
      "inferred": true
    },
    {
      "source": "Yutong Lin",
      "target": "Baining Guo",
      "relation": "authored_by",
      "weight": 0.7096959948539734,
      "inferred": true
    },
    {
      "source": "Yutong Lin",
      "target": "Han Hu",
      "relation": "authored_by",
      "weight": 0.6628939509391785,
      "inferred": true
    },
    {
      "source": "Yixuan Wei",
      "target": "Baining Guo",
      "relation": "authored_by",
      "weight": 0.6942067742347717,
      "inferred": true
    },
    {
      "source": "Yixuan Wei",
      "target": "Zheng Zhang",
      "relation": "authored_by",
      "weight": 0.6623630523681641,
      "inferred": true
    },
    {
      "source": "Zheng Zhang",
      "target": "Baining Guo",
      "relation": "authored_by",
      "weight": 0.7617331147193909,
      "inferred": true
    },
    {
      "source": "Stephen Lin",
      "target": "Baining Guo",
      "relation": "authored_by",
      "weight": 0.7063508629798889,
      "inferred": true
    },
    {
      "source": "feature pyramid networks (FPN)",
      "target": "Swin Transformer",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "U-Net",
      "target": "Swin Transformer",
      "relations": [
        "is_used_in"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "ViT/DeiT",
      "target": "DeiT",
      "relation": "uses",
      "weight": 0.7749258279800415,
      "inferred": true
    },
    {
      "source": "ViT/DeiT",
      "target": "hybrids",
      "relation": "uses",
      "weight": 0.6899690628051758,
      "inferred": true
    },
    {
      "source": "Copy-paste",
      "target": "DetectoRS",
      "relation": "uses",
      "weight": 0.8069326281547546,
      "inferred": true
    },
    {
      "source": "Copy-paste",
      "target": "SETR",
      "relation": "uses",
      "weight": 0.698581337928772,
      "inferred": true
    },
    {
      "source": "DetectoRS",
      "target": "SETR",
      "relation": "uses",
      "weight": 0.6602579355239868,
      "inferred": true
    },
    {
      "source": "AlexNet",
      "target": "GoogleNet",
      "relation": "uses",
      "weight": 0.7176499962806702,
      "inferred": true
    },
    {
      "source": "VGG",
      "target": "GoogleNet",
      "relation": "uses",
      "weight": 0.6676021814346313,
      "inferred": true
    },
    {
      "source": "GoogleNet",
      "target": "DenseNet",
      "relation": "uses",
      "weight": 0.7300130128860474,
      "inferred": true
    },
    {
      "source": "GoogleNet",
      "target": "RegNet",
      "relation": "uses",
      "weight": 0.6648333072662354,
      "inferred": true
    },
    {
      "source": "ViT",
      "target": "Transformer architecture",
      "relations": [
        "is based on"
      ],
      "mentions": 1,
      "sources": [
        "deit"
      ],
      "confidence": 0.95,
      "relation": "uses"
    },
    {
      "source": "ViT",
      "target": "JFT-300M",
      "relations": [
        "pre_trained_on",
        "requires"
      ],
      "mentions": 3,
      "sources": [
        "reformer",
        "swin_transformer",
        "vision_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "ViT",
      "target": "weight decay",
      "relations": [
        "optimized_with"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.7
    },
    {
      "source": "ViT",
      "target": "dropout",
      "relations": [
        "optimized_with"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.7
    },
    {
      "source": "ViT",
      "target": "label smoothing",
      "relations": [
        "optimized_with"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.7
    },
    {
      "source": "ViT",
      "target": "DeiT",
      "relation": "uses",
      "weight": 0.7553346157073975,
      "inferred": true
    },
    {
      "source": "DeiT",
      "target": "DeiT-S",
      "relation": "semantically_related",
      "weight": 0.7456281781196594,
      "inferred": true
    },
    {
      "source": "W-MSA",
      "target": "SW-MSA",
      "relation": "semantically_related",
      "weight": 0.9536216855049133,
      "inferred": true
    },
    {
      "source": "Stage 1",
      "target": "Stage 1",
      "relations": [
        "is followed by"
      ],
      "mentions": 3,
      "sources": [
        "swin_transformer",
        "swin_transformer",
        "swin_transformer"
      ],
      "confidence": 0.95
    },
    {
      "source": "Stage 1",
      "target": "Swin-T",
      "relation": "semantically_related",
      "weight": 0.720930278301239,
      "inferred": true
    },
    {
      "source": "Swin-T",
      "target": "ImageNet-21k",
      "relations": [
        "fine-tuned_on"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.9,
      "relation": "trained_on"
    },
    {
      "source": "Swin-T",
      "target": "ViT",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "Swin-T",
      "target": "DeiT-S",
      "relations": [
        "compared_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.8
    },
    {
      "source": "Cascade Mask R-CNN",
      "target": "Swin-T",
      "relations": [
        "used_with"
      ],
      "mentions": 1,
      "sources": [
        "swin_transformer"
      ],
      "confidence": 0.85
    },
    {
      "source": "Cascade Mask R-CNN",
      "target": "RepPoints v2",
      "relation": "semantically_related",
      "weight": 0.8582189679145813,
      "inferred": true
    },
    {
      "source": "Cascade Mask R-CNN",
      "target": "Sparse RCNN",
      "relation": "semantically_related",
      "weight": 0.8456181883811951,
      "inferred": true
    },
    {
      "source": "Cascade Mask R-CNN",
      "target": "ATSS",
      "relation": "semantically_related",
      "weight": 0.7844905853271484,
      "inferred": true
    },
    {
      "source": "Cascade Mask R-CNN",
      "target": "APmask",
      "relation": "semantically_related",
      "weight": 0.6570625305175781,
      "inferred": true
    },
    {
      "source": "ATSS",
      "target": "RepPoints v2",
      "relation": "semantically_related",
      "weight": 0.8651551604270935,
      "inferred": true
    },
    {
      "source": "ATSS",
      "target": "Sparse RCNN",
      "relation": "semantically_related",
      "weight": 0.7870752811431885,
      "inferred": true
    },
    {
      "source": "RepPoints v2",
      "target": "Sparse RCNN",
      "relation": "semantically_related",
      "weight": 0.8541359305381775,
      "inferred": true
    },
    {
      "source": "LSTM",
      "target": "Transformer-XL",
      "relations": [
        "is compared to"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "LSTM",
      "target": "RNN",
      "relation": "uses",
      "weight": 0.6690409779548645,
      "inferred": true
    },
    {
      "source": "RNN",
      "target": "Transformer-XL",
      "relations": [
        "is compared to",
        "compared_to"
      ],
      "mentions": 2,
      "sources": [
        "transformer_xl",
        "transformer_xl"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "gradient vanishing",
      "target": "gradient explosion",
      "relation": "semantically_related",
      "weight": 0.8924340605735779,
      "inferred": true
    },
    {
      "source": "Softmax function",
      "target": "logits",
      "relation": "semantically_related",
      "weight": 0.7323721051216125,
      "inferred": true
    },
    {
      "source": "Bengio et al. (2003)",
      "target": "Devlin et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.8992746472358704,
      "inferred": true
    },
    {
      "source": "Bengio et al. (2003)",
      "target": "Merity et al. (2016)",
      "relation": "semantically_related",
      "weight": 0.8580909371376038,
      "inferred": true
    },
    {
      "source": "Bengio et al. (2003)",
      "target": "Peters et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.7504196763038635,
      "inferred": true
    },
    {
      "source": "Mikolov et al. (2010)",
      "target": "Devlin et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.7319671511650085,
      "inferred": true
    },
    {
      "source": "Mikolov et al. (2010)",
      "target": "Merity et al. (2016)",
      "relation": "semantically_related",
      "weight": 0.6760547161102295,
      "inferred": true
    },
    {
      "source": "Merity et al. (2016)",
      "target": "Devlin et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.8863439559936523,
      "inferred": true
    },
    {
      "source": "Merity et al. (2016)",
      "target": "Peters et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.7637528777122498,
      "inferred": true
    },
    {
      "source": "Peters et al. (2018)",
      "target": "Devlin et al. (2018)",
      "relation": "semantically_related",
      "weight": 0.7751173377037048,
      "inferred": true
    },
    {
      "source": "SG function",
      "target": "Transformer-XL",
      "relations": [
        "is used in"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "positional encodings",
      "target": "absolute positional embedding",
      "relation": "semantically_related",
      "weight": 0.6520584225654602,
      "inferred": true
    },
    {
      "source": "hidden state",
      "target": "Transformer-XL",
      "relations": [
        "are_cached_in"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9,
      "relation": "uses"
    },
    {
      "source": "long-term dependency",
      "target": "Transformer-XL",
      "relations": [
        "is modeled by"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "BPTT",
      "target": "RNN",
      "relations": [
        "is_based_on"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85,
      "relation": "uses"
    },
    {
      "source": "query vector",
      "target": "key vector",
      "relations": [
        "attends_on"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85
    },
    {
      "source": "key vector",
      "target": "location-based key vectors",
      "relation": "semantically_related",
      "weight": 0.6961673498153687,
      "inferred": true
    },
    {
      "source": "key vector",
      "target": "v",
      "relation": "semantically_related",
      "weight": 0.6696140170097351,
      "inferred": true
    },
    {
      "source": "Shaw et al. (2018)",
      "target": "Transformer-XL",
      "relations": [
        "compares_with"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.7,
      "relation": "uses"
    },
    {
      "source": "Shaw et al. (2018)",
      "target": "absolute positional embedding",
      "relation": "semantically_related",
      "weight": 0.6500394344329834,
      "inferred": true
    },
    {
      "source": "Huang et al. (2018)",
      "target": "Positional Encoding",
      "relations": [
        "uses"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.75,
      "relation": "uses"
    },
    {
      "source": "LayerNorm",
      "target": "Layer Normalization (LN)",
      "relation": "uses",
      "weight": 0.6808212399482727,
      "inferred": true
    },
    {
      "source": "absolute positional embedding",
      "target": "absolute positional embedding",
      "relations": [
        "is replaced by"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.8
    },
    {
      "source": "u",
      "target": "query vector",
      "relations": [
        "replaces"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85
    },
    {
      "source": "u",
      "target": "v",
      "relation": "semantically_related",
      "weight": 0.9541497230529785,
      "inferred": true
    },
    {
      "source": "v",
      "target": "query vector",
      "relations": [
        "substitutes"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.85
    },
    {
      "source": "Khandelwal et al. (2018)",
      "target": "Effective Context Length (ECL)",
      "relations": [
        "proposed"
      ],
      "mentions": 1,
      "sources": [
        "transformer_xl"
      ],
      "confidence": 0.9
    },
    {
      "source": "Khandelwal et al. (2018)",
      "target": "Relative Effective Context Length (RECL)",
      "relation": "semantically_related",
      "weight": 0.6869556307792664,
      "inferred": true
    },
    {
      "source": "Effective Context Length (ECL)",
      "target": "Relative Effective Context Length (RECL)",
      "relation": "semantically_related",
      "weight": 0.7684452533721924,
      "inferred": true
    },
    {
      "source": "Self-Supervision",
      "target": "Vision Transformer",
      "relations": [
        "is_explored_for"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.8,
      "relation": "uses"
    },
    {
      "source": "R50x1",
      "target": "R152x1",
      "relation": "semantically_related",
      "weight": 0.9448045492172241,
      "inferred": true
    },
    {
      "source": "R50x1",
      "target": "R101x1",
      "relation": "semantically_related",
      "weight": 0.9418666362762451,
      "inferred": true
    },
    {
      "source": "R50x1",
      "target": "R200x3",
      "relation": "semantically_related",
      "weight": 0.9149831533432007,
      "inferred": true
    },
    {
      "source": "R101x1",
      "target": "R152x1",
      "relation": "semantically_related",
      "weight": 0.9478583335876465,
      "inferred": true
    },
    {
      "source": "R101x1",
      "target": "R200x3",
      "relation": "semantically_related",
      "weight": 0.9207754731178284,
      "inferred": true
    },
    {
      "source": "R152x1",
      "target": "R200x3",
      "relation": "semantically_related",
      "weight": 0.8963014483451843,
      "inferred": true
    },
    {
      "source": "hybrids",
      "target": "ViT",
      "relations": [
        "outperform"
      ],
      "mentions": 2,
      "sources": [
        "reformer",
        "vision_transformer"
      ],
      "confidence": 0.6,
      "relation": "uses"
    },
    {
      "source": "position embeddings",
      "target": "2D image topology",
      "relations": [
        "represent"
      ],
      "mentions": 1,
      "sources": [
        "vision_transformer"
      ],
      "confidence": 0.8
    }
  ],
  "statistics": {
    "num_nodes": 609,
    "num_edges": 304,
    "density": 0.0008210180623973727,
    "num_communities": 379,
    "avg_pagerank": 0.0016420361247947474,
    "avg_betweenness": 0.0001727658207041207,
    "avg_clustering_coeff": 0.018903037342452085,
    "top_entities_by_pagerank": [
      {
        "entity": "Vision Transformer (ViT)",
        "score": 0.02482473536051485
      },
      {
        "entity": "GPT-3",
        "score": 0.018818293659738593
      },
      {
        "entity": "Transformer-XL",
        "score": 0.012433556432777474
      },
      {
        "entity": "ImageNet-21k",
        "score": 0.010392260613898962
      },
      {
        "entity": "Transformer",
        "score": 0.008595654786197731
      },
      {
        "entity": "Transformer architecture",
        "score": 0.008545848835985975
      },
      {
        "entity": "Common Crawl",
        "score": 0.008060344444078802
      },
      {
        "entity": "Longformer",
        "score": 0.008041616010407168
      },
      {
        "entity": "meta-learning",
        "score": 0.008022251854755252
      },
      {
        "entity": "ViT",
        "score": 0.007876293496778682
      }
    ],
    "top_entities_by_betweenness": [
      {
        "entity": "GPT-3",
        "score": 0.014426401413335647
      },
      {
        "entity": "Transformer architecture",
        "score": 0.01094522240527183
      },
      {
        "entity": "Vision Transformer (ViT)",
        "score": 0.00946333347784618
      },
      {
        "entity": "LLaMA-I",
        "score": 0.008430066620422555
      },
      {
        "entity": "ViT",
        "score": 0.005696154513136217
      },
      {
        "entity": "CLIP",
        "score": 0.005276344692043123
      },
      {
        "entity": "Swin Transformer",
        "score": 0.005171572877828839
      },
      {
        "entity": "Longformer",
        "score": 0.004917952831006676
      },
      {
        "entity": "self-attention",
        "score": 0.004540232376658285
      },
      {
        "entity": "ResNet-101",
        "score": 0.004033805167779415
      }
    ]
  },
  "improvements": {
    "timestamp": "2025-12-11T00:59:01.101977",
    "phases": {
      "phase1": {
        "edges_added": 518,
        "noise_rescued": 26,
        "summaries_generated": 14,
        "time_seconds": 68.8297848701477
      },
      "phase2": {
        "retrieval_tests": [
          {
            "query": "How does Vision Transformer work?",
            "entities": 10,
            "relationships": 15
          },
          {
            "query": "What is self-attention?",
            "entities": 10,
            "relationships": 15
          },
          {
            "query": "GPT-3 training data",
            "entities": 10,
            "relationships": 14
          }
        ],
        "time_seconds": 18.4650616645813
      },
      "phase3": {
        "generic_before": 822,
        "generic_after": 349,
        "typed_count": 473,
        "relation_distribution": {
          "uses": 222,
          "related_to": 112,
          "trained_on": 55,
          "part_of": 13,
          "compared_to": 7,
          "evaluated_on": 100,
          "authored_by": 68,
          "based_on": 1,
          "semantically_related": 237,
          "applied_to": 7
        },
        "time_seconds": 8.470478057861328
      }
    },
    "metrics": {
      "baseline": {
        "nodes": 609,
        "edges": 304,
        "avg_degree": 0.9983579638752053,
        "topics": 14,
        "modularity": 0.786853880385349
      },
      "final": {
        "nodes": 609,
        "edges": 822,
        "avg_degree": 2.6995073891625614,
        "typed_relations": 10,
        "coverage": 0.4269293924466338
      }
    }
  }
}